{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"source/img/cook.jpg","path":"img/cook.jpg","modified":0,"renderable":0},{"_id":"source/img/dao.png","path":"img/dao.png","modified":0,"renderable":0},{"_id":"source/img/fifa-deep-nn.png","path":"img/fifa-deep-nn.png","modified":0,"renderable":0},{"_id":"source/img/ge14.jpg","path":"img/ge14.jpg","modified":0,"renderable":0},{"_id":"source/img/german.jpg","path":"img/german.jpg","modified":0,"renderable":0},{"_id":"source/img/happiness.jpg","path":"img/happiness.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_1.jpg","path":"img/img_small_1.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_2.jpg","path":"img/img_small_2.jpg","modified":0,"renderable":0},{"_id":"source/img/linear-regressor-knockout.PNG","path":"img/linear-regressor-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/loc.png","path":"img/loc.png","modified":0,"renderable":0},{"_id":"source/img/logo.png","path":"img/logo.png","modified":0,"renderable":0},{"_id":"source/img/model_performance.png","path":"img/model_performance.png","modified":0,"renderable":0},{"_id":"source/img/music-supply.jpg","path":"img/music-supply.jpg","modified":0,"renderable":0},{"_id":"source/img/nn-knockout.PNG","path":"img/nn-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/religion.png","path":"img/religion.png","modified":0,"renderable":0},{"_id":"source/img/slide_1.jpg","path":"img/slide_1.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_2.jpg","path":"img/slide_2.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_3.jpg","path":"img/slide_3.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_4.jpg","path":"img/slide_4.jpg","modified":0,"renderable":0},{"_id":"source/img/trust.jpg","path":"img/trust.jpg","modified":0,"renderable":0},{"_id":"source/img/work-for-you.jpg","path":"img/work-for-you.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-life.jpg","path":"img/ai-life.jpg","modified":0,"renderable":0},{"_id":"source/img/belgium.jpg","path":"img/belgium.jpg","modified":0,"renderable":0},{"_id":"source/img/blockchain.png","path":"img/blockchain.png","modified":0,"renderable":0},{"_id":"source/img/money.jpg","path":"img/money.jpg","modified":0,"renderable":0},{"_id":"source/img/passing-mark.jpg","path":"img/passing-mark.jpg","modified":0,"renderable":0},{"_id":"source/img/prof.png","path":"img/prof.png","modified":0,"renderable":0},{"_id":"source/img/results.png","path":"img/results.png","modified":0,"renderable":0},{"_id":"source/img/science-empire.jpg","path":"img/science-empire.jpg","modified":0,"renderable":0},{"_id":"source/img/ubi.jpeg","path":"img/ubi.jpeg","modified":0,"renderable":0},{"_id":"source/img/unstoppable.jpg","path":"img/unstoppable.jpg","modified":0,"renderable":0},{"_id":"source/img/win-ai.jpg","path":"img/win-ai.jpg","modified":0,"renderable":0},{"_id":"source/img/brazil.png","path":"img/brazil.png","modified":0,"renderable":0},{"_id":"source/img/consumerism.gif","path":"img/consumerism.gif","modified":0,"renderable":0},{"_id":"source/img/licc.jpg","path":"img/licc.jpg","modified":0,"renderable":0},{"_id":"source/img/plants.jpg","path":"img/plants.jpg","modified":0,"renderable":0},{"_id":"source/img/science-dark.jpg","path":"img/science-dark.jpg","modified":0,"renderable":0},{"_id":"source/img/udacity-nb.jpg","path":"img/udacity-nb.jpg","modified":0,"renderable":0},{"_id":"source/img/storytelling.png","path":"img/storytelling.png","modified":0,"renderable":0},{"_id":"source/img/imperialism.jpg","path":"img/imperialism.jpg","modified":0,"renderable":0},{"_id":"source/img/money2.jpg","path":"img/money2.jpg","modified":0,"renderable":0},{"_id":"source/img/humanism.jpeg","path":"img/humanism.jpeg","modified":0,"renderable":0},{"_id":"source/img/too-big-to-fail.jpg","path":"img/too-big-to-fail.jpg","modified":0,"renderable":0},{"_id":"source/img/gossip.jpg","path":"img/gossip.jpg","modified":0,"renderable":0},{"_id":"source/img/profile.jpeg","path":"img/profile.jpeg","modified":0,"renderable":0},{"_id":"source/img/najib-tun-m.jpg","path":"img/najib-tun-m.jpg","modified":0,"renderable":0},{"_id":"source/img/sapiens.jpg","path":"img/sapiens.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-electric.png","path":"img/ai-electric.png","modified":0,"renderable":0},{"_id":"source/img/taryn.png","path":"img/taryn.png","modified":0,"renderable":0},{"_id":"source/about/profile.jpeg","path":"about/profile.jpeg","modified":0,"renderable":0},{"_id":"themes/aircloud/source/css/aircloud.css","path":"css/aircloud.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.css.map","path":"css/aircloud.css.map","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.less","path":"css/aircloud.less","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/index.js","path":"js/index.js","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"source/img/extres.png","path":"img/extres.png","modified":0,"renderable":0},{"_id":"source/img/ashis.png","path":"img/ashis.png","modified":0,"renderable":0},{"_id":"source/img/virtuoso.png","path":"img/virtuoso.png","modified":0,"renderable":0},{"_id":"source/img/midivae.png","path":"img/midivae.png","modified":0,"renderable":0},{"_id":"source/img/musicvae.png","path":"img/musicvae.png","modified":0,"renderable":0},{"_id":"source/img/deep-analogy.png","path":"img/deep-analogy.png","modified":0,"renderable":0},{"_id":"source/img/ashis2.png","path":"img/ashis2.png","modified":0,"renderable":0},{"_id":"source/img/relative-attention.png","path":"img/relative-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-attention-2.png","path":"img/relative-attention-2.png","modified":0,"renderable":0},{"_id":"source/img/new-relative-attention.png","path":"img/new-relative-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention.png","path":"img/relative-local-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-2.png","path":"img/relative-local-attention-2.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-srel.png","path":"img/relative-local-attention-srel.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-unmasked.png","path":"img/relative-local-attention-unmasked.png","modified":0,"renderable":0},{"_id":"source/img/music-transformer-results.png","path":"img/music-transformer-results.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl.png","path":"img/kingma-ssl.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl-2.png","path":"img/kingma-ssl-2.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl-3.png","path":"img/kingma-ssl-3.png","modified":0,"renderable":0},{"_id":"source/img/vade-ssl.png","path":"img/vade-ssl.png","modified":0,"renderable":0},{"_id":"source/img/radford-sentiment.png","path":"img/radford-sentiment.png","modified":0,"renderable":0},{"_id":"source/img/stft.png","path":"img/stft.png","modified":0,"renderable":0},{"_id":"source/img/istft.png","path":"img/istft.png","modified":0,"renderable":0},{"_id":"source/img/ismir_attr.png","path":"img/ismir_attr.png","modified":0,"renderable":0},{"_id":"source/img/ismir_conn.png","path":"img/ismir_conn.png","modified":0,"renderable":0},{"_id":"source/img/ismir_dmel.png","path":"img/ismir_dmel.png","modified":0,"renderable":0},{"_id":"source/img/ismir_edit.png","path":"img/ismir_edit.png","modified":0,"renderable":0},{"_id":"source/img/ismir_bebop.png","path":"img/ismir_bebop.png","modified":0,"renderable":0},{"_id":"source/img/ismir_pianotree.png","path":"img/ismir_pianotree.png","modified":0,"renderable":0},{"_id":"source/img/ismir_singing.png","path":"img/ismir_singing.png","modified":0,"renderable":0},{"_id":"source/img/ismir_sketchnet.png","path":"img/ismir_sketchnet.png","modified":0,"renderable":0},{"_id":"source/img/ismir_drumgan.png","path":"img/ismir_drumgan.png","modified":0,"renderable":0},{"_id":"source/img/ismir_fadernets.png","path":"img/ismir_fadernets.png","modified":0,"renderable":0},{"_id":"source/img/ismir_interpretable.png","path":"img/ismir_interpretable.png","modified":0,"renderable":0},{"_id":"source/img/ismir_jazz.png","path":"img/ismir_jazz.png","modified":0,"renderable":0},{"_id":"source/img/ismir_jyun.png","path":"img/ismir_jyun.png","modified":0,"renderable":0},{"_id":"source/img/ismir_metric.png","path":"img/ismir_metric.png","modified":0,"renderable":0},{"_id":"source/img/ismir_unets.png","path":"img/ismir_unets.png","modified":0,"renderable":0},{"_id":"source/img/ismir_phoneme1.png","path":"img/ismir_phoneme1.png","modified":0,"renderable":0},{"_id":"source/img/ismir_phoneme2.png","path":"img/ismir_phoneme2.png","modified":0,"renderable":0},{"_id":"source/img/ismir_multitask.png","path":"img/ismir_multitask.png","modified":0,"renderable":0},{"_id":"source/img/ismir_vocal.png","path":"img/ismir_vocal.png","modified":0,"renderable":0},{"_id":"source/img/ismir_furkan.png","path":"img/ismir_furkan.png","modified":0,"renderable":0},{"_id":"source/img/ismir_doras.png","path":"img/ismir_doras.png","modified":0,"renderable":0},{"_id":"source/img/ismir_lottery.png","path":"img/ismir_lottery.png","modified":0,"renderable":0},{"_id":"source/img/ismir_spherical.png","path":"img/ismir_spherical.png","modified":0,"renderable":0},{"_id":"source/img/ismir_transcription.png","path":"img/ismir_transcription.png","modified":0,"renderable":0},{"_id":"source/img/mean-vs-max-pool.png","path":"img/mean-vs-max-pool.png","modified":0,"renderable":0},{"_id":"source/img/csd-ml-ops.png","path":"img/csd-ml-ops.png","modified":0,"renderable":0},{"_id":"source/img/csd-hpcp.png","path":"img/csd-hpcp.png","modified":0,"renderable":0},{"_id":"source/img/csd-hybrid.png","path":"img/csd-hybrid.png","modified":0,"renderable":0},{"_id":"source/img/csd-bytecover.png","path":"img/csd-bytecover.png","modified":0,"renderable":0},{"_id":"source/img/csd-dominant.png","path":"img/csd-dominant.png","modified":0,"renderable":0},{"_id":"source/img/csd-shingles.png","path":"img/csd-shingles.png","modified":0,"renderable":0},{"_id":"themes/aircloud/source/css/custom.css","path":"css/custom.css","modified":0,"renderable":1},{"_id":"source/img/ieee_fp.png","path":"img/ieee_fp.png","modified":0,"renderable":0},{"_id":"source/img/contentvec.png","path":"img/contentvec.png","modified":0,"renderable":0},{"_id":"source/img/nsf.png","path":"img/nsf.png","modified":0,"renderable":0},{"_id":"source/img/rvc-infer.png","path":"img/rvc-infer.png","modified":0,"renderable":0},{"_id":"source/img/rvc-train.png","path":"img/rvc-train.png","modified":0,"renderable":0}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1579847575470},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1579847575470},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1579847575470},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1579847575470},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1579847575470},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1579847575476},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1579847568417},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1579847575470},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1579847575470},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1579847575470},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1579847575471},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1579847575471},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1579847575471},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1579847575471},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1579847575471},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1579847575471},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1579847575471},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1579847575471},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1579847575472},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1579847575475},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1579847575475},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1579847575475},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1579847575475},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1579847575476},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1579847575476},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1579847575474},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1579847575476},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1579847575479},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1579847575483},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1579847575487},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1579847575487},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1579847575474},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1579847575474},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1579847575480},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1579847575482},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1579847575486},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1579847575482},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1579847575481},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1579847575483},{"_id":"themes/hexo-theme-aircloud/.gitignore","hash":"5a4a925cfd624633dafaacaced416c8d7272dcef","modified":1579853649534},{"_id":"themes/hexo-theme-aircloud/LICENSE","hash":"218b4bf797149a2751a015812a9adefe368185c1","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/_config.yml","hash":"0ad3a6ab2c9bb07fb1e030052622fdcde5c6f28a","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/readme-en.md","hash":"2903b1e9db12cd72ed6f8c10be14cd7f6afd82cf","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/readme.md","hash":"4be1fc64bd1dc335a986a39594564e89bd7eba43","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/languages/en.yml","hash":"93d77f44c0386df2defce0ac465b19e9a85f4d2f","modified":1579887859365},{"_id":"themes/hexo-theme-aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/index.ejs","hash":"09e2407d615be7fe7ac41d11df3b7026e7393080","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/layout.ejs","hash":"7efd113aee90e698e187d0ea1f0b42a1c00d210e","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/post.ejs","hash":"2eb5fc0c2bb801528c3db3b09e6cb4d073e3ad99","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/footer.ejs","hash":"2ab1dc9da5183fc5e74a4bddbf0c29f992057ec9","modified":1579887884627},{"_id":"themes/hexo-theme-aircloud/layout/_partial/head.ejs","hash":"3f18d5d4951a205bab25b08d6bf85b054c84a21b","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/nav.ejs","hash":"f7bb88510ece895a48490c63d33323dc4eff4136","modified":1579887766729},{"_id":"themes/hexo-theme-aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.css","hash":"64914aa6ecd5b948676870e0809e0f220b162e3b","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.less","hash":"8aef4d8cfdefbcd2e28d4985a4f79a5005ca0b6c","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.css","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.less","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.css","hash":"40d7cadf42b130ea1b40de1ae73b2b00e27f476f","modified":1579883019352},{"_id":"themes/hexo-theme-aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.less","hash":"194ac7db2eeee7307fcb7470302f8172100181fb","modified":1579860381468},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.css","hash":"cfe668f5e11de4d20ec6538d480b74a86380de02","modified":1579887938712},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888254394},{"_id":"themes/hexo-theme-aircloud/source/_less/post.css","hash":"4adf531589cb55413264c188b29ae47ab703beb8","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/post.less","hash":"bbbd81c03e7581950d82bf971eda49e8bed7bee1","modified":1579883019353},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css","hash":"e6082557a5f0e546169ab1aa0ba29bda4ef5c182","modified":1579883019355},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1579853649546},{"_id":"themes/hexo-theme-aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579853649547},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579853649547},{"_id":"source/_posts/test-post.md","hash":"623b1b89f04a61ba2905a155459a59ed1dddd8de","modified":1579859460776},{"_id":"source/test-page/index.md","hash":"a80f212d7be56d71b665227a1dcc10ea67d246ef","modified":1579856642376},{"_id":"source/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1579857100564},{"_id":"source/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1579857100564},{"_id":"source/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1579857100565},{"_id":"source/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1579857100565},{"_id":"source/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1579857100566},{"_id":"source/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1579857100566},{"_id":"source/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1579857100567},{"_id":"source/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1579857100567},{"_id":"source/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1579857100569},{"_id":"source/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1579857100569},{"_id":"source/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1579857100569},{"_id":"source/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1579857100569},{"_id":"source/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1579857100571},{"_id":"source/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1579857100572},{"_id":"source/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1579857100574},{"_id":"source/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1579857100577},{"_id":"source/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1579857100577},{"_id":"source/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1579857100577},{"_id":"source/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1579857100578},{"_id":"source/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1579857100582},{"_id":"source/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1579857100584},{"_id":"source/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1579857100562},{"_id":"source/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1579857100562},{"_id":"source/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1579857100563},{"_id":"source/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1579857100570},{"_id":"source/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1579857100572},{"_id":"source/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1579857100573},{"_id":"source/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1579857100575},{"_id":"source/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1579857100576},{"_id":"source/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1579857100582},{"_id":"source/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1579857100583},{"_id":"source/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1579857100584},{"_id":"source/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1579857100563},{"_id":"source/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1579857100563},{"_id":"source/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1579857100568},{"_id":"source/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1579857100572},{"_id":"source/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1579857100576},{"_id":"source/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1579857100583},{"_id":"source/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1579857100578},{"_id":"source/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1579857100568},{"_id":"source/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1579857100570},{"_id":"source/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1579857100567},{"_id":"source/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1579857100582},{"_id":"source/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1579857100566},{"_id":"source/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579857100574},{"_id":"source/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1579857100571},{"_id":"source/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1579857100575},{"_id":"source/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1579857100561},{"_id":"source/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1579857100581},{"_id":"source/about/index.md","hash":"102c7efafc1bf9416fa44a233ea855cdaa552136","modified":1751018521987},{"_id":"source/tags/index.md","hash":"22dd3308e0a3db852e008fa8c8d526142e790dcb","modified":1579859110818},{"_id":"source/bloglist/index.md","hash":"948621e8d767d5a778794b8014b30b9b1e74d2a5","modified":1661089428999},{"_id":"source/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579863788426},{"_id":"source/_posts/ai-music-direction.md","hash":"17b249dc54092c7984a9a9869b8f05d9e1f4fe46","modified":1592548634917},{"_id":"source/_posts/nature-v2.md","hash":"66f2a37a1d885848cb3c97ef9d86529466e96b60","modified":1579866217542},{"_id":"source/_posts/sapiens-1.md","hash":"613604e163770883841e61a4a47fb3fed184d72c","modified":1579866217543},{"_id":"source/_posts/sapiens-2.md","hash":"ed008d144dcaa5a419b30df28d2bca8fd78741b5","modified":1579866217543},{"_id":"themes/hexo-theme-aircloud/source/css/fonts.css","hash":"c5e7b1d0ada40787eb87fdeef7e64d00588046c3","modified":1579867807637},{"_id":"themes/aircloud/_config.yml","hash":"fce8b918a9ee52e05bd95fdc83bd53e8fe8478ac","modified":1579888968113},{"_id":"themes/aircloud/test.md","hash":"501c404781cfbfe856c8d55dd5f1cd612dd83e41","modified":1579888897249},{"_id":"themes/aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968117},{"_id":"themes/aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968119},{"_id":"themes/aircloud/languages/en.yml","hash":"2c58994397fcb3b15a7f5d3014c123153c7ae038","modified":1751015287802},{"_id":"themes/aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579888968114},{"_id":"themes/aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579888968115},{"_id":"themes/aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579888968117},{"_id":"themes/aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579888968116},{"_id":"themes/aircloud/layout/index.ejs","hash":"aa7464f8136c967d13978902a2c23f2f40273926","modified":1748587416062},{"_id":"themes/aircloud/layout/layout.ejs","hash":"f8627111527b8fe08e9e64ca51dd8d7d689a3a99","modified":1748584726702},{"_id":"themes/aircloud/layout/post.ejs","hash":"cf0c402c293599d13c9aa65155348950a6fc8f12","modified":1751014512988},{"_id":"themes/aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579888968116},{"_id":"themes/aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968123},{"_id":"themes/aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968124},{"_id":"themes/aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968130},{"_id":"themes/aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968127},{"_id":"themes/aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968122},{"_id":"themes/aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968121},{"_id":"themes/aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579888968119},{"_id":"themes/aircloud/layout/_partial/footer.ejs","hash":"35ae38065e15944add9e3b039c54dfaeb697f229","modified":1661089429152},{"_id":"themes/aircloud/layout/_partial/head.ejs","hash":"50a15880a694837ffbc66440382330a8dd519403","modified":1751018805551},{"_id":"themes/aircloud/layout/_partial/nav.ejs","hash":"b4ec0d5cf15f1ea5f584a795cb744be593a64f66","modified":1602869899561},{"_id":"themes/aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579888968118},{"_id":"themes/aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579888968125},{"_id":"themes/aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579888968128},{"_id":"themes/aircloud/source/_less/common.css","hash":"31800f127cb208ef6cedbf0bbae804484c6b2dd8","modified":1748584726711},{"_id":"themes/aircloud/source/_less/common.less","hash":"559bae644740ee5becd005295463ba4ba54ff46b","modified":1748584726712},{"_id":"themes/aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579888968129},{"_id":"themes/aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579888968129},{"_id":"themes/aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579888968131},{"_id":"themes/aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579888968121},{"_id":"themes/aircloud/source/_less/hightlight.css","hash":"c8102d3ba9920a2f874be203b3f6925a7d90bdfb","modified":1585907918636},{"_id":"themes/aircloud/source/_less/hightlight.less","hash":"fa11cfd5496659841b3833a9377dccac573f4567","modified":1585907921900},{"_id":"themes/aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579888968125},{"_id":"themes/aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579888968126},{"_id":"themes/aircloud/source/_less/layout.less","hash":"b147f952ca90e2d26a00a915557a5663bf654c12","modified":1751011779416},{"_id":"themes/aircloud/source/_less/layout.css","hash":"98036594db3961b2af6590f83480060319893f2f","modified":1751011718561},{"_id":"themes/aircloud/source/_less/nav.css","hash":"32d0640c30a3c921e1f19f74cff2c5095f6ae02c","modified":1579889523601},{"_id":"themes/aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888968130},{"_id":"themes/aircloud/source/_less/post.css","hash":"bbc233840859e274eeb56a19d176e55d62efd7f8","modified":1748587500671},{"_id":"themes/aircloud/source/_less/post.less","hash":"c080d23530b85bbc77a2cd1e7b1c392c49d9b57f","modified":1748586144376},{"_id":"themes/aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579888968121},{"_id":"themes/aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579888968120},{"_id":"themes/aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579888968134},{"_id":"themes/aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579888968125},{"_id":"themes/aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579888968122},{"_id":"themes/aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579888968124},{"_id":"themes/aircloud/source/css/aircloud.css","hash":"168355fffb458941017292a36d77bcf5d2c7e6dd","modified":1751017259762},{"_id":"themes/aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579888968136},{"_id":"themes/aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579888968135},{"_id":"themes/aircloud/source/css/gitment.css","hash":"6e18c2374cb21db45f49e75bbd813b425ed30522","modified":1661089429175},{"_id":"themes/aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579888968137},{"_id":"themes/aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579888968133},{"_id":"themes/aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579888968132},{"_id":"themes/aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579888968132},{"_id":"themes/aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579888968138},{"_id":"public/about/index.html","hash":"e4d372801b1431bc56a5dd2137b5adf270ccfeca","modified":1751019065293},{"_id":"public/tags/index.html","hash":"24630ac19ab6db2cf74a072b73e79ddafaa6ed59","modified":1751019065293},{"_id":"public/bloglist/index.html","hash":"3a459d766ea3f3ae93ca9cdcf7c29c78d50c718f","modified":1751019065293},{"_id":"public/2018/09/24/ai-music-direction/index.html","hash":"fd06714ee6e5d18149ee1861b37230c55365553e","modified":1751019065293},{"_id":"public/archives/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2018/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2018/09/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/tags/General-Thoughts/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/tags/AI-Music/index.html","hash":"1faa6f215707357b13469eb504df0f0965ca6253","modified":1592546827459},{"_id":"public/index.html","hash":"be1ff3ac129306be85e6fe3ef14752852c5a43bb","modified":1751019065293},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1677053392823},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1677053392823},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1677053392823},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1677053392823},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1677053392823},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1677053392823},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1677053392823},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1677053392823},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1677053392823},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1677053392823},{"_id":"public/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1751018633451},{"_id":"public/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1751018633451},{"_id":"public/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1751018633451},{"_id":"public/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1751018633451},{"_id":"public/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1751018633451},{"_id":"public/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1751018633451},{"_id":"public/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1751018633451},{"_id":"public/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1751018633451},{"_id":"public/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1751018633451},{"_id":"public/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1751018633451},{"_id":"public/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1751018633451},{"_id":"public/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1751018633451},{"_id":"public/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1751018633451},{"_id":"public/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1751018633451},{"_id":"public/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1751018633451},{"_id":"public/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1751018633451},{"_id":"public/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1751018633451},{"_id":"public/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1751018633451},{"_id":"public/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1751018633451},{"_id":"public/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1751018633451},{"_id":"public/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1751018633451},{"_id":"public/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1751018633451},{"_id":"public/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1751018633451},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1677053392823},{"_id":"public/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1751018633451},{"_id":"public/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1751018633451},{"_id":"public/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1751018633451},{"_id":"public/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1751018633451},{"_id":"public/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1751018633451},{"_id":"public/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1751018633451},{"_id":"public/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1751018633451},{"_id":"public/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1751018633451},{"_id":"public/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1751018633451},{"_id":"public/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1751018633451},{"_id":"public/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1751018633451},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1677053392823},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1677053392823},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1677053392823},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1677053392823},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1677053392823},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1677053392823},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1677053392823},{"_id":"public/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1751018633451},{"_id":"public/css/style.css","hash":"d4cfa90089c78a8b791252afae9fafa3b5658900","modified":1677053392823},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1677053392823},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1677053392823},{"_id":"public/css/gitment.css","hash":"6e18c2374cb21db45f49e75bbd813b425ed30522","modified":1751018633451},{"_id":"public/css/aircloud.css","hash":"168355fffb458941017292a36d77bcf5d2c7e6dd","modified":1751018633451},{"_id":"public/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1751018633451},{"_id":"public/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1751018633451},{"_id":"public/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1751018633451},{"_id":"public/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1751018633451},{"_id":"public/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1751018633451},{"_id":"public/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1751018633451},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1677053392823},{"_id":"public/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1751018633451},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1677053392823},{"_id":"public/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1751018633451},{"_id":"public/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1751018633451},{"_id":"public/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1751018633451},{"_id":"public/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1751018633451},{"_id":"public/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1751018633451},{"_id":"public/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1751018633451},{"_id":"public/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1751018633451},{"_id":"public/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1751018633451},{"_id":"public/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1751018633451},{"_id":"public/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1751018633451},{"_id":"public/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1751018633451},{"_id":"public/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1751018633451},{"_id":"source/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1580035544388},{"_id":"public/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1677053392823},{"_id":"source/.DS_Store","hash":"5df041f68ab53b7afd3c7b32dd18d643829e4110","modified":1748584726526},{"_id":"source/_posts/vae-symbolic-music.md","hash":"95a25c219ed699281baa2510da4c3cc20bad6d0a","modified":1751018978787},{"_id":"source/img/extres.png","hash":"69d1d086fa0c0a8cc449a57ac523a6a46345da43","modified":1585282883093},{"_id":"source/img/ashis.png","hash":"e6ddb88bf9d29a480ded59086f70f6880c6fd3cf","modified":1585282883085},{"_id":"source/img/virtuoso.png","hash":"a00ef13f5cfa7775af0c131c62c9c447044ef4b0","modified":1585282883134},{"_id":"source/img/midivae.png","hash":"c060f51c9eb72566f9ec5adcb7581167c6429948","modified":1585282883099},{"_id":"source/img/musicvae.png","hash":"e2d3f7d11481c160bc640d506ab0d8c86ca585d9","modified":1585282883104},{"_id":"source/img/deep-analogy.png","hash":"cbe306dc2c8e08b1254ec41d80b2c277e5363e57","modified":1585282883092},{"_id":"source/img/ashis2.png","hash":"a89e324fb31fe350465a840c1c01d617ac91ec0c","modified":1585280695619},{"_id":"public/archives/2020/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2020/01/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/tags/VAE/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/tags/Symbolic-Music/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/2020/01/26/vae-symbolic-music/index.html","hash":"89c694cd8026bcd12c10186b908e425434b8951a","modified":1751019065293},{"_id":"public/img/extres.png","hash":"69d1d086fa0c0a8cc449a57ac523a6a46345da43","modified":1751018633451},{"_id":"public/img/ashis.png","hash":"e6ddb88bf9d29a480ded59086f70f6880c6fd3cf","modified":1751018633451},{"_id":"public/img/virtuoso.png","hash":"a00ef13f5cfa7775af0c131c62c9c447044ef4b0","modified":1751018633451},{"_id":"public/img/midivae.png","hash":"c060f51c9eb72566f9ec5adcb7581167c6429948","modified":1751018633451},{"_id":"public/img/musicvae.png","hash":"e2d3f7d11481c160bc640d506ab0d8c86ca585d9","modified":1751018633451},{"_id":"public/img/ashis2.png","hash":"a89e324fb31fe350465a840c1c01d617ac91ec0c","modified":1751018633451},{"_id":"public/img/deep-analogy.png","hash":"cbe306dc2c8e08b1254ec41d80b2c277e5363e57","modified":1751018633451},{"_id":"source/_posts/annotated-music-transformer.md","hash":"2214dc198379b2ad76f21d3422ea9f962c606e4a","modified":1751018920376},{"_id":"source/img/new-relative-attention.png","hash":"a2c5013e332df7dc8086be591883d58bc4850b9c","modified":1592469569146},{"_id":"source/img/relative-local-attention-srel.png","hash":"96569fd5291975329f23f950ba1ed831b624c48e","modified":1592497269083},{"_id":"source/img/relative-attention.png","hash":"266bba2f1a135de1f30732985b8fe225cae0749f","modified":1592464556862},{"_id":"source/img/relative-local-attention.png","hash":"65fff103a7ae5bf506c4c0eeb8382a4bb4f5d8ef","modified":1592495938980},{"_id":"source/img/relative-attention-2.png","hash":"7d1540d295547a638177d1ba8ed78e900565db9f","modified":1592465454831},{"_id":"source/img/relative-local-attention-2.png","hash":"c75d4d366e4249f72b396d159365c0101316a998","modified":1592496645623},{"_id":"source/img/relative-local-attention-unmasked.png","hash":"ea943cfb1924f6987c94fadd954448dc8f004ce6","modified":1592534677329},{"_id":"source/img/music-transformer-results.png","hash":"7a9710bf6c7e31b528d8cd0269e29d1cad59e31d","modified":1592536234833},{"_id":"public/archives/2020/04/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/tags/Transformer/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/2020/04/01/annotated-music-transformer/index.html","hash":"88b1f809a4b67a0f9ca4fd3bca874917c16d774a","modified":1751019065293},{"_id":"public/img/new-relative-attention.png","hash":"a2c5013e332df7dc8086be591883d58bc4850b9c","modified":1751018633451},{"_id":"public/img/relative-local-attention.png","hash":"65fff103a7ae5bf506c4c0eeb8382a4bb4f5d8ef","modified":1751018633451},{"_id":"public/img/relative-local-attention-srel.png","hash":"96569fd5291975329f23f950ba1ed831b624c48e","modified":1751018633451},{"_id":"public/img/relative-attention-2.png","hash":"7d1540d295547a638177d1ba8ed78e900565db9f","modified":1751018633451},{"_id":"public/img/relative-attention.png","hash":"266bba2f1a135de1f30732985b8fe225cae0749f","modified":1751018633451},{"_id":"public/img/relative-local-attention-2.png","hash":"c75d4d366e4249f72b396d159365c0101316a998","modified":1751018633451},{"_id":"public/img/relative-local-attention-unmasked.png","hash":"ea943cfb1924f6987c94fadd954448dc8f004ce6","modified":1751018633451},{"_id":"public/img/music-transformer-results.png","hash":"7a9710bf6c7e31b528d8cd0269e29d1cad59e31d","modified":1751018633451},{"_id":"source/_posts/semi-supervised-music.md","hash":"14b3cae6acb3acd42583aa731087b7d1404ba8fb","modified":1751018972906},{"_id":"source/img/.DS_Store","hash":"d9ba8804da793a51e92e48fa05e86f45ebd66a00","modified":1748584726540},{"_id":"source/img/kingma-ssl.png","hash":"cad81409c536059666ba9a280ee5ba4aee411782","modified":1592559255521},{"_id":"source/img/radford-sentiment.png","hash":"42f1e71fe640e5fb42150ccd93d5b459da5e7696","modified":1592820108195},{"_id":"source/img/vade-ssl.png","hash":"8614f67849668b3deec6b661003cfe83da1b253a","modified":1592811491270},{"_id":"source/img/kingma-ssl-2.png","hash":"6ee03f31df4da02be259cf629a35d2c383fc449f","modified":1592797175323},{"_id":"source/img/kingma-ssl-3.png","hash":"1c5a92be6ec67ac7d001332271aa1d27c5c344ab","modified":1592806605473},{"_id":"public/archives/2020/05/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/2020/05/13/semi-supervised-music/index.html","hash":"9902712fa3ac82f374585e3e49342c4c6d4e647b","modified":1751019065293},{"_id":"public/img/radford-sentiment.png","hash":"42f1e71fe640e5fb42150ccd93d5b459da5e7696","modified":1751018633451},{"_id":"public/img/vade-ssl.png","hash":"8614f67849668b3deec6b661003cfe83da1b253a","modified":1751018633451},{"_id":"public/img/kingma-ssl-3.png","hash":"1c5a92be6ec67ac7d001332271aa1d27c5c344ab","modified":1751018633451},{"_id":"public/img/kingma-ssl.png","hash":"cad81409c536059666ba9a280ee5ba4aee411782","modified":1751018633451},{"_id":"public/img/kingma-ssl-2.png","hash":"6ee03f31df4da02be259cf629a35d2c383fc449f","modified":1751018633451},{"_id":"source/Resume_2020.pdf","hash":"f275e524fd8ee71d1b7f58ce6b48f60bad44e001","modified":1602869899464},{"_id":"source/_posts/conv_fourier.md","hash":"e499bceb94a658ca45a15c272c7b9361af5571ff","modified":1751018933306},{"_id":"source/publications/index.md","hash":"1a32031d68b3e54e0e3e839a602160d1883d03c4","modified":1751010021200},{"_id":"themes/aircloud/layout/publications.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1602869899567},{"_id":"source/img/stft.png","hash":"86df8714b98730bb24c18b42223dd06cbe6f11df","modified":1602869899528},{"_id":"source/img/istft.png","hash":"ffa95973d1878a0a14b3d213f56b9eecd15c18cd","modified":1602869899496},{"_id":"source/_posts/ismir_2020_pt2.md","hash":"eb30d3f34b5b4f454daa3c661f6e7a8c5da06eaa","modified":1602952400759},{"_id":"source/_posts/ismir_2020.md","hash":"7efaccd3d8ab16dac483ff343970156047913fb4","modified":1751018952826},{"_id":"source/img/ismir_edit.png","hash":"364d2ece9b5525213a1e700c0ef685784380e807","modified":1602912544063},{"_id":"source/img/ismir_jyun.png","hash":"dd056567e51500155e16ab11bc063fa7f3733017","modified":1602921833892},{"_id":"source/img/ismir_unets.png","hash":"ec6312fdeddaa5b3067d4a02efbd6f2afa84c5ee","modified":1602923135999},{"_id":"source/img/ismir_bebop.png","hash":"fd61564838e19bfb90b84c45299d7f70fe1b30d0","modified":1602921195272},{"_id":"source/img/ismir_transcription.png","hash":"dd055291d7f0b63d152ce8c6d6489bf6e9c883b1","modified":1602928208488},{"_id":"source/img/ismir_conn.png","hash":"3b42a3ed59f54cc8621531b7017cd8be56221392","modified":1602908710514},{"_id":"source/img/ismir_fadernets.png","hash":"1450922df5f3f5468e927f8d67b0214aad6db988","modified":1602921546811},{"_id":"source/img/ismir_doras.png","hash":"bc535670967f6b695c7e9eb2dde6275fa748d019","modified":1602928299007},{"_id":"source/img/ismir_dmel.png","hash":"6a3d3596a10da114493778877545a7213ea41fd5","modified":1602910694673},{"_id":"source/img/ismir_attr.png","hash":"79d0586a896e7bea1b05da1b1a917e371933f2a3","modified":1602907612266},{"_id":"source/img/ismir_jazz.png","hash":"65479baab8b04356c7e76609bb6937097c08ef6e","modified":1602921718171},{"_id":"source/img/ismir_singing.png","hash":"5ef1ae3a6e037cfb2a7aa9b55d3925c53bc7b3a6","modified":1602921937706},{"_id":"source/img/ismir_sketchnet.png","hash":"78223b97711bbbd786b7a57f550b97bf06912816","modified":1602921628844},{"_id":"source/img/ismir_furkan.png","hash":"5e074dd55d3458e09413408cf3472a5cbfdaea47","modified":1602928333389},{"_id":"source/img/ismir_phoneme2.png","hash":"e8909e3db86a9cd5e4f66b1e74f544dd517a5bda","modified":1602923649191},{"_id":"source/img/ismir_drumgan.png","hash":"e041aaaa9341ca7a028ed9a5d3de12625711e992","modified":1602922003887},{"_id":"source/img/ismir_interpretable.png","hash":"66256379a003a5d9d1eec436b6e3199605e1315a","modified":1602921375540},{"_id":"source/img/ismir_metric.png","hash":"dfc2618bd8481693be2441a2b4f6e2da90d6f280","modified":1602921885028},{"_id":"source/img/ismir_multitask.png","hash":"a0d498ac948f9571f5b1b24f822c9289e46eb00f","modified":1602924249584},{"_id":"source/img/ismir_spherical.png","hash":"29160a167b5deba7894a4adebf8efb2546028d18","modified":1602928179258},{"_id":"source/img/ismir_lottery.png","hash":"1587e48a6f7b0220bad0d30b7743fe39f867a913","modified":1602928264016},{"_id":"source/img/ismir_vocal.png","hash":"9a7ef49c936c53c9db2a1a47fa027ae2f98ebc68","modified":1602925018253},{"_id":"source/img/ismir_phoneme1.png","hash":"c79177db6ee7cdfefc54fcd2f70d017fe7470c68","modified":1602923566710},{"_id":"source/img/ismir_pianotree.png","hash":"292f049232ec4a4fbb9a692aae82c883ac4cd7d8","modified":1602921304799},{"_id":"public/publications/index.html","hash":"cf017648aed7f7b83980d0b8927d11e76fc21d68","modified":1751019065293},{"_id":"public/2020/07/24/conv_fourier/index.html","hash":"f7e3694bb4471e843a9fe3fbbb3b56a416126543","modified":1751019065293},{"_id":"public/tags/Music-Signal-Processing/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/tags/Music-Representation-Learning/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/archives/2020/07/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/tags/Music-Information-Retrieval/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/archives/2020/10/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/2020/10/17/ismir_2020/index.html","hash":"61ed0be7497895c8daadadac8dd6739c9c60bf22","modified":1751019065293},{"_id":"public/2020/10/17/ismir_2020_pt2/index.html","hash":"d90bab9080a6b12dfcca72a9141a5081e3a6bbfc","modified":1645371921535},{"_id":"public/img/ismir_edit.png","hash":"364d2ece9b5525213a1e700c0ef685784380e807","modified":1751018633451},{"_id":"public/img/ismir_jyun.png","hash":"dd056567e51500155e16ab11bc063fa7f3733017","modified":1751018633451},{"_id":"public/img/ismir_unets.png","hash":"ec6312fdeddaa5b3067d4a02efbd6f2afa84c5ee","modified":1751018633451},{"_id":"public/img/ismir_bebop.png","hash":"fd61564838e19bfb90b84c45299d7f70fe1b30d0","modified":1751018633451},{"_id":"public/img/ismir_transcription.png","hash":"dd055291d7f0b63d152ce8c6d6489bf6e9c883b1","modified":1751018633451},{"_id":"public/img/ismir_conn.png","hash":"3b42a3ed59f54cc8621531b7017cd8be56221392","modified":1751018633451},{"_id":"public/img/ismir_fadernets.png","hash":"1450922df5f3f5468e927f8d67b0214aad6db988","modified":1751018633451},{"_id":"public/img/ismir_doras.png","hash":"bc535670967f6b695c7e9eb2dde6275fa748d019","modified":1751018633451},{"_id":"public/img/ismir_sketchnet.png","hash":"78223b97711bbbd786b7a57f550b97bf06912816","modified":1751018633451},{"_id":"public/img/ismir_jazz.png","hash":"65479baab8b04356c7e76609bb6937097c08ef6e","modified":1751018633451},{"_id":"public/img/ismir_furkan.png","hash":"5e074dd55d3458e09413408cf3472a5cbfdaea47","modified":1751018633451},{"_id":"public/img/ismir_dmel.png","hash":"6a3d3596a10da114493778877545a7213ea41fd5","modified":1751018633451},{"_id":"public/img/ismir_attr.png","hash":"79d0586a896e7bea1b05da1b1a917e371933f2a3","modified":1751018633451},{"_id":"public/img/ismir_singing.png","hash":"5ef1ae3a6e037cfb2a7aa9b55d3925c53bc7b3a6","modified":1751018633451},{"_id":"public/img/ismir_drumgan.png","hash":"e041aaaa9341ca7a028ed9a5d3de12625711e992","modified":1751018633451},{"_id":"public/img/ismir_phoneme2.png","hash":"e8909e3db86a9cd5e4f66b1e74f544dd517a5bda","modified":1751018633451},{"_id":"public/img/ismir_lottery.png","hash":"1587e48a6f7b0220bad0d30b7743fe39f867a913","modified":1751018633451},{"_id":"public/img/ismir_vocal.png","hash":"9a7ef49c936c53c9db2a1a47fa027ae2f98ebc68","modified":1751018633451},{"_id":"public/img/ismir_spherical.png","hash":"29160a167b5deba7894a4adebf8efb2546028d18","modified":1751018633451},{"_id":"public/img/ismir_metric.png","hash":"dfc2618bd8481693be2441a2b4f6e2da90d6f280","modified":1751018633451},{"_id":"public/img/ismir_multitask.png","hash":"a0d498ac948f9571f5b1b24f822c9289e46eb00f","modified":1751018633451},{"_id":"public/img/ismir_interpretable.png","hash":"66256379a003a5d9d1eec436b6e3199605e1315a","modified":1751018633451},{"_id":"public/img/ismir_phoneme1.png","hash":"c79177db6ee7cdfefc54fcd2f70d017fe7470c68","modified":1751018633451},{"_id":"public/img/ismir_pianotree.png","hash":"292f049232ec4a4fbb9a692aae82c883ac4cd7d8","modified":1751018633451},{"_id":"source/_posts/param-pooling.md","hash":"b1a237aa506a77e06caabda27610eb5dd511f331","modified":1751018968033},{"_id":"source/img/mean-vs-max-pool.png","hash":"79ed22fdd7924a2375389830ab99f6eb08645f2b","modified":1606278632425},{"_id":"public/archives/2020/11/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/tags/Deep-Learning/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/2020/11/25/param-pooling/index.html","hash":"1e1c6499015750c1bdd5e1e1593e1491c472f920","modified":1751019065293},{"_id":"public/img/mean-vs-max-pool.png","hash":"79ed22fdd7924a2375389830ab99f6eb08645f2b","modified":1751018633451},{"_id":"source/_posts/challenge-csd.md","hash":"dcc9b8fb2b23e6e37f96e24152121187ce2dc3be","modified":1751018925832},{"_id":"source/img/csd-shingles.png","hash":"64ca2d1021c74502805c01ac7dc28d9029087b84","modified":1614705608141},{"_id":"source/img/csd-ml-ops.png","hash":"25a5597798234d7d6226e6d0b7e3ac0e4bfffde9","modified":1614703676074},{"_id":"source/img/csd-dominant.png","hash":"10c2b448c90f0c2f609952b7fc62783efd0345ec","modified":1614703939313},{"_id":"source/img/csd-hybrid.png","hash":"13521a1a300fcf5dd343d4aa3f82ac7d7c975520","modified":1614704370900},{"_id":"source/img/csd-bytecover.png","hash":"89fe60f67e666e26da7ad386fe84ebe5beeacc80","modified":1614704968059},{"_id":"source/img/csd-hpcp.png","hash":"853f4bb3e5da58bd5fde8d71c38516a4e2452e58","modified":1614704302666},{"_id":"public/tags/ML-in-Production/index.html","hash":"db92bb1c8f051028dff2d06f974a7c9ab341466c","modified":1751019065293},{"_id":"public/archives/page/2/index.html","hash":"6a61e70ee420aad02b5a7b88bfe68981e5e5908e","modified":1682177935787},{"_id":"public/archives/2021/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2021/02/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/2021/02/25/challenge-csd/index.html","hash":"d47d47d85aaaab6130e185bc0581ec24f9600c73","modified":1751019065293},{"_id":"public/img/csd-shingles.png","hash":"64ca2d1021c74502805c01ac7dc28d9029087b84","modified":1751018633451},{"_id":"public/img/csd-dominant.png","hash":"10c2b448c90f0c2f609952b7fc62783efd0345ec","modified":1751018633451},{"_id":"public/img/csd-ml-ops.png","hash":"25a5597798234d7d6226e6d0b7e3ac0e4bfffde9","modified":1751018633451},{"_id":"public/img/csd-hybrid.png","hash":"13521a1a300fcf5dd343d4aa3f82ac7d7c975520","modified":1751018633451},{"_id":"public/img/csd-bytecover.png","hash":"89fe60f67e666e26da7ad386fe84ebe5beeacc80","modified":1751018633451},{"_id":"public/img/csd-hpcp.png","hash":"853f4bb3e5da58bd5fde8d71c38516a4e2452e58","modified":1751018633451},{"_id":"source/_posts/ismir-2021.md","hash":"310a937023d48e8576f421946ec4277b85b4b0a2","modified":1751018961405},{"_id":"public/2022/02/19/ismir-2021/index.html","hash":"3ff9d17b8ce6b4b6556c048980b05564d0f5a879","modified":1751019065293},{"_id":"public/archives/2022/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2022/02/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"source/_posts/streaming-label-dsp.md","hash":"dd6f8e0e0ad7688966b8f2100879ba78893333f6","modified":1661089651486},{"_id":"themes/aircloud/source/css/custom.css","hash":"59f8ab58019a94f27f19cd7ea310541d1bdc128c","modified":1661089429175},{"_id":"public/css/custom.css","hash":"59f8ab58019a94f27f19cd7ea310541d1bdc128c","modified":1751018633451},{"_id":"public/Resume_2020.pdf","hash":"f275e524fd8ee71d1b7f58ce6b48f60bad44e001","modified":1677053392823},{"_id":"public/img/stft.png","hash":"86df8714b98730bb24c18b42223dd06cbe6f11df","modified":1751018633451},{"_id":"public/img/istft.png","hash":"ffa95973d1878a0a14b3d213f56b9eecd15c18cd","modified":1751018633451},{"_id":"source/_posts/generative-ai-direction.md","hash":"145bef9171f2044bdad783e318029a988f54596c","modified":1751018947215},{"_id":"public/2023/04/24/generative-ai-direction/index.html","hash":"13cf31d610a1d14e1bd2efe15ce7c00a47ae791b","modified":1751019065293},{"_id":"public/archives/2023/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2023/04/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"themes/aircloud/.DS_Store","hash":"f5bad1d3a85c7410ce461bf68f5dea1a48d013af","modified":1728312797167},{"_id":"themes/aircloud/layout/google-analytics.ejs","hash":"ede8cd640255aa8c4b60a5129d9802ecca9a7115","modified":1748584726700},{"_id":"source/_posts/annotated-rvc.md","hash":"f9e8ba4d3fc96da06014ec9ac4fe0affa5468be6","modified":1751015732473},{"_id":"source/_posts/exp-2.md","hash":"de05e47909851292eb160987c9febfc7aff46870","modified":1751018941552},{"_id":"source/img/ieee_fp.png","hash":"6dad43dfc713a161ef995f801983ec324a1f1f06","modified":1748584726568},{"_id":"source/img/contentvec.png","hash":"7be59cb0af26e09f34677d7e897577ced3a4aa35","modified":1748584726547},{"_id":"source/img/nsf.png","hash":"b62332bc3236f1f1646db20fd44f0d779d81c88c","modified":1748584726643},{"_id":"source/img/rvc-infer.png","hash":"22d8837aaf1f17440dd8dace4e6002d9e10f1f49","modified":1748584726651},{"_id":"source/img/rvc-train.png","hash":"b4d2b563966f4832abb9846ab6d922e847337caa","modified":1748584726653},{"_id":"public/2024/01/02/exp-2/index.html","hash":"243654c2cbeac93c9b3ed328098796b295aff6da","modified":1751019065293},{"_id":"public/2024/09/26/annotated-rvc/index.html","hash":"e3a7176fda6a649fb0c903313f9369c64ac61c9a","modified":1751019065293},{"_id":"public/archives/2024/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2024/01/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/archives/2024/09/index.html","hash":"2b6307e421f3495e931f0b2d2deb65135a5c0cf1","modified":1751019065293},{"_id":"public/img/ieee_fp.png","hash":"6dad43dfc713a161ef995f801983ec324a1f1f06","modified":1751018633451},{"_id":"public/img/contentvec.png","hash":"7be59cb0af26e09f34677d7e897577ced3a4aa35","modified":1751018633451},{"_id":"public/img/nsf.png","hash":"b62332bc3236f1f1646db20fd44f0d779d81c88c","modified":1751018633451},{"_id":"public/img/rvc-infer.png","hash":"22d8837aaf1f17440dd8dace4e6002d9e10f1f49","modified":1751018633451},{"_id":"public/img/rvc-train.png","hash":"b4d2b563966f4832abb9846ab6d922e847337caa","modified":1751018633451}],"Category":[],"Data":[],"Page":[{"layout":"about","title":"About","date":"2016-04-20T20:48:33.000Z","comments":1,"_content":"## About Me\n\nMy name is Hao Hao Tan (郑豪好), or you can call me Harry. I am from Kuala Lumpur, Malaysia, and currently residing in Singapore.\n\nI currently work on AI-powered features at [BandLab](https://www.bandlab.com/) to enhance music creation workflow, e.g. *SongStarter*, *Voice Changer*, *Splitter*, *MIDI tools*, *Audio-to-MIDI*, *Similar Sounds*, etc. I also work on [on-device model serving](https://mlsysbook.ai/contents/ondevice_learning/ondevice_learning.html) and [optimization](https://mlsysbook.ai/contents/optimizations/optimizations.html) -- some of the features above run directly on users' mobile device!\n\nI'm passionate about building technologies that <u>enhance creative applications</u>. My [bachelor's thesis](https://www.computationalcreativity.net/iccc2019/assets/creative-submissions/iccc19-tan-chordal.pdf) was about a chord-based music generation system using LSTMs. Then, I worked on controllable music generation and audio synthesis at [Prof. Dorien Herremans' lab](https://dorienherremans.com/team). I later joined [Fairphonic](https://www.fairphonic.com) and built a deep learning–based music content ID system (originals and covers included!) to fix issues on rights attribution. Before joining BandLab, I did backend engineering at TikTok on its (massive) feed recommendation system.\n\nSometimes I work on independent research / open-source projects, you can find them on my [GitHub page](https://github.com/gudgud96). I'm generally interested about topics related to audio, synthesizers, (fun ways of) search & retrieval, and small on-device models.\n\nI am a classically trained pianist, and I listen to all kinds of music ranging from [Cantonese classics](https://open.spotify.com/album/0ExJszOInX2i1J2UQ4hNCF?si=9wStJzo9QamSbTpWvD9CqQ), [Edvard Grieg](https://open.spotify.com/playlist/0XkyuCwxp8EtnDZ5VNieS1?si=e1bc1592aa1549a3) to [pure electronic beats](https://open.spotify.com/artist/5NlawbBDGkH8W9SblamHZO?si=7zFP3AFjSWq8Bbpf1PhvQQ). Stay tuned for [**some awesome playlists**]() that I made.\n\nFeel free reach me at `helloharry66` [at] `gmail` or [@GoodGood014](https://twitter.com/GoodGood014) on Twitter.","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About\"\ndate: 2016-04-21 04:48:33\ncomments: true\n---\n## About Me\n\nMy name is Hao Hao Tan (郑豪好), or you can call me Harry. I am from Kuala Lumpur, Malaysia, and currently residing in Singapore.\n\nI currently work on AI-powered features at [BandLab](https://www.bandlab.com/) to enhance music creation workflow, e.g. *SongStarter*, *Voice Changer*, *Splitter*, *MIDI tools*, *Audio-to-MIDI*, *Similar Sounds*, etc. I also work on [on-device model serving](https://mlsysbook.ai/contents/ondevice_learning/ondevice_learning.html) and [optimization](https://mlsysbook.ai/contents/optimizations/optimizations.html) -- some of the features above run directly on users' mobile device!\n\nI'm passionate about building technologies that <u>enhance creative applications</u>. My [bachelor's thesis](https://www.computationalcreativity.net/iccc2019/assets/creative-submissions/iccc19-tan-chordal.pdf) was about a chord-based music generation system using LSTMs. Then, I worked on controllable music generation and audio synthesis at [Prof. Dorien Herremans' lab](https://dorienherremans.com/team). I later joined [Fairphonic](https://www.fairphonic.com) and built a deep learning–based music content ID system (originals and covers included!) to fix issues on rights attribution. Before joining BandLab, I did backend engineering at TikTok on its (massive) feed recommendation system.\n\nSometimes I work on independent research / open-source projects, you can find them on my [GitHub page](https://github.com/gudgud96). I'm generally interested about topics related to audio, synthesizers, (fun ways of) search & retrieval, and small on-device models.\n\nI am a classically trained pianist, and I listen to all kinds of music ranging from [Cantonese classics](https://open.spotify.com/album/0ExJszOInX2i1J2UQ4hNCF?si=9wStJzo9QamSbTpWvD9CqQ), [Edvard Grieg](https://open.spotify.com/playlist/0XkyuCwxp8EtnDZ5VNieS1?si=e1bc1592aa1549a3) to [pure electronic beats](https://open.spotify.com/artist/5NlawbBDGkH8W9SblamHZO?si=7zFP3AFjSWq8Bbpf1PhvQQ). Stay tuned for [**some awesome playlists**]() that I made.\n\nFeel free reach me at `helloharry66` [at] `gmail` or [@GoodGood014](https://twitter.com/GoodGood014) on Twitter.","updated":"2025-06-27T10:02:01.987Z","path":"about/index.html","_id":"ck5ryykx7000011v5f6uj62kx","content":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>My name is Hao Hao Tan (郑豪好), or you can call me Harry. I am from Kuala Lumpur, Malaysia, and currently residing in Singapore.</p>\n<p>I currently work on AI-powered features at <a href=\"https://www.bandlab.com/\" target=\"_blank\" rel=\"noopener\">BandLab</a> to enhance music creation workflow, e.g. <em>SongStarter</em>, <em>Voice Changer</em>, <em>Splitter</em>, <em>MIDI tools</em>, <em>Audio-to-MIDI</em>, <em>Similar Sounds</em>, etc. I also work on <a href=\"https://mlsysbook.ai/contents/ondevice_learning/ondevice_learning.html\" target=\"_blank\" rel=\"noopener\">on-device model serving</a> and <a href=\"https://mlsysbook.ai/contents/optimizations/optimizations.html\" target=\"_blank\" rel=\"noopener\">optimization</a> – some of the features above run directly on users’ mobile device!</p>\n<p>I’m passionate about building technologies that <u>enhance creative applications</u>. My <a href=\"https://www.computationalcreativity.net/iccc2019/assets/creative-submissions/iccc19-tan-chordal.pdf\" target=\"_blank\" rel=\"noopener\">bachelor’s thesis</a> was about a chord-based music generation system using LSTMs. Then, I worked on controllable music generation and audio synthesis at <a href=\"https://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Prof. Dorien Herremans’ lab</a>. I later joined <a href=\"https://www.fairphonic.com\" target=\"_blank\" rel=\"noopener\">Fairphonic</a> and built a deep learning–based music content ID system (originals and covers included!) to fix issues on rights attribution. Before joining BandLab, I did backend engineering at TikTok on its (massive) feed recommendation system.</p>\n<p>Sometimes I work on independent research / open-source projects, you can find them on my <a href=\"https://github.com/gudgud96\" target=\"_blank\" rel=\"noopener\">GitHub page</a>. I’m generally interested about topics related to audio, synthesizers, (fun ways of) search &amp; retrieval, and small on-device models.</p>\n<p>I am a classically trained pianist, and I listen to all kinds of music ranging from <a href=\"https://open.spotify.com/album/0ExJszOInX2i1J2UQ4hNCF?si=9wStJzo9QamSbTpWvD9CqQ\" target=\"_blank\" rel=\"noopener\">Cantonese classics</a>, <a href=\"https://open.spotify.com/playlist/0XkyuCwxp8EtnDZ5VNieS1?si=e1bc1592aa1549a3\" target=\"_blank\" rel=\"noopener\">Edvard Grieg</a> to <a href=\"https://open.spotify.com/artist/5NlawbBDGkH8W9SblamHZO?si=7zFP3AFjSWq8Bbpf1PhvQQ\" target=\"_blank\" rel=\"noopener\">pure electronic beats</a>. Stay tuned for <a href=\"\"><strong>some awesome playlists</strong></a> that I made.</p>\n<p>Feel free reach me at <code>helloharry66</code> [at] <code>gmail</code> or <a href=\"https://twitter.com/GoodGood014\" target=\"_blank\" rel=\"noopener\">@GoodGood014</a> on Twitter.</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>My name is Hao Hao Tan (郑豪好), or you can call me Harry. I am from Kuala Lumpur, Malaysia, and currently residing in Singapore.</p>\n<p>I currently work on AI-powered features at <a href=\"https://www.bandlab.com/\" target=\"_blank\" rel=\"noopener\">BandLab</a> to enhance music creation workflow, e.g. <em>SongStarter</em>, <em>Voice Changer</em>, <em>Splitter</em>, <em>MIDI tools</em>, <em>Audio-to-MIDI</em>, <em>Similar Sounds</em>, etc. I also work on <a href=\"https://mlsysbook.ai/contents/ondevice_learning/ondevice_learning.html\" target=\"_blank\" rel=\"noopener\">on-device model serving</a> and <a href=\"https://mlsysbook.ai/contents/optimizations/optimizations.html\" target=\"_blank\" rel=\"noopener\">optimization</a> – some of the features above run directly on users’ mobile device!</p>\n<p>I’m passionate about building technologies that <u>enhance creative applications</u>. My <a href=\"https://www.computationalcreativity.net/iccc2019/assets/creative-submissions/iccc19-tan-chordal.pdf\" target=\"_blank\" rel=\"noopener\">bachelor’s thesis</a> was about a chord-based music generation system using LSTMs. Then, I worked on controllable music generation and audio synthesis at <a href=\"https://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Prof. Dorien Herremans’ lab</a>. I later joined <a href=\"https://www.fairphonic.com\" target=\"_blank\" rel=\"noopener\">Fairphonic</a> and built a deep learning–based music content ID system (originals and covers included!) to fix issues on rights attribution. Before joining BandLab, I did backend engineering at TikTok on its (massive) feed recommendation system.</p>\n<p>Sometimes I work on independent research / open-source projects, you can find them on my <a href=\"https://github.com/gudgud96\" target=\"_blank\" rel=\"noopener\">GitHub page</a>. I’m generally interested about topics related to audio, synthesizers, (fun ways of) search &amp; retrieval, and small on-device models.</p>\n<p>I am a classically trained pianist, and I listen to all kinds of music ranging from <a href=\"https://open.spotify.com/album/0ExJszOInX2i1J2UQ4hNCF?si=9wStJzo9QamSbTpWvD9CqQ\" target=\"_blank\" rel=\"noopener\">Cantonese classics</a>, <a href=\"https://open.spotify.com/playlist/0XkyuCwxp8EtnDZ5VNieS1?si=e1bc1592aa1549a3\" target=\"_blank\" rel=\"noopener\">Edvard Grieg</a> to <a href=\"https://open.spotify.com/artist/5NlawbBDGkH8W9SblamHZO?si=7zFP3AFjSWq8Bbpf1PhvQQ\" target=\"_blank\" rel=\"noopener\">pure electronic beats</a>. Stay tuned for <a href=\"\"><strong>some awesome playlists</strong></a> that I made.</p>\n<p>Feel free reach me at <code>helloharry66</code> [at] <code>gmail</code> or <a href=\"https://twitter.com/GoodGood014\" target=\"_blank\" rel=\"noopener\">@GoodGood014</a> on Twitter.</p>\n"},{"layout":"tags","title":"Tags","_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\n---","date":"2020-01-24T09:45:10.818Z","updated":"2020-01-24T09:45:10.818Z","path":"tags/index.html","comments":1,"_id":"ck5rzbz8j00008dv5cddl8ema","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"Blog List","date":"2020-01-22T20:48:33.000Z","comments":1,"_content":"## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\nSander Dieleman: https://benanne.github.io/\nJustin Salamon: https://www.justinsalamon.com/news\nJordi Pons: http://www.jordipons.me/\nIlaria Manco: https://ilariamanco.com/\n\n### ML Systems / Engineering\nChip Huyen: https://huyenchip.com/\nEugene Yan: https://eugeneyan.com/\nShreya Shankar: https://www.shreya-shankar.com/\nNetflix Blog: https://netflixtechblog.com/\nAssaf Pinhasi: https://medium.com/@assaf.pinhasi\nSystems in ML reading list: https://xzhu0027.gitbook.io/blog/reading-list\nmadawei: https://www.bmpi.dev/dev/\nML Compilation by Tianqi Chen: https://mlc.ai/summer22/\n\n### Audio / Music Programming\nJUCE: https://juce.com/learn/tutorials\nRoss Bencina: http://www.rossbencina.com/code\n\n### Other Wonderful Folks\nThomas Kipf on Graph DL: http://tkipf.github.io/\nChaitanya Joshi on Graph DL: https://chaitjo.github.io/\nCyanide on SWE: https://kemingy.github.io/\n","source":"bloglist/index.md","raw":"---\nlayout: \"about\"\ntitle: \"Blog List\"\ndate: 2020-01-23 04:48:33\ncomments: true\n---\n## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\nSander Dieleman: https://benanne.github.io/\nJustin Salamon: https://www.justinsalamon.com/news\nJordi Pons: http://www.jordipons.me/\nIlaria Manco: https://ilariamanco.com/\n\n### ML Systems / Engineering\nChip Huyen: https://huyenchip.com/\nEugene Yan: https://eugeneyan.com/\nShreya Shankar: https://www.shreya-shankar.com/\nNetflix Blog: https://netflixtechblog.com/\nAssaf Pinhasi: https://medium.com/@assaf.pinhasi\nSystems in ML reading list: https://xzhu0027.gitbook.io/blog/reading-list\nmadawei: https://www.bmpi.dev/dev/\nML Compilation by Tianqi Chen: https://mlc.ai/summer22/\n\n### Audio / Music Programming\nJUCE: https://juce.com/learn/tutorials\nRoss Bencina: http://www.rossbencina.com/code\n\n### Other Wonderful Folks\nThomas Kipf on Graph DL: http://tkipf.github.io/\nChaitanya Joshi on Graph DL: https://chaitjo.github.io/\nCyanide on SWE: https://kemingy.github.io/\n","updated":"2022-08-21T13:43:48.999Z","path":"bloglist/index.html","_id":"ck5s0x47j0000w5v57s73193s","content":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a><br>Sander Dieleman: <a href=\"https://benanne.github.io/\" target=\"_blank\" rel=\"noopener\">https://benanne.github.io/</a><br>Justin Salamon: <a href=\"https://www.justinsalamon.com/news\" target=\"_blank\" rel=\"noopener\">https://www.justinsalamon.com/news</a><br>Jordi Pons: <a href=\"http://www.jordipons.me/\" target=\"_blank\" rel=\"noopener\">http://www.jordipons.me/</a><br>Ilaria Manco: <a href=\"https://ilariamanco.com/\" target=\"_blank\" rel=\"noopener\">https://ilariamanco.com/</a></p>\n<h3 id=\"ML-Systems-Engineering\"><a href=\"#ML-Systems-Engineering\" class=\"headerlink\" title=\"ML Systems / Engineering\"></a>ML Systems / Engineering</h3><p>Chip Huyen: <a href=\"https://huyenchip.com/\" target=\"_blank\" rel=\"noopener\">https://huyenchip.com/</a><br>Eugene Yan: <a href=\"https://eugeneyan.com/\" target=\"_blank\" rel=\"noopener\">https://eugeneyan.com/</a><br>Shreya Shankar: <a href=\"https://www.shreya-shankar.com/\" target=\"_blank\" rel=\"noopener\">https://www.shreya-shankar.com/</a><br>Netflix Blog: <a href=\"https://netflixtechblog.com/\" target=\"_blank\" rel=\"noopener\">https://netflixtechblog.com/</a><br>Assaf Pinhasi: <a href=\"https://medium.com/@assaf.pinhasi\" target=\"_blank\" rel=\"noopener\">https://medium.com/@assaf.pinhasi</a><br>Systems in ML reading list: <a href=\"https://xzhu0027.gitbook.io/blog/reading-list\" target=\"_blank\" rel=\"noopener\">https://xzhu0027.gitbook.io/blog/reading-list</a><br>madawei: <a href=\"https://www.bmpi.dev/dev/\" target=\"_blank\" rel=\"noopener\">https://www.bmpi.dev/dev/</a><br>ML Compilation by Tianqi Chen: <a href=\"https://mlc.ai/summer22/\" target=\"_blank\" rel=\"noopener\">https://mlc.ai/summer22/</a></p>\n<h3 id=\"Audio-Music-Programming\"><a href=\"#Audio-Music-Programming\" class=\"headerlink\" title=\"Audio / Music Programming\"></a>Audio / Music Programming</h3><p>JUCE: <a href=\"https://juce.com/learn/tutorials\" target=\"_blank\" rel=\"noopener\">https://juce.com/learn/tutorials</a><br>Ross Bencina: <a href=\"http://www.rossbencina.com/code\" target=\"_blank\" rel=\"noopener\">http://www.rossbencina.com/code</a></p>\n<h3 id=\"Other-Wonderful-Folks\"><a href=\"#Other-Wonderful-Folks\" class=\"headerlink\" title=\"Other Wonderful Folks\"></a>Other Wonderful Folks</h3><p>Thomas Kipf on Graph DL: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi on Graph DL: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a><br>Cyanide on SWE: <a href=\"https://kemingy.github.io/\" target=\"_blank\" rel=\"noopener\">https://kemingy.github.io/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a><br>Sander Dieleman: <a href=\"https://benanne.github.io/\" target=\"_blank\" rel=\"noopener\">https://benanne.github.io/</a><br>Justin Salamon: <a href=\"https://www.justinsalamon.com/news\" target=\"_blank\" rel=\"noopener\">https://www.justinsalamon.com/news</a><br>Jordi Pons: <a href=\"http://www.jordipons.me/\" target=\"_blank\" rel=\"noopener\">http://www.jordipons.me/</a><br>Ilaria Manco: <a href=\"https://ilariamanco.com/\" target=\"_blank\" rel=\"noopener\">https://ilariamanco.com/</a></p>\n<h3 id=\"ML-Systems-Engineering\"><a href=\"#ML-Systems-Engineering\" class=\"headerlink\" title=\"ML Systems / Engineering\"></a>ML Systems / Engineering</h3><p>Chip Huyen: <a href=\"https://huyenchip.com/\" target=\"_blank\" rel=\"noopener\">https://huyenchip.com/</a><br>Eugene Yan: <a href=\"https://eugeneyan.com/\" target=\"_blank\" rel=\"noopener\">https://eugeneyan.com/</a><br>Shreya Shankar: <a href=\"https://www.shreya-shankar.com/\" target=\"_blank\" rel=\"noopener\">https://www.shreya-shankar.com/</a><br>Netflix Blog: <a href=\"https://netflixtechblog.com/\" target=\"_blank\" rel=\"noopener\">https://netflixtechblog.com/</a><br>Assaf Pinhasi: <a href=\"https://medium.com/@assaf.pinhasi\" target=\"_blank\" rel=\"noopener\">https://medium.com/@assaf.pinhasi</a><br>Systems in ML reading list: <a href=\"https://xzhu0027.gitbook.io/blog/reading-list\" target=\"_blank\" rel=\"noopener\">https://xzhu0027.gitbook.io/blog/reading-list</a><br>madawei: <a href=\"https://www.bmpi.dev/dev/\" target=\"_blank\" rel=\"noopener\">https://www.bmpi.dev/dev/</a><br>ML Compilation by Tianqi Chen: <a href=\"https://mlc.ai/summer22/\" target=\"_blank\" rel=\"noopener\">https://mlc.ai/summer22/</a></p>\n<h3 id=\"Audio-Music-Programming\"><a href=\"#Audio-Music-Programming\" class=\"headerlink\" title=\"Audio / Music Programming\"></a>Audio / Music Programming</h3><p>JUCE: <a href=\"https://juce.com/learn/tutorials\" target=\"_blank\" rel=\"noopener\">https://juce.com/learn/tutorials</a><br>Ross Bencina: <a href=\"http://www.rossbencina.com/code\" target=\"_blank\" rel=\"noopener\">http://www.rossbencina.com/code</a></p>\n<h3 id=\"Other-Wonderful-Folks\"><a href=\"#Other-Wonderful-Folks\" class=\"headerlink\" title=\"Other Wonderful Folks\"></a>Other Wonderful Folks</h3><p>Thomas Kipf on Graph DL: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi on Graph DL: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a><br>Cyanide on SWE: <a href=\"https://kemingy.github.io/\" target=\"_blank\" rel=\"noopener\">https://kemingy.github.io/</a></p>\n"},{"layout":"publications","title":"Publications","date":"2020-01-22T20:48:33.000Z","comments":1,"_content":"## Publications / Talks\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ A Gentle Introduction to Music Content Identification</mark></h3>\n<i>NUS Hackers Talk, October 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1mcDlhuILZtzZqOhu-UPyyZB7WYstHHCqvbU2CQ2avWk/edit?usp=sharing\n    \">SLIDES</a>\n</div>\n<br/>\n\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage</mark></h3>\n<ins>Hao Hao Tan</ins>, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji.<br/>\n<i>Preprint, under review</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/abs/2403.10024\">arXiv</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Exciting Trends in Music Technology with Deep Learning</mark></h3>\n<i>NUS Hackers Talk, March 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=j_RL7m0n1_I&t=286s&ab_channel=NUSHackers\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1c1S5GmVecDmIlEQiO2acMa2VjQKzUmUgdaE2WxndJMM/edit?usp=sharing\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Parameter Inference of Music Synthesizers using Deep Learning</mark></h3>\n<i>Audio Developer Conference (ADC) 2022</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=nZ560W6bA3o\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1PA4fom6QvCW_YG8L0MMVumrAluljcymndNlaK2HW5t0/edit\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles</mark></h3>\n<i>MediaEval 2021, Emotions and Themes in Music Challenge</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2112.00702.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/noisy-student-emotion-training\">CODE</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\">DEMO</a>\n</div>","source":"publications/index.md","raw":"---\nlayout: \"publications\"\ntitle: \"Publications\"\ndate: 2020-01-23 04:48:33\ncomments: true\n---\n## Publications / Talks\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ A Gentle Introduction to Music Content Identification</mark></h3>\n<i>NUS Hackers Talk, October 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1mcDlhuILZtzZqOhu-UPyyZB7WYstHHCqvbU2CQ2avWk/edit?usp=sharing\n    \">SLIDES</a>\n</div>\n<br/>\n\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage</mark></h3>\n<ins>Hao Hao Tan</ins>, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji.<br/>\n<i>Preprint, under review</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/abs/2403.10024\">arXiv</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Exciting Trends in Music Technology with Deep Learning</mark></h3>\n<i>NUS Hackers Talk, March 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=j_RL7m0n1_I&t=286s&ab_channel=NUSHackers\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1c1S5GmVecDmIlEQiO2acMa2VjQKzUmUgdaE2WxndJMM/edit?usp=sharing\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Parameter Inference of Music Synthesizers using Deep Learning</mark></h3>\n<i>Audio Developer Conference (ADC) 2022</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=nZ560W6bA3o\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1PA4fom6QvCW_YG8L0MMVumrAluljcymndNlaK2HW5t0/edit\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles</mark></h3>\n<i>MediaEval 2021, Emotions and Themes in Music Challenge</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2112.00702.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/noisy-student-emotion-training\">CODE</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\">DEMO</a>\n</div>","updated":"2025-06-27T07:40:21.200Z","path":"publications/index.html","_id":"ckgcjd4cv0001w19k2zkg3ytc","content":"<h2 id=\"Publications-Talks\"><a href=\"#Publications-Talks\" class=\"headerlink\" title=\"Publications / Talks\"></a>Publications / Talks</h2><h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ A Gentle Introduction to Music Content Identification</mark></h3>\n<i>NUS Hackers Talk, October 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1mcDlhuILZtzZqOhu-UPyyZB7WYstHHCqvbU2CQ2avWk/edit?usp=sharing\n    \">SLIDES</a>\n</div>\n<br/>\n\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage</mark></h3>\n<ins>Hao Hao Tan</ins>, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji.<br/>\n<i>Preprint, under review</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/abs/2403.10024\" target=\"_blank\" rel=\"noopener\">arXiv</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Exciting Trends in Music Technology with Deep Learning</mark></h3>\n<i>NUS Hackers Talk, March 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=j_RL7m0n1_I&t=286s&ab_channel=NUSHackers\" target=\"_blank\" rel=\"noopener\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1c1S5GmVecDmIlEQiO2acMa2VjQKzUmUgdaE2WxndJMM/edit?usp=sharing\" target=\"_blank\" rel=\"noopener\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Parameter Inference of Music Synthesizers using Deep Learning</mark></h3>\n<i>Audio Developer Conference (ADC) 2022</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=nZ560W6bA3o\" target=\"_blank\" rel=\"noopener\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1PA4fom6QvCW_YG8L0MMVumrAluljcymndNlaK2HW5t0/edit\" target=\"_blank\" rel=\"noopener\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles</mark></h3>\n<i>MediaEval 2021, Emotions and Themes in Music Challenge</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2112.00702.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">CODE</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Publications-Talks\"><a href=\"#Publications-Talks\" class=\"headerlink\" title=\"Publications / Talks\"></a>Publications / Talks</h2><h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ A Gentle Introduction to Music Content Identification</mark></h3>\n<i>NUS Hackers Talk, October 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1mcDlhuILZtzZqOhu-UPyyZB7WYstHHCqvbU2CQ2avWk/edit?usp=sharing\n    \">SLIDES</a>\n</div>\n<br/>\n\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage</mark></h3>\n<ins>Hao Hao Tan</ins>, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki Mitsufuji.<br/>\n<i>Preprint, under review</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/abs/2403.10024\" target=\"_blank\" rel=\"noopener\">arXiv</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Exciting Trends in Music Technology with Deep Learning</mark></h3>\n<i>NUS Hackers Talk, March 2024</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=j_RL7m0n1_I&t=286s&ab_channel=NUSHackers\" target=\"_blank\" rel=\"noopener\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1c1S5GmVecDmIlEQiO2acMa2VjQKzUmUgdaE2WxndJMM/edit?usp=sharing\" target=\"_blank\" rel=\"noopener\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">🗣️ Parameter Inference of Music Synthesizers using Deep Learning</mark></h3>\n<i>Audio Developer Conference (ADC) 2022</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://www.youtube.com/watch?v=nZ560W6bA3o\" target=\"_blank\" rel=\"noopener\">VIDEO</a>\n    <a class=\"item\" href=\"https://docs.google.com/presentation/d/1PA4fom6QvCW_YG8L0MMVumrAluljcymndNlaK2HW5t0/edit\" target=\"_blank\" rel=\"noopener\">SLIDES</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles</mark></h3>\n<i>MediaEval 2021, Emotions and Themes in Music Challenge</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2112.00702.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">CODE</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">📄 ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>"}],"Post":[{"title":"Where Could AI Music Possibly Head Towards?","date":"2018-09-24T08:52:50.000Z","_content":"\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","source":"_posts/ai-music-direction.md","raw":"---\ntitle: Where Could AI Music Possibly Head Towards?\ndate: 2018-09-24 16:52:50\ntags:\n    - General Thoughts\n---\n\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","slug":"ai-music-direction","published":1,"updated":"2020-06-19T06:37:14.917Z","_id":"ck5s3jzfj00004qv5g9865rpz","comments":1,"layout":"post","photos":[],"link":"","content":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n"},{"title":"VAE In Symbolic Music Modelling","date":"2020-01-26T09:54:50.000Z","estimatedReadTime":"~10 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","source":"_posts/vae-symbolic-music.md","raw":"---\ntitle: VAE In Symbolic Music Modelling\ndate: 2020-01-26 17:54:50\ntags:\n    - VAE\n    - Symbolic Music\n    - Music Representation Learning\nestimatedReadTime: ~10 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","slug":"vae-symbolic-music","published":1,"updated":"2025-06-27T10:09:38.787Z","_id":"ck89oi0it0000tbm8hg9jhjt3","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n"},{"title":"Understanding Music Transformer","date":"2020-04-01T09:54:50.000Z","estimatedReadTime":"~20 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Concepts discussed in the Music Transformer paper\n2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention\n3 - Ideas in the paper to efficiently implement the above attention mechanisms\n4 - Results on music generation\n\n## 1 - Introduction \nI personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the \"skewing\" procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.\n\n## 2 - Background of the Music Transformer\n\nThe [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf) paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. \n\nThe idea is as follows: by modelling each **performance event** as a token (as proposed in [Oore et al.](https://arxiv.org/pdf/1808.03715)), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the [demo samples](https://magenta.tensorflow.org/music-transformer) provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.\n\nSome related work of using Transformer architecture on generating music include [MuseNet](https://openai.com/blog/musenet/) (from OpenAI), and also [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf). It is evident that the Transformer architecture would be the backbone of music generation models in future research.\n\nFor the basics of Transformers, we refer the readers who are interested to these links: \n- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP;\n- [Understanding and Applying Self-Attention for NLP Video](https://www.youtube.com/watch?v=OYygPG4d9H0);\n- [Transformer Video by Hung-yi Lee](https://www.youtube.com/watch?v=ugWDIIOHtPA), Mandarin version;\n- [Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/), a great blog post by Chaitanya Joshi.\n\n## 3 - Transformer with Relative Attention\n\nIn my opinion, the Music Transformer paper is not only an application work, but its crux also includes *optimization* work on implementing Transformer with **relative attention**. We will delve into this part below.\n\nThe essence of Transformer is on **self-attention**: for an output sequence of the Transformer, the elements on each position is a result of \"attending to\" (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$\n\nwhere \\\\(Q, K, V\\\\) represents the query, key and value tensors, each having tensor shape \\\\((l, d)\\\\), with \\\\(l\\\\) and \\\\(d\\\\) representing the sequence length and the number of dimensions used in the model respectively.\n\nIn [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), the concept of **relative attention** is proposed. The idea is to represent **relative position representations** more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$\n\nThe difference is to learn an additional tensor \\\\(S^{rel}\\\\) of shape \\\\((l, l)\\\\). From the shape itself, we can see that the values \\\\(v_{ij}\\\\) in \\\\(S^{rel}\\\\) must be related to the relative distance of position \\\\(i\\\\) and \\\\(j\\\\) in length \\\\(l\\\\). \nIn fact, we can also view a sequence as a **fully connected graph** of its elements, hence \\\\(S^{rel}\\\\) can be seen as a tensor representing information about the **edges** in the graph.  \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\nHow do we learn the values in \\\\(S^{rel}\\\\)? In Shaw et al., this is done by learning an extra weight tensor \\\\(R\\\\) of shape \\\\((l, l, d)\\\\) -- only 1 dimension extra as compared to \\\\(S^{rel}\\\\). We can see it as having an **embedding** of dimension \\\\(d\\\\), at each position in a distance matrix of shape \\\\((l, l)\\\\). The values \\\\(R_{i, j}\\\\) means the embeddings which encode the relative position between position \\\\(i\\\\) from query tensor,  and position \\\\(j\\\\) from key tensor. Here, we only discuss cases of **masked self-attention** without looking forward -- only positions where \\\\(i \\ge j\\\\) is concerned because the model only attends to nodes that occur at the current time step or before.\n\nAfter that, we reshape \\\\(Q\\\\) into a \\\\((l, 1, d)\\\\) tensor, and multiply by $$S^{rel} = QR^T$$\n\nHowever, this incurs a total space complexity of \\\\(O(L^2 D)\\\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to \"perform some algorithmic tricks\" in order to obtain \\\\(S^{rel}\\\\) in space complexity of \\\\(O(LD)\\\\).\n\n## 4 - Relative Global Attention\n\nIn Music Transformer, the number of unique embeddings in \\\\(E^r\\\\) is assumed to be fixed. They used \\\\(l\\\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\\\(l - 1\\\\), from first position to the \\\\(l^{th}\\\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the \"edge\" between 1st and 3rd element, is the same as the embedding for the \"edge\" between 2nd and 4th element. \n\nIf we follow the assumptions made above, we can observe that the desired \\\\(S_{rel}\\\\) can actually be calculated from multiplying \\\\(Q{E^{r}}^T\\\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\\\(S_{rel}\\\\) from \\\\(Q{E^{r}}^T\\\\).\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\nThe bottom row in Figure 2 is known as the \"skewing\" procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\\\(M_\\textrm{masked}\\\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\\\(S_{rel}\\\\).\n\nHence, without using the gigantic \\\\(R\\\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\\\(O(LD)\\\\) space complexity. Although both implementations result in \\\\(O(L^{2}D)\\\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.\n\n## 4 - Relative Local Attention\n\n**Local attention**, first coined in [Luong et al](https://arxiv.org/pdf/1508.04025) and [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf), means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens *nearby* at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several *non-overlapping* blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a *memory* block which includes the current block and a finite number of tokens before the block).\n\nFor simplicity, let's start with an example of dividing the input sequence of 6 tokens into 2 blocks -- Block 1 \\\\([x_1, x_2, x_3]\\\\), and Block 2 \\\\([x_4, x_5, x_6]\\\\). Hence, the attention components include: Block 1 \\\\(\\leftrightarrow\\\\) Block 1, Block 2 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2, as shown in Figure 2.\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\nThe difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\\\(\\leftrightarrow\\\\) Block 1 as in global attention.\n\nSo, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\\\(S_{rel}\\\\) which can capture relative position information, which similarly we can obtain the values from \\\\(Q{E^r}^T\\\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\\\(S_{rel}\\\\) look like for 2 blocks:\n\n<figure>\n  <img style=\"width:40%; margin-top: 20px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\nFrom the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\\\(S_{rel}\\\\) output for Block 1 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and \"skewing\" procedure can be used. The only thing we need to change is the unmasked \\\\(S_{rel}\\\\) output for Block 2 \\\\(\\leftrightarrow\\\\) Block 1, which will be the focus of discussion below.\n\n<br/>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\nWe denote \\\\(N\\\\) to be the block length, hence in our simple example \\\\(N = 6 \\div 2 = 3\\\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\\\(E^r\\\\), only that this time we collect indices from \\\\(1\\\\) to \\\\(2N-1\\\\) (if we notice in the desired \\\\(S_{rel}\\\\), these are the involved embeddings). This index range \\\\([1:2N -1]\\\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\\\(Q{E^r}^T_{1:2N-1}\\\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.\n\nOne thing to note is that the discussion above omits the head indices -- in actual implementation, one can choose to use the same \\\\(S_{rel}\\\\) for each head, or learn separate \\\\(S_{rel}\\\\) for different heads.\n\nNote that this technique can be extended to larger numbers of divided blocks -- generally, self-attending blocks can used the \"skewing\" procedure from relative global attention; non-self-attending blocks should use the latter unmasked \"skewing\" procedure. By this, we can succesfully compute \\\\(QK^T + S_{rel}\\\\), with both shapes \\\\((N, N)\\\\), for each local attention procedure.\n\n## 5 - Results\n\nFinally, we take a quick glance on the results and comparisons made by the paper.\n\n<figure>\n  <img style=\"width:70%; margin-top:30px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\nIn general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.\n\nThe main takeway from this work is clear: **relative information** does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf)), so we could probably foresee comparisons on different Transformer architectures for music generation in future.\n\n## 6 - Code Implementation\n\nThe Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:\n\n1. [The Main Script](https://github.com/magenta/magenta/tree/master/magenta/models/score2perf)\n2. [Relative Global Attention](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934)\n3. [Relative Local Attention](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964)\n4. [Skewing for Masked Cases (Global)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830)\n5. [Skewing for Unmasked Cases (Local)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813)\n\nThe code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both [Tensorflow](https://github.com/jason9693/MusicTransformer-tensorflow2.0) and [PyTorch](https://github.com/jason9693/MusicTransformer-pytorch/), and I definitely recommend to check them out because it really helped me a lot on understanding the paper.","source":"_posts/annotated-music-transformer.md","raw":"---\ntitle: Understanding Music Transformer\ndate: 2020-04-01 17:54:50\ntags:\n    - Transformer\n    - Symbolic Music\nestimatedReadTime: ~20 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Concepts discussed in the Music Transformer paper\n2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention\n3 - Ideas in the paper to efficiently implement the above attention mechanisms\n4 - Results on music generation\n\n## 1 - Introduction \nI personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the \"skewing\" procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.\n\n## 2 - Background of the Music Transformer\n\nThe [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf) paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. \n\nThe idea is as follows: by modelling each **performance event** as a token (as proposed in [Oore et al.](https://arxiv.org/pdf/1808.03715)), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the [demo samples](https://magenta.tensorflow.org/music-transformer) provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.\n\nSome related work of using Transformer architecture on generating music include [MuseNet](https://openai.com/blog/musenet/) (from OpenAI), and also [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf). It is evident that the Transformer architecture would be the backbone of music generation models in future research.\n\nFor the basics of Transformers, we refer the readers who are interested to these links: \n- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP;\n- [Understanding and Applying Self-Attention for NLP Video](https://www.youtube.com/watch?v=OYygPG4d9H0);\n- [Transformer Video by Hung-yi Lee](https://www.youtube.com/watch?v=ugWDIIOHtPA), Mandarin version;\n- [Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/), a great blog post by Chaitanya Joshi.\n\n## 3 - Transformer with Relative Attention\n\nIn my opinion, the Music Transformer paper is not only an application work, but its crux also includes *optimization* work on implementing Transformer with **relative attention**. We will delve into this part below.\n\nThe essence of Transformer is on **self-attention**: for an output sequence of the Transformer, the elements on each position is a result of \"attending to\" (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$\n\nwhere \\\\(Q, K, V\\\\) represents the query, key and value tensors, each having tensor shape \\\\((l, d)\\\\), with \\\\(l\\\\) and \\\\(d\\\\) representing the sequence length and the number of dimensions used in the model respectively.\n\nIn [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), the concept of **relative attention** is proposed. The idea is to represent **relative position representations** more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$\n\nThe difference is to learn an additional tensor \\\\(S^{rel}\\\\) of shape \\\\((l, l)\\\\). From the shape itself, we can see that the values \\\\(v_{ij}\\\\) in \\\\(S^{rel}\\\\) must be related to the relative distance of position \\\\(i\\\\) and \\\\(j\\\\) in length \\\\(l\\\\). \nIn fact, we can also view a sequence as a **fully connected graph** of its elements, hence \\\\(S^{rel}\\\\) can be seen as a tensor representing information about the **edges** in the graph.  \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\nHow do we learn the values in \\\\(S^{rel}\\\\)? In Shaw et al., this is done by learning an extra weight tensor \\\\(R\\\\) of shape \\\\((l, l, d)\\\\) -- only 1 dimension extra as compared to \\\\(S^{rel}\\\\). We can see it as having an **embedding** of dimension \\\\(d\\\\), at each position in a distance matrix of shape \\\\((l, l)\\\\). The values \\\\(R_{i, j}\\\\) means the embeddings which encode the relative position between position \\\\(i\\\\) from query tensor,  and position \\\\(j\\\\) from key tensor. Here, we only discuss cases of **masked self-attention** without looking forward -- only positions where \\\\(i \\ge j\\\\) is concerned because the model only attends to nodes that occur at the current time step or before.\n\nAfter that, we reshape \\\\(Q\\\\) into a \\\\((l, 1, d)\\\\) tensor, and multiply by $$S^{rel} = QR^T$$\n\nHowever, this incurs a total space complexity of \\\\(O(L^2 D)\\\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to \"perform some algorithmic tricks\" in order to obtain \\\\(S^{rel}\\\\) in space complexity of \\\\(O(LD)\\\\).\n\n## 4 - Relative Global Attention\n\nIn Music Transformer, the number of unique embeddings in \\\\(E^r\\\\) is assumed to be fixed. They used \\\\(l\\\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\\\(l - 1\\\\), from first position to the \\\\(l^{th}\\\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the \"edge\" between 1st and 3rd element, is the same as the embedding for the \"edge\" between 2nd and 4th element. \n\nIf we follow the assumptions made above, we can observe that the desired \\\\(S_{rel}\\\\) can actually be calculated from multiplying \\\\(Q{E^{r}}^T\\\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\\\(S_{rel}\\\\) from \\\\(Q{E^{r}}^T\\\\).\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\nThe bottom row in Figure 2 is known as the \"skewing\" procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\\\(M_\\textrm{masked}\\\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\\\(S_{rel}\\\\).\n\nHence, without using the gigantic \\\\(R\\\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\\\(O(LD)\\\\) space complexity. Although both implementations result in \\\\(O(L^{2}D)\\\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.\n\n## 4 - Relative Local Attention\n\n**Local attention**, first coined in [Luong et al](https://arxiv.org/pdf/1508.04025) and [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf), means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens *nearby* at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several *non-overlapping* blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a *memory* block which includes the current block and a finite number of tokens before the block).\n\nFor simplicity, let's start with an example of dividing the input sequence of 6 tokens into 2 blocks -- Block 1 \\\\([x_1, x_2, x_3]\\\\), and Block 2 \\\\([x_4, x_5, x_6]\\\\). Hence, the attention components include: Block 1 \\\\(\\leftrightarrow\\\\) Block 1, Block 2 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2, as shown in Figure 2.\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\nThe difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\\\(\\leftrightarrow\\\\) Block 1 as in global attention.\n\nSo, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\\\(S_{rel}\\\\) which can capture relative position information, which similarly we can obtain the values from \\\\(Q{E^r}^T\\\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\\\(S_{rel}\\\\) look like for 2 blocks:\n\n<figure>\n  <img style=\"width:40%; margin-top: 20px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\nFrom the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\\\(S_{rel}\\\\) output for Block 1 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and \"skewing\" procedure can be used. The only thing we need to change is the unmasked \\\\(S_{rel}\\\\) output for Block 2 \\\\(\\leftrightarrow\\\\) Block 1, which will be the focus of discussion below.\n\n<br/>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\nWe denote \\\\(N\\\\) to be the block length, hence in our simple example \\\\(N = 6 \\div 2 = 3\\\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\\\(E^r\\\\), only that this time we collect indices from \\\\(1\\\\) to \\\\(2N-1\\\\) (if we notice in the desired \\\\(S_{rel}\\\\), these are the involved embeddings). This index range \\\\([1:2N -1]\\\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\\\(Q{E^r}^T_{1:2N-1}\\\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.\n\nOne thing to note is that the discussion above omits the head indices -- in actual implementation, one can choose to use the same \\\\(S_{rel}\\\\) for each head, or learn separate \\\\(S_{rel}\\\\) for different heads.\n\nNote that this technique can be extended to larger numbers of divided blocks -- generally, self-attending blocks can used the \"skewing\" procedure from relative global attention; non-self-attending blocks should use the latter unmasked \"skewing\" procedure. By this, we can succesfully compute \\\\(QK^T + S_{rel}\\\\), with both shapes \\\\((N, N)\\\\), for each local attention procedure.\n\n## 5 - Results\n\nFinally, we take a quick glance on the results and comparisons made by the paper.\n\n<figure>\n  <img style=\"width:70%; margin-top:30px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\nIn general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.\n\nThe main takeway from this work is clear: **relative information** does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf)), so we could probably foresee comparisons on different Transformer architectures for music generation in future.\n\n## 6 - Code Implementation\n\nThe Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:\n\n1. [The Main Script](https://github.com/magenta/magenta/tree/master/magenta/models/score2perf)\n2. [Relative Global Attention](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934)\n3. [Relative Local Attention](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964)\n4. [Skewing for Masked Cases (Global)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830)\n5. [Skewing for Unmasked Cases (Local)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813)\n\nThe code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both [Tensorflow](https://github.com/jason9693/MusicTransformer-tensorflow2.0) and [PyTorch](https://github.com/jason9693/MusicTransformer-pytorch/), and I definitely recommend to check them out because it really helped me a lot on understanding the paper.","slug":"annotated-music-transformer","published":1,"updated":"2025-06-27T10:08:40.376Z","_id":"ck8jt5xfe0000jbm81um92vdu","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Concepts discussed in the Music Transformer paper<br>2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention<br>3 - Ideas in the paper to efficiently implement the above attention mechanisms<br>4 - Results on music generation</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>I personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the “skewing” procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.</p>\n<h2 id=\"2-Background-of-the-Music-Transformer\"><a href=\"#2-Background-of-the-Music-Transformer\" class=\"headerlink\" title=\"2 - Background of the Music Transformer\"></a>2 - Background of the Music Transformer</h2><p>The <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a> paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. </p>\n<p>The idea is as follows: by modelling each <strong>performance event</strong> as a token (as proposed in <a href=\"https://arxiv.org/pdf/1808.03715\" target=\"_blank\" rel=\"noopener\">Oore et al.</a>), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the <a href=\"https://magenta.tensorflow.org/music-transformer\" target=\"_blank\" rel=\"noopener\">demo samples</a> provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.</p>\n<p>Some related work of using Transformer architecture on generating music include <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a> (from OpenAI), and also <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>. It is evident that the Transformer architecture would be the backbone of music generation models in future research.</p>\n<p>For the basics of Transformers, we refer the readers who are interested to these links: </p>\n<ul>\n<li><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"noopener\">The Annotated Transformer</a> from Harvard NLP;</li>\n<li><a href=\"https://www.youtube.com/watch?v=OYygPG4d9H0\" target=\"_blank\" rel=\"noopener\">Understanding and Applying Self-Attention for NLP Video</a>;</li>\n<li><a href=\"https://www.youtube.com/watch?v=ugWDIIOHtPA\" target=\"_blank\" rel=\"noopener\">Transformer Video by Hung-yi Lee</a>, Mandarin version;</li>\n<li><a href=\"https://graphdeeplearning.github.io/post/transformers-are-gnns/\" target=\"_blank\" rel=\"noopener\">Transformers are Graph Neural Networks</a>, a great blog post by Chaitanya Joshi.</li>\n</ul>\n<h2 id=\"3-Transformer-with-Relative-Attention\"><a href=\"#3-Transformer-with-Relative-Attention\" class=\"headerlink\" title=\"3 - Transformer with Relative Attention\"></a>3 - Transformer with Relative Attention</h2><p>In my opinion, the Music Transformer paper is not only an application work, but its crux also includes <em>optimization</em> work on implementing Transformer with <strong>relative attention</strong>. We will delve into this part below.</p>\n<p>The essence of Transformer is on <strong>self-attention</strong>: for an output sequence of the Transformer, the elements on each position is a result of “attending to” (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$</p>\n<p>where \\(Q, K, V\\) represents the query, key and value tensors, each having tensor shape \\((l, d)\\), with \\(l\\) and \\(d\\) representing the sequence length and the number of dimensions used in the model respectively.</p>\n<p>In <a href=\"https://arxiv.org/pdf/1803.02155.pdf\" target=\"_blank\" rel=\"noopener\">Shaw et al.</a>, the concept of <strong>relative attention</strong> is proposed. The idea is to represent <strong>relative position representations</strong> more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$</p>\n<p>The difference is to learn an additional tensor \\(S^{rel}\\) of shape \\((l, l)\\). From the shape itself, we can see that the values \\(v_{ij}\\) in \\(S^{rel}\\) must be related to the relative distance of position \\(i\\) and \\(j\\) in length \\(l\\).<br>In fact, we can also view a sequence as a <strong>fully connected graph</strong> of its elements, hence \\(S^{rel}\\) can be seen as a tensor representing information about the <strong>edges</strong> in the graph.  </p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\n<p>How do we learn the values in \\(S^{rel}\\)? In Shaw et al., this is done by learning an extra weight tensor \\(R\\) of shape \\((l, l, d)\\) – only 1 dimension extra as compared to \\(S^{rel}\\). We can see it as having an <strong>embedding</strong> of dimension \\(d\\), at each position in a distance matrix of shape \\((l, l)\\). The values \\(R_{i, j}\\) means the embeddings which encode the relative position between position \\(i\\) from query tensor,  and position \\(j\\) from key tensor. Here, we only discuss cases of <strong>masked self-attention</strong> without looking forward – only positions where \\(i \\ge j\\) is concerned because the model only attends to nodes that occur at the current time step or before.</p>\n<p>After that, we reshape \\(Q\\) into a \\((l, 1, d)\\) tensor, and multiply by $$S^{rel} = QR^T$$</p>\n<p>However, this incurs a total space complexity of \\(O(L^2 D)\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to “perform some algorithmic tricks” in order to obtain \\(S^{rel}\\) in space complexity of \\(O(LD)\\).</p>\n<h2 id=\"4-Relative-Global-Attention\"><a href=\"#4-Relative-Global-Attention\" class=\"headerlink\" title=\"4 - Relative Global Attention\"></a>4 - Relative Global Attention</h2><p>In Music Transformer, the number of unique embeddings in \\(E^r\\) is assumed to be fixed. They used \\(l\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\(l - 1\\), from first position to the \\(l^{th}\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the “edge” between 1st and 3rd element, is the same as the embedding for the “edge” between 2nd and 4th element. </p>\n<p>If we follow the assumptions made above, we can observe that the desired \\(S_{rel}\\) can actually be calculated from multiplying \\(Q{E^{r}}^T\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\(S_{rel}\\) from \\(Q{E^{r}}^T\\).</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\n<p>The bottom row in Figure 2 is known as the “skewing” procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\(M_\\textrm{masked}\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\(S_{rel}\\).</p>\n<p>Hence, without using the gigantic \\(R\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\(O(LD)\\) space complexity. Although both implementations result in \\(O(L^{2}D)\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.</p>\n<h2 id=\"4-Relative-Local-Attention\"><a href=\"#4-Relative-Local-Attention\" class=\"headerlink\" title=\"4 - Relative Local Attention\"></a>4 - Relative Local Attention</h2><p><strong>Local attention</strong>, first coined in <a href=\"https://arxiv.org/pdf/1508.04025\" target=\"_blank\" rel=\"noopener\">Luong et al</a> and <a href=\"https://arxiv.org/pdf/1802.05751.pdf\" target=\"_blank\" rel=\"noopener\">Image Transformer</a>, means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens <em>nearby</em> at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several <em>non-overlapping</em> blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a <em>memory</em> block which includes the current block and a finite number of tokens before the block).</p>\n<p>For simplicity, let’s start with an example of dividing the input sequence of 6 tokens into 2 blocks – Block 1 \\([x_1, x_2, x_3]\\), and Block 2 \\([x_4, x_5, x_6]\\). Hence, the attention components include: Block 1 \\(\\leftrightarrow\\) Block 1, Block 2 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2, as shown in Figure 2.</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\n<p>The difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\(\\leftrightarrow\\) Block 1 as in global attention.</p>\n<p>So, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\(S_{rel}\\) which can capture relative position information, which similarly we can obtain the values from \\(Q{E^r}^T\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\(S_{rel}\\) look like for 2 blocks:</p>\n<figure>\n  <img style=\"width:40%; margin-top: 20px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\n<p>From the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\(S_{rel}\\) output for Block 1 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and “skewing” procedure can be used. The only thing we need to change is the unmasked \\(S_{rel}\\) output for Block 2 \\(\\leftrightarrow\\) Block 1, which will be the focus of discussion below.</p>\n<br/>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\n<p>We denote \\(N\\) to be the block length, hence in our simple example \\(N = 6 \\div 2 = 3\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\(E^r\\), only that this time we collect indices from \\(1\\) to \\(2N-1\\) (if we notice in the desired \\(S_{rel}\\), these are the involved embeddings). This index range \\([1:2N -1]\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\(Q{E^r}^T_{1:2N-1}\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.</p>\n<p>One thing to note is that the discussion above omits the head indices – in actual implementation, one can choose to use the same \\(S_{rel}\\) for each head, or learn separate \\(S_{rel}\\) for different heads.</p>\n<p>Note that this technique can be extended to larger numbers of divided blocks – generally, self-attending blocks can used the “skewing” procedure from relative global attention; non-self-attending blocks should use the latter unmasked “skewing” procedure. By this, we can succesfully compute \\(QK^T + S_{rel}\\), with both shapes \\((N, N)\\), for each local attention procedure.</p>\n<h2 id=\"5-Results\"><a href=\"#5-Results\" class=\"headerlink\" title=\"5 - Results\"></a>5 - Results</h2><p>Finally, we take a quick glance on the results and comparisons made by the paper.</p>\n<figure>\n  <img style=\"width:70%; margin-top:30px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\n<p>In general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.</p>\n<p>The main takeway from this work is clear: <strong>relative information</strong> does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>), so we could probably foresee comparisons on different Transformer architectures for music generation in future.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>The Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:</p>\n<ol>\n<li><a href=\"https://github.com/magenta/magenta/tree/master/magenta/models/score2perf\" target=\"_blank\" rel=\"noopener\">The Main Script</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934\" target=\"_blank\" rel=\"noopener\">Relative Global Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964\" target=\"_blank\" rel=\"noopener\">Relative Local Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830\" target=\"_blank\" rel=\"noopener\">Skewing for Masked Cases (Global)</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813\" target=\"_blank\" rel=\"noopener\">Skewing for Unmasked Cases (Local)</a></li>\n</ol>\n<p>The code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both <a href=\"https://github.com/jason9693/MusicTransformer-tensorflow2.0\" target=\"_blank\" rel=\"noopener\">Tensorflow</a> and <a href=\"https://github.com/jason9693/MusicTransformer-pytorch/\" target=\"_blank\" rel=\"noopener\">PyTorch</a>, and I definitely recommend to check them out because it really helped me a lot on understanding the paper.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Concepts discussed in the Music Transformer paper<br>2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention<br>3 - Ideas in the paper to efficiently implement the above attention mechanisms<br>4 - Results on music generation</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>I personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the “skewing” procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.</p>\n<h2 id=\"2-Background-of-the-Music-Transformer\"><a href=\"#2-Background-of-the-Music-Transformer\" class=\"headerlink\" title=\"2 - Background of the Music Transformer\"></a>2 - Background of the Music Transformer</h2><p>The <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a> paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. </p>\n<p>The idea is as follows: by modelling each <strong>performance event</strong> as a token (as proposed in <a href=\"https://arxiv.org/pdf/1808.03715\" target=\"_blank\" rel=\"noopener\">Oore et al.</a>), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the <a href=\"https://magenta.tensorflow.org/music-transformer\" target=\"_blank\" rel=\"noopener\">demo samples</a> provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.</p>\n<p>Some related work of using Transformer architecture on generating music include <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a> (from OpenAI), and also <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>. It is evident that the Transformer architecture would be the backbone of music generation models in future research.</p>\n<p>For the basics of Transformers, we refer the readers who are interested to these links: </p>\n<ul>\n<li><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"noopener\">The Annotated Transformer</a> from Harvard NLP;</li>\n<li><a href=\"https://www.youtube.com/watch?v=OYygPG4d9H0\" target=\"_blank\" rel=\"noopener\">Understanding and Applying Self-Attention for NLP Video</a>;</li>\n<li><a href=\"https://www.youtube.com/watch?v=ugWDIIOHtPA\" target=\"_blank\" rel=\"noopener\">Transformer Video by Hung-yi Lee</a>, Mandarin version;</li>\n<li><a href=\"https://graphdeeplearning.github.io/post/transformers-are-gnns/\" target=\"_blank\" rel=\"noopener\">Transformers are Graph Neural Networks</a>, a great blog post by Chaitanya Joshi.</li>\n</ul>\n<h2 id=\"3-Transformer-with-Relative-Attention\"><a href=\"#3-Transformer-with-Relative-Attention\" class=\"headerlink\" title=\"3 - Transformer with Relative Attention\"></a>3 - Transformer with Relative Attention</h2><p>In my opinion, the Music Transformer paper is not only an application work, but its crux also includes <em>optimization</em> work on implementing Transformer with <strong>relative attention</strong>. We will delve into this part below.</p>\n<p>The essence of Transformer is on <strong>self-attention</strong>: for an output sequence of the Transformer, the elements on each position is a result of “attending to” (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$</p>\n<p>where \\(Q, K, V\\) represents the query, key and value tensors, each having tensor shape \\((l, d)\\), with \\(l\\) and \\(d\\) representing the sequence length and the number of dimensions used in the model respectively.</p>\n<p>In <a href=\"https://arxiv.org/pdf/1803.02155.pdf\" target=\"_blank\" rel=\"noopener\">Shaw et al.</a>, the concept of <strong>relative attention</strong> is proposed. The idea is to represent <strong>relative position representations</strong> more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$</p>\n<p>The difference is to learn an additional tensor \\(S^{rel}\\) of shape \\((l, l)\\). From the shape itself, we can see that the values \\(v_{ij}\\) in \\(S^{rel}\\) must be related to the relative distance of position \\(i\\) and \\(j\\) in length \\(l\\).<br>In fact, we can also view a sequence as a <strong>fully connected graph</strong> of its elements, hence \\(S^{rel}\\) can be seen as a tensor representing information about the <strong>edges</strong> in the graph.  </p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\n<p>How do we learn the values in \\(S^{rel}\\)? In Shaw et al., this is done by learning an extra weight tensor \\(R\\) of shape \\((l, l, d)\\) – only 1 dimension extra as compared to \\(S^{rel}\\). We can see it as having an <strong>embedding</strong> of dimension \\(d\\), at each position in a distance matrix of shape \\((l, l)\\). The values \\(R_{i, j}\\) means the embeddings which encode the relative position between position \\(i\\) from query tensor,  and position \\(j\\) from key tensor. Here, we only discuss cases of <strong>masked self-attention</strong> without looking forward – only positions where \\(i \\ge j\\) is concerned because the model only attends to nodes that occur at the current time step or before.</p>\n<p>After that, we reshape \\(Q\\) into a \\((l, 1, d)\\) tensor, and multiply by $$S^{rel} = QR^T$$</p>\n<p>However, this incurs a total space complexity of \\(O(L^2 D)\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to “perform some algorithmic tricks” in order to obtain \\(S^{rel}\\) in space complexity of \\(O(LD)\\).</p>\n<h2 id=\"4-Relative-Global-Attention\"><a href=\"#4-Relative-Global-Attention\" class=\"headerlink\" title=\"4 - Relative Global Attention\"></a>4 - Relative Global Attention</h2><p>In Music Transformer, the number of unique embeddings in \\(E^r\\) is assumed to be fixed. They used \\(l\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\(l - 1\\), from first position to the \\(l^{th}\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the “edge” between 1st and 3rd element, is the same as the embedding for the “edge” between 2nd and 4th element. </p>\n<p>If we follow the assumptions made above, we can observe that the desired \\(S_{rel}\\) can actually be calculated from multiplying \\(Q{E^{r}}^T\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\(S_{rel}\\) from \\(Q{E^{r}}^T\\).</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\n<p>The bottom row in Figure 2 is known as the “skewing” procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\(M_\\textrm{masked}\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\(S_{rel}\\).</p>\n<p>Hence, without using the gigantic \\(R\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\(O(LD)\\) space complexity. Although both implementations result in \\(O(L^{2}D)\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.</p>\n<h2 id=\"4-Relative-Local-Attention\"><a href=\"#4-Relative-Local-Attention\" class=\"headerlink\" title=\"4 - Relative Local Attention\"></a>4 - Relative Local Attention</h2><p><strong>Local attention</strong>, first coined in <a href=\"https://arxiv.org/pdf/1508.04025\" target=\"_blank\" rel=\"noopener\">Luong et al</a> and <a href=\"https://arxiv.org/pdf/1802.05751.pdf\" target=\"_blank\" rel=\"noopener\">Image Transformer</a>, means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens <em>nearby</em> at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several <em>non-overlapping</em> blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a <em>memory</em> block which includes the current block and a finite number of tokens before the block).</p>\n<p>For simplicity, let’s start with an example of dividing the input sequence of 6 tokens into 2 blocks – Block 1 \\([x_1, x_2, x_3]\\), and Block 2 \\([x_4, x_5, x_6]\\). Hence, the attention components include: Block 1 \\(\\leftrightarrow\\) Block 1, Block 2 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2, as shown in Figure 2.</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\n<p>The difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\(\\leftrightarrow\\) Block 1 as in global attention.</p>\n<p>So, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\(S_{rel}\\) which can capture relative position information, which similarly we can obtain the values from \\(Q{E^r}^T\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\(S_{rel}\\) look like for 2 blocks:</p>\n<figure>\n  <img style=\"width:40%; margin-top: 20px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\n<p>From the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\(S_{rel}\\) output for Block 1 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and “skewing” procedure can be used. The only thing we need to change is the unmasked \\(S_{rel}\\) output for Block 2 \\(\\leftrightarrow\\) Block 1, which will be the focus of discussion below.</p>\n<br/>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\n<p>We denote \\(N\\) to be the block length, hence in our simple example \\(N = 6 \\div 2 = 3\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\(E^r\\), only that this time we collect indices from \\(1\\) to \\(2N-1\\) (if we notice in the desired \\(S_{rel}\\), these are the involved embeddings). This index range \\([1:2N -1]\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\(Q{E^r}^T_{1:2N-1}\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.</p>\n<p>One thing to note is that the discussion above omits the head indices – in actual implementation, one can choose to use the same \\(S_{rel}\\) for each head, or learn separate \\(S_{rel}\\) for different heads.</p>\n<p>Note that this technique can be extended to larger numbers of divided blocks – generally, self-attending blocks can used the “skewing” procedure from relative global attention; non-self-attending blocks should use the latter unmasked “skewing” procedure. By this, we can succesfully compute \\(QK^T + S_{rel}\\), with both shapes \\((N, N)\\), for each local attention procedure.</p>\n<h2 id=\"5-Results\"><a href=\"#5-Results\" class=\"headerlink\" title=\"5 - Results\"></a>5 - Results</h2><p>Finally, we take a quick glance on the results and comparisons made by the paper.</p>\n<figure>\n  <img style=\"width:70%; margin-top:30px; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\n<p>In general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.</p>\n<p>The main takeway from this work is clear: <strong>relative information</strong> does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>), so we could probably foresee comparisons on different Transformer architectures for music generation in future.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>The Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:</p>\n<ol>\n<li><a href=\"https://github.com/magenta/magenta/tree/master/magenta/models/score2perf\" target=\"_blank\" rel=\"noopener\">The Main Script</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934\" target=\"_blank\" rel=\"noopener\">Relative Global Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964\" target=\"_blank\" rel=\"noopener\">Relative Local Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830\" target=\"_blank\" rel=\"noopener\">Skewing for Masked Cases (Global)</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813\" target=\"_blank\" rel=\"noopener\">Skewing for Unmasked Cases (Local)</a></li>\n</ol>\n<p>The code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both <a href=\"https://github.com/jason9693/MusicTransformer-tensorflow2.0\" target=\"_blank\" rel=\"noopener\">Tensorflow</a> and <a href=\"https://github.com/jason9693/MusicTransformer-pytorch/\" target=\"_blank\" rel=\"noopener\">PyTorch</a>, and I definitely recommend to check them out because it really helped me a lot on understanding the paper.</p>\n"},{"title":"Semi-Supervised Learning for Music Modelling","date":"2020-05-13T10:23:35.000Z","estimatedReadTime":"~10 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Motivation of using semi-supervised learning in music modelling\n2 - Two SSL frameworks based on latent generative models - **Kingma et al** and **VaDE**\n3 - Applications of these frameworks on music-related tasks\n<br/>\n\n## 1 - Introduction\n\nIn a [previous post](/2020/01/26/vae-symbolic-music/), we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both **inference** and **generation** capabilities. Furthermore, by using extra techniques such as **disentanglement**, **latent regularization**, or using a more complex prior such as **Gaussian mixture model**, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.\n\nIn this post, we introduce the application of **semi-supervised learning (SSL)**, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times **we do not have enough labelled data** for all kinds of reasons -- annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.\n\n## 2 - Why Semi-Supervised Learning?\n\nThe strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal & valence, style, etc.), we can often observe two scenarios: (i) either the **amount of labels is too little**, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes **too noisy and deviated**, due to the subjective nature of these annotations, which hinders the model from learning good representations. \n\nTherefore, one of the solutions is to introduce SSL -- we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of *quality* labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of **representation learning** because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.\n\n## 3 - Applying SSL to Deep Learning Models\n\n### SSL using Deep Generative Models\nWe start from one of the earliest papers that discuss SSL in deep learning models. In [Kingma et al.](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf) the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.\n\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\nThe generation components can be understood as how each model assumes each data point to be generated. \\\\(\\textrm{M1}\\\\) resembles the idea of **latent variable models**, where a data point is generated from a latent prior, and further being projected to the observation space. \\\\(\\textrm{M2}\\\\) is simply two strands of \\\\(\\textrm{M1}\\\\) -- one on the discrete class variable \\\\(\\textbf{y}\\\\), and the other on the continuous latent \\\\(\\textbf{z}\\\\). \\\\(\\textrm{M2}\\\\) can also be viewed as a **disentanglement** model, if we understand it as learning separate spaces for labels in \\\\(\\textbf{y}\\\\), and residual information in \\\\(\\textbf{z}\\\\) (e.g. writing styles in MNIST). \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is generally a hierachical combination of both.\n\nOn the other hand, all exact posterior \\\\(p(\\textbf{z} | \\textbf{X})\\\\) are approximated using variational inference by introducing a new distribution \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\). The posterior can also be called the **inference** component, as we are **inferring** the latent distributions from the observations. \n\nThe posterior for \\\\(\\textrm{M1}\\\\) is evident to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\), and the model employs a separate classifier (e.g. an SVM) to predict \\\\(\\textbf{y}\\\\) from the low-dimension manifold \\\\(\\textbf{z}\\\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\\\(\\textrm{M2}\\\\), the authors parameterized the posterior to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\\\), which the class labels are inferred directly from \\\\(\\textbf{X}\\\\) using a separate Gaussian inference network.\n\nSo, how does the objective function look like if we want to train the model in a semi-supervised manner?\n\nFor \\\\(\\textrm{M1}\\\\), we are basically training a VAE, so the objective function is:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.\n\nFor \\\\(\\textrm{M2}\\\\), we need to consider two cases: if label is present (*supervised*), then the objective function is very similar to the VAE objective function, other than an additional given \\\\(\\textbf{y}\\\\):\n$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (*unsupervised*), then we **marginalize** over all possibilities of class labels as below:\n\n$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ \n\nwhere the additional **entropy** term \\\\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\\\(q_\\phi(y|\\textbf{X})\\\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\\\(\\phi\\\\) and \\\\(\\theta\\\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.\n\n### Variational Deep Embedding (VaDE)\n\n<figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n[VaDE](https://arxiv.org/pdf/1611.05148.pdf) employs the idea of **unsupervised and generative approach on clustering**. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\\\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\\\) for the inference component. One can relate this to discrete representation learning using VAE with a **Gaussian mixture prior** -- after inferring the latent variable \\\\(\\textbf{z}\\\\), the variable is assigned to a particular cluster with index \\\\(y\\\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\\\(z\\\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (*supervised*), the KL term is written as:\n\n$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$\n\nand when labels are not present (*unsupervised*), we similarly **marginalize** over all possibilities of class labels, as we have done for the \\\\(\\textrm{M2}\\\\) model before:\n$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$\n\n### Comparison\n\nHere, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are **latent variable models**, and make use of the **generative** approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to **marginalize** over all classes. In fact, if we look close at the inference component in \\\\(\\textrm{M1} + \\textrm{M2}\\\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\\\(y\\\\) and a standard Gaussian for \\\\(\\textbf{z}\\\\), whereas VaDE integrates both into a single mixture-of-Gaussians.\n\n## 4 - Applications\n\nThe SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both **discriminative** capability for analysis / feature extraction, and **generation** capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn **meaningful musical representations** from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.\n\nAn example discussed for music generation is by [Ferreira et al](http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf) on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from [Radford et al](https://arxiv.org/pdf/1704.01444.pdf) on generating reviews with sentiment. The model used is an \\\\(\\textrm{mLSTM}\\\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\\\(\\textrm{mLSTM}\\\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\\\(\\textrm{M1}\\\\), with the intermediate representation from \\\\(\\textrm{mLSTM}\\\\) as \\\\(\\textbf{z}\\\\), and a separate logistic regressor is used to predict \\\\(y\\\\) from \\\\(\\textbf{z}\\\\).\n\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\nAnother example is by [Luo et al](https://arxiv.org/pdf/1906.08152.pdf) on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional *disentanglement* added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both **discrete** and **continuous** representations for both pitch and timbre -- *discrete* representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the *continuous* representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.\n\nAnother two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the [Tacotron](https://google.github.io/tacotron/) team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve [hierarchical modelling](https://arxiv.org/pdf/1810.07217.pdf) and [semi-supervised learning](https://arxiv.org/pdf/1910.01709.pdf) for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.\n\n## Conclusion\n\nWith the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to \"self-supervisedly\" learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.\n\nFor the fundamental framework papers, please refer to the list below:\n1. [Semi-supervised Learning with Deep Generative Models](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf)\n2. [Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering](https://arxiv.org/pdf/1611.05148.pdf)\n3. [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf)\n\n\n","source":"_posts/semi-supervised-music.md","raw":"---\ntitle: Semi-Supervised Learning for Music Modelling\ndate: 2020-05-13 18:23:35\ntags:\n    - VAE\n    - Music Representation Learning\nestimatedReadTime: ~10 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Motivation of using semi-supervised learning in music modelling\n2 - Two SSL frameworks based on latent generative models - **Kingma et al** and **VaDE**\n3 - Applications of these frameworks on music-related tasks\n<br/>\n\n## 1 - Introduction\n\nIn a [previous post](/2020/01/26/vae-symbolic-music/), we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both **inference** and **generation** capabilities. Furthermore, by using extra techniques such as **disentanglement**, **latent regularization**, or using a more complex prior such as **Gaussian mixture model**, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.\n\nIn this post, we introduce the application of **semi-supervised learning (SSL)**, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times **we do not have enough labelled data** for all kinds of reasons -- annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.\n\n## 2 - Why Semi-Supervised Learning?\n\nThe strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal & valence, style, etc.), we can often observe two scenarios: (i) either the **amount of labels is too little**, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes **too noisy and deviated**, due to the subjective nature of these annotations, which hinders the model from learning good representations. \n\nTherefore, one of the solutions is to introduce SSL -- we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of *quality* labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of **representation learning** because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.\n\n## 3 - Applying SSL to Deep Learning Models\n\n### SSL using Deep Generative Models\nWe start from one of the earliest papers that discuss SSL in deep learning models. In [Kingma et al.](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf) the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.\n\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\nThe generation components can be understood as how each model assumes each data point to be generated. \\\\(\\textrm{M1}\\\\) resembles the idea of **latent variable models**, where a data point is generated from a latent prior, and further being projected to the observation space. \\\\(\\textrm{M2}\\\\) is simply two strands of \\\\(\\textrm{M1}\\\\) -- one on the discrete class variable \\\\(\\textbf{y}\\\\), and the other on the continuous latent \\\\(\\textbf{z}\\\\). \\\\(\\textrm{M2}\\\\) can also be viewed as a **disentanglement** model, if we understand it as learning separate spaces for labels in \\\\(\\textbf{y}\\\\), and residual information in \\\\(\\textbf{z}\\\\) (e.g. writing styles in MNIST). \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is generally a hierachical combination of both.\n\nOn the other hand, all exact posterior \\\\(p(\\textbf{z} | \\textbf{X})\\\\) are approximated using variational inference by introducing a new distribution \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\). The posterior can also be called the **inference** component, as we are **inferring** the latent distributions from the observations. \n\nThe posterior for \\\\(\\textrm{M1}\\\\) is evident to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\), and the model employs a separate classifier (e.g. an SVM) to predict \\\\(\\textbf{y}\\\\) from the low-dimension manifold \\\\(\\textbf{z}\\\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\\\(\\textrm{M2}\\\\), the authors parameterized the posterior to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\\\), which the class labels are inferred directly from \\\\(\\textbf{X}\\\\) using a separate Gaussian inference network.\n\nSo, how does the objective function look like if we want to train the model in a semi-supervised manner?\n\nFor \\\\(\\textrm{M1}\\\\), we are basically training a VAE, so the objective function is:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.\n\nFor \\\\(\\textrm{M2}\\\\), we need to consider two cases: if label is present (*supervised*), then the objective function is very similar to the VAE objective function, other than an additional given \\\\(\\textbf{y}\\\\):\n$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (*unsupervised*), then we **marginalize** over all possibilities of class labels as below:\n\n$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ \n\nwhere the additional **entropy** term \\\\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\\\(q_\\phi(y|\\textbf{X})\\\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\\\(\\phi\\\\) and \\\\(\\theta\\\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.\n\n### Variational Deep Embedding (VaDE)\n\n<figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n[VaDE](https://arxiv.org/pdf/1611.05148.pdf) employs the idea of **unsupervised and generative approach on clustering**. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\\\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\\\) for the inference component. One can relate this to discrete representation learning using VAE with a **Gaussian mixture prior** -- after inferring the latent variable \\\\(\\textbf{z}\\\\), the variable is assigned to a particular cluster with index \\\\(y\\\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\\\(z\\\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (*supervised*), the KL term is written as:\n\n$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$\n\nand when labels are not present (*unsupervised*), we similarly **marginalize** over all possibilities of class labels, as we have done for the \\\\(\\textrm{M2}\\\\) model before:\n$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$\n\n### Comparison\n\nHere, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are **latent variable models**, and make use of the **generative** approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to **marginalize** over all classes. In fact, if we look close at the inference component in \\\\(\\textrm{M1} + \\textrm{M2}\\\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\\\(y\\\\) and a standard Gaussian for \\\\(\\textbf{z}\\\\), whereas VaDE integrates both into a single mixture-of-Gaussians.\n\n## 4 - Applications\n\nThe SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both **discriminative** capability for analysis / feature extraction, and **generation** capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn **meaningful musical representations** from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.\n\nAn example discussed for music generation is by [Ferreira et al](http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf) on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from [Radford et al](https://arxiv.org/pdf/1704.01444.pdf) on generating reviews with sentiment. The model used is an \\\\(\\textrm{mLSTM}\\\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\\\(\\textrm{mLSTM}\\\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\\\(\\textrm{M1}\\\\), with the intermediate representation from \\\\(\\textrm{mLSTM}\\\\) as \\\\(\\textbf{z}\\\\), and a separate logistic regressor is used to predict \\\\(y\\\\) from \\\\(\\textbf{z}\\\\).\n\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\nAnother example is by [Luo et al](https://arxiv.org/pdf/1906.08152.pdf) on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional *disentanglement* added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both **discrete** and **continuous** representations for both pitch and timbre -- *discrete* representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the *continuous* representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.\n\nAnother two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the [Tacotron](https://google.github.io/tacotron/) team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve [hierarchical modelling](https://arxiv.org/pdf/1810.07217.pdf) and [semi-supervised learning](https://arxiv.org/pdf/1910.01709.pdf) for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.\n\n## Conclusion\n\nWith the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to \"self-supervisedly\" learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.\n\nFor the fundamental framework papers, please refer to the list below:\n1. [Semi-supervised Learning with Deep Generative Models](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf)\n2. [Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering](https://arxiv.org/pdf/1611.05148.pdf)\n3. [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf)\n\n\n","slug":"semi-supervised-music","published":1,"updated":"2025-06-27T10:09:32.906Z","_id":"ckbpvdoks0000qlm88ndz3z7a","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Motivation of using semi-supervised learning in music modelling<br>2 - Two SSL frameworks based on latent generative models - <strong>Kingma et al</strong> and <strong>VaDE</strong><br>3 - Applications of these frameworks on music-related tasks<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>In a <a href=\"/2020/01/26/vae-symbolic-music/\">previous post</a>, we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both <strong>inference</strong> and <strong>generation</strong> capabilities. Furthermore, by using extra techniques such as <strong>disentanglement</strong>, <strong>latent regularization</strong>, or using a more complex prior such as <strong>Gaussian mixture model</strong>, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.</p>\n<p>In this post, we introduce the application of <strong>semi-supervised learning (SSL)</strong>, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times <strong>we do not have enough labelled data</strong> for all kinds of reasons – annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.</p>\n<h2 id=\"2-Why-Semi-Supervised-Learning\"><a href=\"#2-Why-Semi-Supervised-Learning\" class=\"headerlink\" title=\"2 - Why Semi-Supervised Learning?\"></a>2 - Why Semi-Supervised Learning?</h2><p>The strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal &amp; valence, style, etc.), we can often observe two scenarios: (i) either the <strong>amount of labels is too little</strong>, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes <strong>too noisy and deviated</strong>, due to the subjective nature of these annotations, which hinders the model from learning good representations. </p>\n<p>Therefore, one of the solutions is to introduce SSL – we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of <em>quality</em> labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of <strong>representation learning</strong> because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.</p>\n<h2 id=\"3-Applying-SSL-to-Deep-Learning-Models\"><a href=\"#3-Applying-SSL-to-Deep-Learning-Models\" class=\"headerlink\" title=\"3 - Applying SSL to Deep Learning Models\"></a>3 - Applying SSL to Deep Learning Models</h2><h3 id=\"SSL-using-Deep-Generative-Models\"><a href=\"#SSL-using-Deep-Generative-Models\" class=\"headerlink\" title=\"SSL using Deep Generative Models\"></a>SSL using Deep Generative Models</h3><p>We start from one of the earliest papers that discuss SSL in deep learning models. In <a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Kingma et al.</a> the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.</p>\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\n<p>The generation components can be understood as how each model assumes each data point to be generated. \\(\\textrm{M1}\\) resembles the idea of <strong>latent variable models</strong>, where a data point is generated from a latent prior, and further being projected to the observation space. \\(\\textrm{M2}\\) is simply two strands of \\(\\textrm{M1}\\) – one on the discrete class variable \\(\\textbf{y}\\), and the other on the continuous latent \\(\\textbf{z}\\). \\(\\textrm{M2}\\) can also be viewed as a <strong>disentanglement</strong> model, if we understand it as learning separate spaces for labels in \\(\\textbf{y}\\), and residual information in \\(\\textbf{z}\\) (e.g. writing styles in MNIST). \\(\\textrm{M1} + \\textrm{M2}\\) is generally a hierachical combination of both.</p>\n<p>On the other hand, all exact posterior \\(p(\\textbf{z} | \\textbf{X})\\) are approximated using variational inference by introducing a new distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\). The posterior can also be called the <strong>inference</strong> component, as we are <strong>inferring</strong> the latent distributions from the observations. </p>\n<p>The posterior for \\(\\textrm{M1}\\) is evident to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\), and the model employs a separate classifier (e.g. an SVM) to predict \\(\\textbf{y}\\) from the low-dimension manifold \\(\\textbf{z}\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\(\\textrm{M2}\\), the authors parameterized the posterior to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\), which the class labels are inferred directly from \\(\\textbf{X}\\) using a separate Gaussian inference network.</p>\n<p>So, how does the objective function look like if we want to train the model in a semi-supervised manner?</p>\n<p>For \\(\\textrm{M1}\\), we are basically training a VAE, so the objective function is:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.</p>\n<p>For \\(\\textrm{M2}\\), we need to consider two cases: if label is present (<em>supervised</em>), then the objective function is very similar to the VAE objective function, other than an additional given \\(\\textbf{y}\\):<br>$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (<em>unsupervised</em>), then we <strong>marginalize</strong> over all possibilities of class labels as below:</p>\n<p>$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ </p>\n<p>where the additional <strong>entropy</strong> term \\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\(q_\\phi(y|\\textbf{X})\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\(\\textrm{M1} + \\textrm{M2}\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\(\\phi\\) and \\(\\theta\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.</p>\n<h3 id=\"Variational-Deep-Embedding-VaDE\"><a href=\"#Variational-Deep-Embedding-VaDE\" class=\"headerlink\" title=\"Variational Deep Embedding (VaDE)\"></a>Variational Deep Embedding (VaDE)</h3><figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n<p><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">VaDE</a> employs the idea of <strong>unsupervised and generative approach on clustering</strong>. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\) for the inference component. One can relate this to discrete representation learning using VAE with a <strong>Gaussian mixture prior</strong> – after inferring the latent variable \\(\\textbf{z}\\), the variable is assigned to a particular cluster with index \\(y\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\(z\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (<em>supervised</em>), the KL term is written as:</p>\n<p>$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$</p>\n<p>and when labels are not present (<em>unsupervised</em>), we similarly <strong>marginalize</strong> over all possibilities of class labels, as we have done for the \\(\\textrm{M2}\\) model before:<br>$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$</p>\n<h3 id=\"Comparison\"><a href=\"#Comparison\" class=\"headerlink\" title=\"Comparison\"></a>Comparison</h3><p>Here, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are <strong>latent variable models</strong>, and make use of the <strong>generative</strong> approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to <strong>marginalize</strong> over all classes. In fact, if we look close at the inference component in \\(\\textrm{M1} + \\textrm{M2}\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\(y\\) and a standard Gaussian for \\(\\textbf{z}\\), whereas VaDE integrates both into a single mixture-of-Gaussians.</p>\n<h2 id=\"4-Applications\"><a href=\"#4-Applications\" class=\"headerlink\" title=\"4 - Applications\"></a>4 - Applications</h2><p>The SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both <strong>discriminative</strong> capability for analysis / feature extraction, and <strong>generation</strong> capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn <strong>meaningful musical representations</strong> from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.</p>\n<p>An example discussed for music generation is by <a href=\"http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf\" target=\"_blank\" rel=\"noopener\">Ferreira et al</a> on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from <a href=\"https://arxiv.org/pdf/1704.01444.pdf\" target=\"_blank\" rel=\"noopener\">Radford et al</a> on generating reviews with sentiment. The model used is an \\(\\textrm{mLSTM}\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\(\\textrm{mLSTM}\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\(\\textrm{M1}\\), with the intermediate representation from \\(\\textrm{mLSTM}\\) as \\(\\textbf{z}\\), and a separate logistic regressor is used to predict \\(y\\) from \\(\\textbf{z}\\).</p>\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\n<p>Another example is by <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">Luo et al</a> on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional <em>disentanglement</em> added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both <strong>discrete</strong> and <strong>continuous</strong> representations for both pitch and timbre – <em>discrete</em> representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the <em>continuous</em> representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.</p>\n<p>Another two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the <a href=\"https://google.github.io/tacotron/\" target=\"_blank\" rel=\"noopener\">Tacotron</a> team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve <a href=\"https://arxiv.org/pdf/1810.07217.pdf\" target=\"_blank\" rel=\"noopener\">hierarchical modelling</a> and <a href=\"https://arxiv.org/pdf/1910.01709.pdf\" target=\"_blank\" rel=\"noopener\">semi-supervised learning</a> for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>With the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to “self-supervisedly” learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.</p>\n<p>For the fundamental framework papers, please refer to the list below:</p>\n<ol>\n<li><a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Semi-supervised Learning with Deep Generative Models</a></li>\n<li><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering</a></li>\n<li><a href=\"https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Motivation of using semi-supervised learning in music modelling<br>2 - Two SSL frameworks based on latent generative models - <strong>Kingma et al</strong> and <strong>VaDE</strong><br>3 - Applications of these frameworks on music-related tasks<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>In a <a href=\"/2020/01/26/vae-symbolic-music/\">previous post</a>, we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both <strong>inference</strong> and <strong>generation</strong> capabilities. Furthermore, by using extra techniques such as <strong>disentanglement</strong>, <strong>latent regularization</strong>, or using a more complex prior such as <strong>Gaussian mixture model</strong>, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.</p>\n<p>In this post, we introduce the application of <strong>semi-supervised learning (SSL)</strong>, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times <strong>we do not have enough labelled data</strong> for all kinds of reasons – annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.</p>\n<h2 id=\"2-Why-Semi-Supervised-Learning\"><a href=\"#2-Why-Semi-Supervised-Learning\" class=\"headerlink\" title=\"2 - Why Semi-Supervised Learning?\"></a>2 - Why Semi-Supervised Learning?</h2><p>The strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal &amp; valence, style, etc.), we can often observe two scenarios: (i) either the <strong>amount of labels is too little</strong>, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes <strong>too noisy and deviated</strong>, due to the subjective nature of these annotations, which hinders the model from learning good representations. </p>\n<p>Therefore, one of the solutions is to introduce SSL – we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of <em>quality</em> labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of <strong>representation learning</strong> because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.</p>\n<h2 id=\"3-Applying-SSL-to-Deep-Learning-Models\"><a href=\"#3-Applying-SSL-to-Deep-Learning-Models\" class=\"headerlink\" title=\"3 - Applying SSL to Deep Learning Models\"></a>3 - Applying SSL to Deep Learning Models</h2><h3 id=\"SSL-using-Deep-Generative-Models\"><a href=\"#SSL-using-Deep-Generative-Models\" class=\"headerlink\" title=\"SSL using Deep Generative Models\"></a>SSL using Deep Generative Models</h3><p>We start from one of the earliest papers that discuss SSL in deep learning models. In <a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Kingma et al.</a> the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.</p>\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\n<p>The generation components can be understood as how each model assumes each data point to be generated. \\(\\textrm{M1}\\) resembles the idea of <strong>latent variable models</strong>, where a data point is generated from a latent prior, and further being projected to the observation space. \\(\\textrm{M2}\\) is simply two strands of \\(\\textrm{M1}\\) – one on the discrete class variable \\(\\textbf{y}\\), and the other on the continuous latent \\(\\textbf{z}\\). \\(\\textrm{M2}\\) can also be viewed as a <strong>disentanglement</strong> model, if we understand it as learning separate spaces for labels in \\(\\textbf{y}\\), and residual information in \\(\\textbf{z}\\) (e.g. writing styles in MNIST). \\(\\textrm{M1} + \\textrm{M2}\\) is generally a hierachical combination of both.</p>\n<p>On the other hand, all exact posterior \\(p(\\textbf{z} | \\textbf{X})\\) are approximated using variational inference by introducing a new distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\). The posterior can also be called the <strong>inference</strong> component, as we are <strong>inferring</strong> the latent distributions from the observations. </p>\n<p>The posterior for \\(\\textrm{M1}\\) is evident to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\), and the model employs a separate classifier (e.g. an SVM) to predict \\(\\textbf{y}\\) from the low-dimension manifold \\(\\textbf{z}\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\(\\textrm{M2}\\), the authors parameterized the posterior to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\), which the class labels are inferred directly from \\(\\textbf{X}\\) using a separate Gaussian inference network.</p>\n<p>So, how does the objective function look like if we want to train the model in a semi-supervised manner?</p>\n<p>For \\(\\textrm{M1}\\), we are basically training a VAE, so the objective function is:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.</p>\n<p>For \\(\\textrm{M2}\\), we need to consider two cases: if label is present (<em>supervised</em>), then the objective function is very similar to the VAE objective function, other than an additional given \\(\\textbf{y}\\):<br>$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (<em>unsupervised</em>), then we <strong>marginalize</strong> over all possibilities of class labels as below:</p>\n<p>$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ </p>\n<p>where the additional <strong>entropy</strong> term \\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\(q_\\phi(y|\\textbf{X})\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\(\\textrm{M1} + \\textrm{M2}\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\(\\phi\\) and \\(\\theta\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.</p>\n<h3 id=\"Variational-Deep-Embedding-VaDE\"><a href=\"#Variational-Deep-Embedding-VaDE\" class=\"headerlink\" title=\"Variational Deep Embedding (VaDE)\"></a>Variational Deep Embedding (VaDE)</h3><figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n<p><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">VaDE</a> employs the idea of <strong>unsupervised and generative approach on clustering</strong>. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\) for the inference component. One can relate this to discrete representation learning using VAE with a <strong>Gaussian mixture prior</strong> – after inferring the latent variable \\(\\textbf{z}\\), the variable is assigned to a particular cluster with index \\(y\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\(z\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (<em>supervised</em>), the KL term is written as:</p>\n<p>$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$</p>\n<p>and when labels are not present (<em>unsupervised</em>), we similarly <strong>marginalize</strong> over all possibilities of class labels, as we have done for the \\(\\textrm{M2}\\) model before:<br>$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$</p>\n<h3 id=\"Comparison\"><a href=\"#Comparison\" class=\"headerlink\" title=\"Comparison\"></a>Comparison</h3><p>Here, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are <strong>latent variable models</strong>, and make use of the <strong>generative</strong> approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to <strong>marginalize</strong> over all classes. In fact, if we look close at the inference component in \\(\\textrm{M1} + \\textrm{M2}\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\(y\\) and a standard Gaussian for \\(\\textbf{z}\\), whereas VaDE integrates both into a single mixture-of-Gaussians.</p>\n<h2 id=\"4-Applications\"><a href=\"#4-Applications\" class=\"headerlink\" title=\"4 - Applications\"></a>4 - Applications</h2><p>The SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both <strong>discriminative</strong> capability for analysis / feature extraction, and <strong>generation</strong> capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn <strong>meaningful musical representations</strong> from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.</p>\n<p>An example discussed for music generation is by <a href=\"http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf\" target=\"_blank\" rel=\"noopener\">Ferreira et al</a> on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from <a href=\"https://arxiv.org/pdf/1704.01444.pdf\" target=\"_blank\" rel=\"noopener\">Radford et al</a> on generating reviews with sentiment. The model used is an \\(\\textrm{mLSTM}\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\(\\textrm{mLSTM}\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\(\\textrm{M1}\\), with the intermediate representation from \\(\\textrm{mLSTM}\\) as \\(\\textbf{z}\\), and a separate logistic regressor is used to predict \\(y\\) from \\(\\textbf{z}\\).</p>\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\n<p>Another example is by <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">Luo et al</a> on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional <em>disentanglement</em> added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both <strong>discrete</strong> and <strong>continuous</strong> representations for both pitch and timbre – <em>discrete</em> representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the <em>continuous</em> representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.</p>\n<p>Another two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the <a href=\"https://google.github.io/tacotron/\" target=\"_blank\" rel=\"noopener\">Tacotron</a> team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve <a href=\"https://arxiv.org/pdf/1810.07217.pdf\" target=\"_blank\" rel=\"noopener\">hierarchical modelling</a> and <a href=\"https://arxiv.org/pdf/1910.01709.pdf\" target=\"_blank\" rel=\"noopener\">semi-supervised learning</a> for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>With the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to “self-supervisedly” learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.</p>\n<p>For the fundamental framework papers, please refer to the list below:</p>\n<ol>\n<li><a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Semi-supervised Learning with Deep Generative Models</a></li>\n<li><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering</a></li>\n<li><a href=\"https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a></li>\n</ol>\n"},{"title":"Spectrogram Conversion with CNNs","date":"2020-07-24T10:20:39.000Z","estimatedReadTime":"~8 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief introduction on short-time Fourier transform\n2 - How spectrogram conversion can be implemented using CNNs (based on [nnAudio](https://github.com/KinWaiCheuk/nnAudio))\n<br/>\n\n## 1 - Introduction\n\nRecently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely **spectrograms**. My wonderful colleague Raven Cheuk had released a GPU audio processing named [nnAudio](https://github.com/KinWaiCheuk/nnAudio) last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.\n\n## 2 - Short Time Fourier Transform (STFT)\n\nFirst, we discuss the case for **discrete Fourier transform** (DFT), which converts a given audio signal of length \\\\(L\\\\) into a vector \\\\(X_{DFT}\\\\) of size \\\\(N\\\\), where \\\\(N\\\\) is the number of frequency bins (commonly, we set \\\\(L = N\\\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nHowever, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple **windows**, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the **discrete short-time Fourier transform** (STFT), which converts a given audio signal of length \\\\(L\\\\) into a time-frequency representation of shape \\\\((N, T)\\\\). \\\\(N\\\\) is the number of frequency bins, and \\\\(T\\\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\\\(L_{\\textrm{w}}\\\\) (similarly, \\\\(L_{\\textrm{w}} = N\\\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\\\(H\\\\)) to finish \"sweeping\" the audio signal.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\nGiven an audio signal \\\\(x\\\\), a **complex-form spectrogram** \\\\(X\\\\) which is the output of STFT is expressed by:\n$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$\n\nWe can further use Euler's formula to expand \\\\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\\\) into \\\\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\\\). The term \\\\(w[l]\\\\) is an additional [**window function**](https://en.wikipedia.org/wiki/Window_function) which helps to distribute spectral leakage according to the needs of the application. \n\nFrom Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\\\(N\\\\) **cosine and sine \"filters\"** respectively, and perform **1D convolution** on the audio signal, whereby the **stride** is exactly of the **hop length** \\\\(H\\\\). \n\n## 3 - Inverse STFT\n\nCan inverse STFT be implemented in terms of CNNs as well? In fact, this [torch-stft](https://github.com/pseeth/torch-stft) library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.\n\nIf we put together the equations of discrete DFT and inverse DFT (with window function) as below:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nwe can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, **we can perform inverse STFT using the same cosine and sine \"filters\"**, as the Euler term stays the same. \n\nAs \\\\(X_{DFT}\\\\) is in complex form, we can observe that the multiplication with the Euler term results in:\n$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\\\(x\\\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\\\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\nSince STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\\\((N, 1)\\\\) on the spectrogram with shape \\\\((N, T)\\\\) with stride \\\\((1, 1)\\\\). \n\nThe final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\\\(L_w - H\\\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.\n\n## 4 - Code Implementation\n\nThe above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:\n1. [Short-Time Fourier Transform with 1D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534)\n1. [Inverse STFT with 2D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581)\n\n\n\n\n\n\n","source":"_posts/conv_fourier.md","raw":"---\ntitle: Spectrogram Conversion with CNNs\ndate: 2020-07-24 18:20:39\ntags:\n    - Music Signal Processing\nestimatedReadTime: ~8 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief introduction on short-time Fourier transform\n2 - How spectrogram conversion can be implemented using CNNs (based on [nnAudio](https://github.com/KinWaiCheuk/nnAudio))\n<br/>\n\n## 1 - Introduction\n\nRecently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely **spectrograms**. My wonderful colleague Raven Cheuk had released a GPU audio processing named [nnAudio](https://github.com/KinWaiCheuk/nnAudio) last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.\n\n## 2 - Short Time Fourier Transform (STFT)\n\nFirst, we discuss the case for **discrete Fourier transform** (DFT), which converts a given audio signal of length \\\\(L\\\\) into a vector \\\\(X_{DFT}\\\\) of size \\\\(N\\\\), where \\\\(N\\\\) is the number of frequency bins (commonly, we set \\\\(L = N\\\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nHowever, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple **windows**, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the **discrete short-time Fourier transform** (STFT), which converts a given audio signal of length \\\\(L\\\\) into a time-frequency representation of shape \\\\((N, T)\\\\). \\\\(N\\\\) is the number of frequency bins, and \\\\(T\\\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\\\(L_{\\textrm{w}}\\\\) (similarly, \\\\(L_{\\textrm{w}} = N\\\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\\\(H\\\\)) to finish \"sweeping\" the audio signal.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\nGiven an audio signal \\\\(x\\\\), a **complex-form spectrogram** \\\\(X\\\\) which is the output of STFT is expressed by:\n$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$\n\nWe can further use Euler's formula to expand \\\\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\\\) into \\\\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\\\). The term \\\\(w[l]\\\\) is an additional [**window function**](https://en.wikipedia.org/wiki/Window_function) which helps to distribute spectral leakage according to the needs of the application. \n\nFrom Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\\\(N\\\\) **cosine and sine \"filters\"** respectively, and perform **1D convolution** on the audio signal, whereby the **stride** is exactly of the **hop length** \\\\(H\\\\). \n\n## 3 - Inverse STFT\n\nCan inverse STFT be implemented in terms of CNNs as well? In fact, this [torch-stft](https://github.com/pseeth/torch-stft) library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.\n\nIf we put together the equations of discrete DFT and inverse DFT (with window function) as below:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nwe can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, **we can perform inverse STFT using the same cosine and sine \"filters\"**, as the Euler term stays the same. \n\nAs \\\\(X_{DFT}\\\\) is in complex form, we can observe that the multiplication with the Euler term results in:\n$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\\\(x\\\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\\\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\nSince STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\\\((N, 1)\\\\) on the spectrogram with shape \\\\((N, T)\\\\) with stride \\\\((1, 1)\\\\). \n\nThe final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\\\(L_w - H\\\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.\n\n## 4 - Code Implementation\n\nThe above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:\n1. [Short-Time Fourier Transform with 1D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534)\n1. [Inverse STFT with 2D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581)\n\n\n\n\n\n\n","slug":"conv_fourier","published":1,"updated":"2025-06-27T10:08:53.306Z","_id":"ckgcjd4cq0000w19khjzs6q1z","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief introduction on short-time Fourier transform<br>2 - How spectrogram conversion can be implemented using CNNs (based on <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a>)<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely <strong>spectrograms</strong>. My wonderful colleague Raven Cheuk had released a GPU audio processing named <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.</p>\n<h2 id=\"2-Short-Time-Fourier-Transform-STFT\"><a href=\"#2-Short-Time-Fourier-Transform-STFT\" class=\"headerlink\" title=\"2 - Short Time Fourier Transform (STFT)\"></a>2 - Short Time Fourier Transform (STFT)</h2><p>First, we discuss the case for <strong>discrete Fourier transform</strong> (DFT), which converts a given audio signal of length \\(L\\) into a vector \\(X_{DFT}\\) of size \\(N\\), where \\(N\\) is the number of frequency bins (commonly, we set \\(L = N\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>However, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple <strong>windows</strong>, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the <strong>discrete short-time Fourier transform</strong> (STFT), which converts a given audio signal of length \\(L\\) into a time-frequency representation of shape \\((N, T)\\). \\(N\\) is the number of frequency bins, and \\(T\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\(L_{\\textrm{w}}\\) (similarly, \\(L_{\\textrm{w}} = N\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\(H\\)) to finish “sweeping” the audio signal.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\n<p>Given an audio signal \\(x\\), a <strong>complex-form spectrogram</strong> \\(X\\) which is the output of STFT is expressed by:<br>$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$</p>\n<p>We can further use Euler’s formula to expand \\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\) into \\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\). The term \\(w[l]\\) is an additional <a href=\"https://en.wikipedia.org/wiki/Window_function\" target=\"_blank\" rel=\"noopener\"><strong>window function</strong></a> which helps to distribute spectral leakage according to the needs of the application. </p>\n<p>From Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\(N\\) <strong>cosine and sine “filters”</strong> respectively, and perform <strong>1D convolution</strong> on the audio signal, whereby the <strong>stride</strong> is exactly of the <strong>hop length</strong> \\(H\\). </p>\n<h2 id=\"3-Inverse-STFT\"><a href=\"#3-Inverse-STFT\" class=\"headerlink\" title=\"3 - Inverse STFT\"></a>3 - Inverse STFT</h2><p>Can inverse STFT be implemented in terms of CNNs as well? In fact, this <a href=\"https://github.com/pseeth/torch-stft\" target=\"_blank\" rel=\"noopener\">torch-stft</a> library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.</p>\n<p>If we put together the equations of discrete DFT and inverse DFT (with window function) as below:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>we can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, <strong>we can perform inverse STFT using the same cosine and sine “filters”</strong>, as the Euler term stays the same. </p>\n<p>As \\(X_{DFT}\\) is in complex form, we can observe that the multiplication with the Euler term results in:<br>$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\(x\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\n<p>Since STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\((N, 1)\\) on the spectrogram with shape \\((N, T)\\) with stride \\((1, 1)\\). </p>\n<p>The final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\(L_w - H\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.</p>\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>The above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:</p>\n<ol>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534\" target=\"_blank\" rel=\"noopener\">Short-Time Fourier Transform with 1D-CNNs</a></li>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581\" target=\"_blank\" rel=\"noopener\">Inverse STFT with 2D-CNNs</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief introduction on short-time Fourier transform<br>2 - How spectrogram conversion can be implemented using CNNs (based on <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a>)<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely <strong>spectrograms</strong>. My wonderful colleague Raven Cheuk had released a GPU audio processing named <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.</p>\n<h2 id=\"2-Short-Time-Fourier-Transform-STFT\"><a href=\"#2-Short-Time-Fourier-Transform-STFT\" class=\"headerlink\" title=\"2 - Short Time Fourier Transform (STFT)\"></a>2 - Short Time Fourier Transform (STFT)</h2><p>First, we discuss the case for <strong>discrete Fourier transform</strong> (DFT), which converts a given audio signal of length \\(L\\) into a vector \\(X_{DFT}\\) of size \\(N\\), where \\(N\\) is the number of frequency bins (commonly, we set \\(L = N\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>However, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple <strong>windows</strong>, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the <strong>discrete short-time Fourier transform</strong> (STFT), which converts a given audio signal of length \\(L\\) into a time-frequency representation of shape \\((N, T)\\). \\(N\\) is the number of frequency bins, and \\(T\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\(L_{\\textrm{w}}\\) (similarly, \\(L_{\\textrm{w}} = N\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\(H\\)) to finish “sweeping” the audio signal.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\n<p>Given an audio signal \\(x\\), a <strong>complex-form spectrogram</strong> \\(X\\) which is the output of STFT is expressed by:<br>$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$</p>\n<p>We can further use Euler’s formula to expand \\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\) into \\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\). The term \\(w[l]\\) is an additional <a href=\"https://en.wikipedia.org/wiki/Window_function\" target=\"_blank\" rel=\"noopener\"><strong>window function</strong></a> which helps to distribute spectral leakage according to the needs of the application. </p>\n<p>From Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\(N\\) <strong>cosine and sine “filters”</strong> respectively, and perform <strong>1D convolution</strong> on the audio signal, whereby the <strong>stride</strong> is exactly of the <strong>hop length</strong> \\(H\\). </p>\n<h2 id=\"3-Inverse-STFT\"><a href=\"#3-Inverse-STFT\" class=\"headerlink\" title=\"3 - Inverse STFT\"></a>3 - Inverse STFT</h2><p>Can inverse STFT be implemented in terms of CNNs as well? In fact, this <a href=\"https://github.com/pseeth/torch-stft\" target=\"_blank\" rel=\"noopener\">torch-stft</a> library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.</p>\n<p>If we put together the equations of discrete DFT and inverse DFT (with window function) as below:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>we can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, <strong>we can perform inverse STFT using the same cosine and sine “filters”</strong>, as the Euler term stays the same. </p>\n<p>As \\(X_{DFT}\\) is in complex form, we can observe that the multiplication with the Euler term results in:<br>$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\(x\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\n<p>Since STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\((N, 1)\\) on the spectrogram with shape \\((N, T)\\) with stride \\((1, 1)\\). </p>\n<p>The final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\(L_w - H\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.</p>\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>The above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:</p>\n<ol>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534\" target=\"_blank\" rel=\"noopener\">Short-Time Fourier Transform with 1D-CNNs</a></li>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581\" target=\"_blank\" rel=\"noopener\">Inverse STFT with 2D-CNNs</a></li>\n</ol>\n"},{"title":"MIR Papers 2020 (and ISMIR)","date":"2020-10-17T01:10:42.000Z","estimatedReadTime":"~20 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 1 - Controllable Symbolic Music Generation\n\n[**Attributes-Aware Deep Music Transformation**](https://program.ismir2020.net/poster_5-06.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\nThis work uses a very similar architecture like [Fader Networks](https://arxiv.org/pdf/1706.00409.pdf) in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\\\(z\\\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in [Music FaderNets](https://program.ismir2020.net/poster_1-13.html), but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.\nOne interesting thing is that the authors do not compare results on linear correlation with [GLSR-VAE](https://arxiv.org/pdf/1707.04588.pdf), because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.\n\n[**BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)**](https://program.ismir2020.net/poster_6-08.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\nCongrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on **personalization**. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user's taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.\n\n[**Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation**](https://program.ismir2020.net/poster_1-05.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\nThis work proposes **connective fusion**, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the [Latent Constraint](https://arxiv.org/pdf/1711.05772.pdf) paper - firstly, we pretrain a VAE to learn latent code \\\\(z\\\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\\\(G\\\\) that generates latent code pair \\\\((z^\\prime_L, z^\\prime_R)\\\\) that is indistuingishable from the input pair\\\\((z_L, z_R)\\\\). During training, we also add in an additional style vector \\\\(s\\\\), hence also learning a style space which controls how the two sequences are connectively fused.\nI was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. [His work last year](http://archives.ismir.net/ismir2019/paper/000100.pdf) has also helped me a lot with Music FaderNets, so huge kudos to him!\n\n[**Generating Music with a Self-Correcting Non-Chronological Autoregressive Model**](https://program.ismir2020.net/poster_6-16.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\nI spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using **edit distance**. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against [orderless NADE](https://www.jmlr.org/papers/volume17/16-272/16-272.pdf) and [CoCoNet](https://arxiv.org/pdf/1903.07227.pdf), but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.\n\n[**PIANOTREE VAE: Structured Representation Learning for Polyphonic Music**](https://program.ismir2020.net/poster_3-06.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a **tree-like structure**, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.\nI definitely think that PianoTree has the potential to be the *de facto* representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: **the ease of usage** (e.g. open source of encoder components and examples of usage), and whether **the data structure is tightly coupled with the proposed VAE model**. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.\n\n[**Learning Interpretable Representation for Controllable Polyphonic Music Generation**](https://program.ismir2020.net/poster_5-05.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\nThis work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the **disentanglement of chords and texture** of a music piece. The architecture adopts a similar idea as their prior work called [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.\nMeeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!\n\n[**Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)**](https://program.ismir2020.net/poster_1-13.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\nMy work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the **insufficiency of labels**, or the **subjectiveness** (and hence large variance) in human-annotated labels. We propose to use low-level features as \"bridges\" to between the music and the high level features. Hence, the model consists of:\n-  **faders**, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement\n-  **presets**, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.\n\nThis method combines the advantages of **rule-based methods** and **data-driven machine learning**. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of **semi-supervised learning**. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.\n\n[**Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm**](https://program.ismir2020.net/poster_1-09.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\nThis work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to \"fill up the blanks\" with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) as well). There are 3 separate components: **SketchVAE** for latent representation learning, **SketchInpainter** for predicting missing measures based on previous and future contexts, and **SketchConnector** which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).\n\n[**The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures**](https://program.ismir2020.net/poster_1-17.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\nThis is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?\n\nFor me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall [this work](https://arxiv.org/pdf/1912.05537.pdf) suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using [fitness scape plot](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html) to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.\n\n## 2 - Disentangled Representation Learning\n\n[**Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds**](https://program.ismir2020.net/poster_5-10.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\nWork by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on [pitch-timbre disentanglement](https://arxiv.org/pdf/1906.08152.pdf) before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? \n\nThis work employs a key idea: **moderate pitch shiftings will not change timbre**. Hence, even if we don't have any labels annotated on pitch and timbre, we can still achieve disentanglement by [contrastive learning paradigms](https://paperswithcode.com/task/contrastive-learning) - data augmentation by transposing the pitch, but enforce relations in \\\\(z_\\textrm{pitch}\\\\) and \\\\(z_\\textrm{timbre}\\\\). The authors propose 4 losses: regression loss, [contrastive loss](https://arxiv.org/pdf/2002.05709.pdf), [cycle consistency loss](https://arxiv.org/pdf/1703.10593v7.pdf) and a new **surrogate label loss**. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.\n\n[**Metric learning VS classification for disentangled music representation learning**](https://program.ismir2020.net/poster_3-15.html)\n\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\nThis interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to [this work](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf)). Firstly, the authors connect classication and metric learning with **proxy-based metric learning**. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are\ngenerally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.\n\n[**dMelodies: A Music Dataset for Disentanglement Learning**](https://program.ismir2020.net/poster_1-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new dataset which resembles [dSprites](https://github.com/deepmind/dsprites-dataset) in the computer vision domain, which is designed for learning and **evaluating disentangled representation learning algorithms for music**. The authors also ran benchmark experiments using common disentanglement methods (\\\\(\\beta\\\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.\nI discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. [Nottingham dataset](https://ifdo.ca/~seymour/nottingham/nottingham.html)) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis' work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)\n\n## 3 - Singing Voice Conversion\n\n[**Zero-Shot Singing Voice Conversion**](https://program.ismir2020.net/poster_1-08.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\nThe most interesting part of this work is the **zero-shot** part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a [pretrained speaker embedding network](https://github.com/CorentinJ/Real-Time-Voice-Cloning), and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the \"pre-trained fine-tune\" idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.\n\n## 4 - Audio Synthesis\n\n[**DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks**](https://program.ismir2020.net/poster_4-16.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\nSuper cool and useful work (can't wait to use the plugin as a producer)! This work uses a **progressive growing GAN** (similar to the idea in [GANSynth]()) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or [R-score in linear regressor](http://proceedings.mlr.press/v80/adel18a/adel18a.pdf), which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in [a previous work using U-Net architecture](https://arxiv.org/pdf/1911.11853.pdf). This could probably shed light to a new spectrum of measurements in terms of factor controllability.\n\nAnother interesting thing is that this work uses **complex STFT spectrogram** as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.\n\n## 5 - Music Source Separation\n\n[**Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation**](https://program.ismir2020.net/poster_2-04.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\nU-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, **Time-Distributed Blocks** which does not have inter-frame operations, and **Time-Frequency Blocks** which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The [demo](https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab) provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.\n\n[**Content based singing voice source separation via strong conditioning using aligned phonemes**](https://program.ismir2020.net/poster_6-07.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\nThis work explores **informed source separation** - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the [FiLM](https://arxiv.org/pdf/1709.07871.pdf) layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.\n\n[**Exploring Aligned Lyrics-informed Singing Voice Separation**](https://program.ismir2020.net/poster_5-08.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\nSimilar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art [Open Unmix](https://sigsep.github.io/open-unmix/) model, then the authors propose to use an additional **lyric encoder** to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from [text-to-speech](https://paperswithcode.com/task/text-to-speech-synthesis) models, where the text information is encoded to condition on the speech synthesis component.\n\n[**Multitask Learning for Instrument Activation Aware Music Source Separation**](https://program.ismir2020.net/poster_5-16.html)\n\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\nThis work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use **instrument activation detection** as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.\n\n## 6 - Music Transcription / Pitch Estimation\n\n[**Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks**](https://program.ismir2020.net/poster_2-18.html)\n\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\nThis work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) **phase information does help** for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.\n\n[**Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams**](https://program.ismir2020.net/poster_3-01.html)\n\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\nThis is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where **deep clustering** separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an [existing multi-pitch estimator](https://brianmcfee.net/papers/ismir2017_salience.pdf)) provide complementary information for timbre-based clustering and part separation.\n\n[**Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model**](https://program.ismir2020.net/poster_3-17.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\nThis work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an **autoregressive model** by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their [demo](https://program.ismir2020.net/lbd_444.html) is super excellent, such sleek and smooth visualization on real-time music transcription!\n\n## 7 - Model Pruning\n\n[**Ultra-light deep MIR by trimming lottery ticket**](https://program.ismir2020.net/poster_4-11.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\nThe [lottery ticket hypothesis](https://arxiv.org/pdf/1803.03635.pdf) paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as **structured trimming**, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.\n\n## 8 - Cover Song Detection\n\n[**Combining musical features for cover detection**](https://program.ismir2020.net/poster_2-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\nIn previous cover song detection works, either the harmonic-related representation (e.g. [HPCP](https://www.upf.edu/web/mtg/hpcp), [cremaPCP](https://brianmcfee.net/papers/ismir2017_chord.pdf)) or the melody-related representation (e.g. [dominant melody](https://arxiv.org/pdf/1907.01824.pdf), [multi-pitch](https://arxiv.org/pdf/1910.09862.pdf)) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\\\(d_\\textrm{melody}\\\\) and \\\\(d_\\textrm{cremaPCP}\\\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.\n\n[**Less is more: Faster and better music version identification with embedding distillation**](https://program.ismir2020.net/poster_6-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\nIn [a previous work](https://arxiv.org/pdf/1910.12551.pdf), the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a **latent space reconfiguration** method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.","source":"_posts/ismir_2020.md","raw":"---\ntitle: MIR Papers 2020 (and ISMIR)\ndate: 2020-10-17 09:10:42\ntags:\n    - Music Information Retrieval\nestimatedReadTime: ~20 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 1 - Controllable Symbolic Music Generation\n\n[**Attributes-Aware Deep Music Transformation**](https://program.ismir2020.net/poster_5-06.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\nThis work uses a very similar architecture like [Fader Networks](https://arxiv.org/pdf/1706.00409.pdf) in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\\\(z\\\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in [Music FaderNets](https://program.ismir2020.net/poster_1-13.html), but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.\nOne interesting thing is that the authors do not compare results on linear correlation with [GLSR-VAE](https://arxiv.org/pdf/1707.04588.pdf), because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.\n\n[**BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)**](https://program.ismir2020.net/poster_6-08.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\nCongrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on **personalization**. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user's taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.\n\n[**Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation**](https://program.ismir2020.net/poster_1-05.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\nThis work proposes **connective fusion**, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the [Latent Constraint](https://arxiv.org/pdf/1711.05772.pdf) paper - firstly, we pretrain a VAE to learn latent code \\\\(z\\\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\\\(G\\\\) that generates latent code pair \\\\((z^\\prime_L, z^\\prime_R)\\\\) that is indistuingishable from the input pair\\\\((z_L, z_R)\\\\). During training, we also add in an additional style vector \\\\(s\\\\), hence also learning a style space which controls how the two sequences are connectively fused.\nI was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. [His work last year](http://archives.ismir.net/ismir2019/paper/000100.pdf) has also helped me a lot with Music FaderNets, so huge kudos to him!\n\n[**Generating Music with a Self-Correcting Non-Chronological Autoregressive Model**](https://program.ismir2020.net/poster_6-16.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\nI spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using **edit distance**. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against [orderless NADE](https://www.jmlr.org/papers/volume17/16-272/16-272.pdf) and [CoCoNet](https://arxiv.org/pdf/1903.07227.pdf), but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.\n\n[**PIANOTREE VAE: Structured Representation Learning for Polyphonic Music**](https://program.ismir2020.net/poster_3-06.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a **tree-like structure**, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.\nI definitely think that PianoTree has the potential to be the *de facto* representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: **the ease of usage** (e.g. open source of encoder components and examples of usage), and whether **the data structure is tightly coupled with the proposed VAE model**. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.\n\n[**Learning Interpretable Representation for Controllable Polyphonic Music Generation**](https://program.ismir2020.net/poster_5-05.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\nThis work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the **disentanglement of chords and texture** of a music piece. The architecture adopts a similar idea as their prior work called [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.\nMeeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!\n\n[**Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)**](https://program.ismir2020.net/poster_1-13.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\nMy work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the **insufficiency of labels**, or the **subjectiveness** (and hence large variance) in human-annotated labels. We propose to use low-level features as \"bridges\" to between the music and the high level features. Hence, the model consists of:\n-  **faders**, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement\n-  **presets**, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.\n\nThis method combines the advantages of **rule-based methods** and **data-driven machine learning**. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of **semi-supervised learning**. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.\n\n[**Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm**](https://program.ismir2020.net/poster_1-09.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\nThis work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to \"fill up the blanks\" with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) as well). There are 3 separate components: **SketchVAE** for latent representation learning, **SketchInpainter** for predicting missing measures based on previous and future contexts, and **SketchConnector** which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).\n\n[**The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures**](https://program.ismir2020.net/poster_1-17.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\nThis is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?\n\nFor me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall [this work](https://arxiv.org/pdf/1912.05537.pdf) suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using [fitness scape plot](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html) to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.\n\n## 2 - Disentangled Representation Learning\n\n[**Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds**](https://program.ismir2020.net/poster_5-10.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\nWork by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on [pitch-timbre disentanglement](https://arxiv.org/pdf/1906.08152.pdf) before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? \n\nThis work employs a key idea: **moderate pitch shiftings will not change timbre**. Hence, even if we don't have any labels annotated on pitch and timbre, we can still achieve disentanglement by [contrastive learning paradigms](https://paperswithcode.com/task/contrastive-learning) - data augmentation by transposing the pitch, but enforce relations in \\\\(z_\\textrm{pitch}\\\\) and \\\\(z_\\textrm{timbre}\\\\). The authors propose 4 losses: regression loss, [contrastive loss](https://arxiv.org/pdf/2002.05709.pdf), [cycle consistency loss](https://arxiv.org/pdf/1703.10593v7.pdf) and a new **surrogate label loss**. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.\n\n[**Metric learning VS classification for disentangled music representation learning**](https://program.ismir2020.net/poster_3-15.html)\n\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\nThis interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to [this work](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf)). Firstly, the authors connect classication and metric learning with **proxy-based metric learning**. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are\ngenerally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.\n\n[**dMelodies: A Music Dataset for Disentanglement Learning**](https://program.ismir2020.net/poster_1-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new dataset which resembles [dSprites](https://github.com/deepmind/dsprites-dataset) in the computer vision domain, which is designed for learning and **evaluating disentangled representation learning algorithms for music**. The authors also ran benchmark experiments using common disentanglement methods (\\\\(\\beta\\\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.\nI discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. [Nottingham dataset](https://ifdo.ca/~seymour/nottingham/nottingham.html)) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis' work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)\n\n## 3 - Singing Voice Conversion\n\n[**Zero-Shot Singing Voice Conversion**](https://program.ismir2020.net/poster_1-08.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\nThe most interesting part of this work is the **zero-shot** part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a [pretrained speaker embedding network](https://github.com/CorentinJ/Real-Time-Voice-Cloning), and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the \"pre-trained fine-tune\" idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.\n\n## 4 - Audio Synthesis\n\n[**DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks**](https://program.ismir2020.net/poster_4-16.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\nSuper cool and useful work (can't wait to use the plugin as a producer)! This work uses a **progressive growing GAN** (similar to the idea in [GANSynth]()) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or [R-score in linear regressor](http://proceedings.mlr.press/v80/adel18a/adel18a.pdf), which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in [a previous work using U-Net architecture](https://arxiv.org/pdf/1911.11853.pdf). This could probably shed light to a new spectrum of measurements in terms of factor controllability.\n\nAnother interesting thing is that this work uses **complex STFT spectrogram** as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.\n\n## 5 - Music Source Separation\n\n[**Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation**](https://program.ismir2020.net/poster_2-04.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\nU-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, **Time-Distributed Blocks** which does not have inter-frame operations, and **Time-Frequency Blocks** which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The [demo](https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab) provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.\n\n[**Content based singing voice source separation via strong conditioning using aligned phonemes**](https://program.ismir2020.net/poster_6-07.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\nThis work explores **informed source separation** - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the [FiLM](https://arxiv.org/pdf/1709.07871.pdf) layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.\n\n[**Exploring Aligned Lyrics-informed Singing Voice Separation**](https://program.ismir2020.net/poster_5-08.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\nSimilar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art [Open Unmix](https://sigsep.github.io/open-unmix/) model, then the authors propose to use an additional **lyric encoder** to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from [text-to-speech](https://paperswithcode.com/task/text-to-speech-synthesis) models, where the text information is encoded to condition on the speech synthesis component.\n\n[**Multitask Learning for Instrument Activation Aware Music Source Separation**](https://program.ismir2020.net/poster_5-16.html)\n\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\nThis work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use **instrument activation detection** as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.\n\n## 6 - Music Transcription / Pitch Estimation\n\n[**Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks**](https://program.ismir2020.net/poster_2-18.html)\n\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\nThis work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) **phase information does help** for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.\n\n[**Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams**](https://program.ismir2020.net/poster_3-01.html)\n\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\nThis is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where **deep clustering** separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an [existing multi-pitch estimator](https://brianmcfee.net/papers/ismir2017_salience.pdf)) provide complementary information for timbre-based clustering and part separation.\n\n[**Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model**](https://program.ismir2020.net/poster_3-17.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\nThis work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an **autoregressive model** by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their [demo](https://program.ismir2020.net/lbd_444.html) is super excellent, such sleek and smooth visualization on real-time music transcription!\n\n## 7 - Model Pruning\n\n[**Ultra-light deep MIR by trimming lottery ticket**](https://program.ismir2020.net/poster_4-11.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\nThe [lottery ticket hypothesis](https://arxiv.org/pdf/1803.03635.pdf) paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as **structured trimming**, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.\n\n## 8 - Cover Song Detection\n\n[**Combining musical features for cover detection**](https://program.ismir2020.net/poster_2-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\nIn previous cover song detection works, either the harmonic-related representation (e.g. [HPCP](https://www.upf.edu/web/mtg/hpcp), [cremaPCP](https://brianmcfee.net/papers/ismir2017_chord.pdf)) or the melody-related representation (e.g. [dominant melody](https://arxiv.org/pdf/1907.01824.pdf), [multi-pitch](https://arxiv.org/pdf/1910.09862.pdf)) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\\\(d_\\textrm{melody}\\\\) and \\\\(d_\\textrm{cremaPCP}\\\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.\n\n[**Less is more: Faster and better music version identification with embedding distillation**](https://program.ismir2020.net/poster_6-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\nIn [a previous work](https://arxiv.org/pdf/1910.12551.pdf), the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a **latent space reconfiguration** method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.","slug":"ismir_2020","published":1,"updated":"2025-06-27T10:09:12.826Z","_id":"ckgcjeans0008w19k9uev444u","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"1-Controllable-Symbolic-Music-Generation\"><a href=\"#1-Controllable-Symbolic-Music-Generation\" class=\"headerlink\" title=\"1 - Controllable Symbolic Music Generation\"></a>1 - Controllable Symbolic Music Generation</h2><p><a href=\"https://program.ismir2020.net/poster_5-06.html\" target=\"_blank\" rel=\"noopener\"><strong>Attributes-Aware Deep Music Transformation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\n<p>This work uses a very similar architecture like <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks</a> in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\(z\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in <a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\">Music FaderNets</a>, but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.<br>One interesting thing is that the authors do not compare results on linear correlation with <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE</a>, because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-08.html\" target=\"_blank\" rel=\"noopener\"><strong>BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\n<p>Congrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on <strong>personalization</strong>. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user’s taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes <strong>connective fusion</strong>, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraint</a> paper - firstly, we pretrain a VAE to learn latent code \\(z\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\(G\\) that generates latent code pair \\((z^\\prime_L, z^\\prime_R)\\) that is indistuingishable from the input pair\\((z_L, z_R)\\). During training, we also add in an additional style vector \\(s\\), hence also learning a style space which controls how the two sequences are connectively fused.<br>I was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\">His work last year</a> has also helped me a lot with Music FaderNets, so huge kudos to him!</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Generating Music with a Self-Correcting Non-Chronological Autoregressive Model</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\n<p>I spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using <strong>edit distance</strong>. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against <a href=\"https://www.jmlr.org/papers/volume17/16-272/16-272.pdf\" target=\"_blank\" rel=\"noopener\">orderless NADE</a> and <a href=\"https://arxiv.org/pdf/1903.07227.pdf\" target=\"_blank\" rel=\"noopener\">CoCoNet</a>, but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-06.html\" target=\"_blank\" rel=\"noopener\"><strong>PIANOTREE VAE: Structured Representation Learning for Polyphonic Music</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a <strong>tree-like structure</strong>, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.<br>I definitely think that PianoTree has the potential to be the <em>de facto</em> representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: <strong>the ease of usage</strong> (e.g. open source of encoder components and examples of usage), and whether <strong>the data structure is tightly coupled with the proposed VAE model</strong>. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Learning Interpretable Representation for Controllable Polyphonic Music Generation</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the <strong>disentanglement of chords and texture</strong> of a music piece. The architecture adopts a similar idea as their prior work called <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.<br>Meeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\"><strong>Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\n<p>My work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the <strong>insufficiency of labels</strong>, or the <strong>subjectiveness</strong> (and hence large variance) in human-annotated labels. We propose to use low-level features as “bridges” to between the music and the high level features. Hence, the model consists of:</p>\n<ul>\n<li><strong>faders</strong>, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement</li>\n<li><strong>presets</strong>, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.</li>\n</ul>\n<p>This method combines the advantages of <strong>rule-based methods</strong> and <strong>data-driven machine learning</strong>. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of <strong>semi-supervised learning</strong>. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-09.html\" target=\"_blank\" rel=\"noopener\"><strong>Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to “fill up the blanks” with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> as well). There are 3 separate components: <strong>SketchVAE</strong> for latent representation learning, <strong>SketchInpainter</strong> for predicting missing measures based on previous and future contexts, and <strong>SketchConnector</strong> which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-17.html\" target=\"_blank\" rel=\"noopener\"><strong>The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\n<p>This is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?</p>\n<p>For me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">this work</a> suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using <a href=\"https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html\" target=\"_blank\" rel=\"noopener\">fitness scape plot</a> to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.</p>\n<h2 id=\"2-Disentangled-Representation-Learning\"><a href=\"#2-Disentangled-Representation-Learning\" class=\"headerlink\" title=\"2 - Disentangled Representation Learning\"></a>2 - Disentangled Representation Learning</h2><p><a href=\"https://program.ismir2020.net/poster_5-10.html\" target=\"_blank\" rel=\"noopener\"><strong>Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\n<p>Work by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">pitch-timbre disentanglement</a> before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? </p>\n<p>This work employs a key idea: <strong>moderate pitch shiftings will not change timbre</strong>. Hence, even if we don’t have any labels annotated on pitch and timbre, we can still achieve disentanglement by <a href=\"https://paperswithcode.com/task/contrastive-learning\" target=\"_blank\" rel=\"noopener\">contrastive learning paradigms</a> - data augmentation by transposing the pitch, but enforce relations in \\(z_\\textrm{pitch}\\) and \\(z_\\textrm{timbre}\\). The authors propose 4 losses: regression loss, <a href=\"https://arxiv.org/pdf/2002.05709.pdf\" target=\"_blank\" rel=\"noopener\">contrastive loss</a>, <a href=\"https://arxiv.org/pdf/1703.10593v7.pdf\" target=\"_blank\" rel=\"noopener\">cycle consistency loss</a> and a new <strong>surrogate label loss</strong>. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Metric learning VS classification for disentangled music representation learning</strong></a></p>\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\n<p>This interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to <a href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf\" target=\"_blank\" rel=\"noopener\">this work</a>). Firstly, the authors connect classication and metric learning with <strong>proxy-based metric learning</strong>. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are<br>generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-15.html\" target=\"_blank\" rel=\"noopener\"><strong>dMelodies: A Music Dataset for Disentanglement Learning</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new dataset which resembles <a href=\"https://github.com/deepmind/dsprites-dataset\" target=\"_blank\" rel=\"noopener\">dSprites</a> in the computer vision domain, which is designed for learning and <strong>evaluating disentangled representation learning algorithms for music</strong>. The authors also ran benchmark experiments using common disentanglement methods (\\(\\beta\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.<br>I discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. <a href=\"https://ifdo.ca/~seymour/nottingham/nottingham.html\" target=\"_blank\" rel=\"noopener\">Nottingham dataset</a>) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis’ work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)</p>\n<h2 id=\"3-Singing-Voice-Conversion\"><a href=\"#3-Singing-Voice-Conversion\" class=\"headerlink\" title=\"3 - Singing Voice Conversion\"></a>3 - Singing Voice Conversion</h2><p><a href=\"https://program.ismir2020.net/poster_1-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Zero-Shot Singing Voice Conversion</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\n<p>The most interesting part of this work is the <strong>zero-shot</strong> part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a <a href=\"https://github.com/CorentinJ/Real-Time-Voice-Cloning\" target=\"_blank\" rel=\"noopener\">pretrained speaker embedding network</a>, and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the “pre-trained fine-tune” idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.</p>\n<h2 id=\"4-Audio-Synthesis\"><a href=\"#4-Audio-Synthesis\" class=\"headerlink\" title=\"4 - Audio Synthesis\"></a>4 - Audio Synthesis</h2><p><a href=\"https://program.ismir2020.net/poster_4-16.html\" target=\"_blank\" rel=\"noopener\"><strong>DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\n<p>Super cool and useful work (can’t wait to use the plugin as a producer)! This work uses a <strong>progressive growing GAN</strong> (similar to the idea in <a href=\"\">GANSynth</a>) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or <a href=\"http://proceedings.mlr.press/v80/adel18a/adel18a.pdf\">R-score in linear regressor</a>, which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in <a href=\"https://arxiv.org/pdf/1911.11853.pdf\" target=\"_blank\" rel=\"noopener\">a previous work using U-Net architecture</a>. This could probably shed light to a new spectrum of measurements in terms of factor controllability.</p>\n<p>Another interesting thing is that this work uses <strong>complex STFT spectrogram</strong> as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.</p>\n<h2 id=\"5-Music-Source-Separation\"><a href=\"#5-Music-Source-Separation\" class=\"headerlink\" title=\"5 - Music Source Separation\"></a>5 - Music Source Separation</h2><p><a href=\"https://program.ismir2020.net/poster_2-04.html\" target=\"_blank\" rel=\"noopener\"><strong>Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\n<p>U-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, <strong>Time-Distributed Blocks</strong> which does not have inter-frame operations, and <strong>Time-Frequency Blocks</strong> which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The <a href=\"https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab\" target=\"_blank\" rel=\"noopener\">demo</a> provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-07.html\" target=\"_blank\" rel=\"noopener\"><strong>Content based singing voice source separation via strong conditioning using aligned phonemes</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores <strong>informed source separation</strong> - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the <a href=\"https://arxiv.org/pdf/1709.07871.pdf\" target=\"_blank\" rel=\"noopener\">FiLM</a> layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Exploring Aligned Lyrics-informed Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\n<p>Similar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art <a href=\"https://sigsep.github.io/open-unmix/\" target=\"_blank\" rel=\"noopener\">Open Unmix</a> model, then the authors propose to use an additional <strong>lyric encoder</strong> to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from <a href=\"https://paperswithcode.com/task/text-to-speech-synthesis\" target=\"_blank\" rel=\"noopener\">text-to-speech</a> models, where the text information is encoded to condition on the speech synthesis component.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Multitask Learning for Instrument Activation Aware Music Source Separation</strong></a></p>\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\n<p>This work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use <strong>instrument activation detection</strong> as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.</p>\n<h2 id=\"6-Music-Transcription-Pitch-Estimation\"><a href=\"#6-Music-Transcription-Pitch-Estimation\" class=\"headerlink\" title=\"6 - Music Transcription / Pitch Estimation\"></a>6 - Music Transcription / Pitch Estimation</h2><p><a href=\"https://program.ismir2020.net/poster_2-18.html\" target=\"_blank\" rel=\"noopener\"><strong>Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks</strong></a></p>\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) <strong>phase information does help</strong> for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-01.html\" target=\"_blank\" rel=\"noopener\"><strong>Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams</strong></a></p>\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\n<p>This is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where <strong>deep clustering</strong> separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">existing multi-pitch estimator</a>) provide complementary information for timbre-based clustering and part separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-17.html\" target=\"_blank\" rel=\"noopener\"><strong>Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\n<p>This work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an <strong>autoregressive model</strong> by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their <a href=\"https://program.ismir2020.net/lbd_444.html\" target=\"_blank\" rel=\"noopener\">demo</a> is super excellent, such sleek and smooth visualization on real-time music transcription!</p>\n<h2 id=\"7-Model-Pruning\"><a href=\"#7-Model-Pruning\" class=\"headerlink\" title=\"7 - Model Pruning\"></a>7 - Model Pruning</h2><p><a href=\"https://program.ismir2020.net/poster_4-11.html\" target=\"_blank\" rel=\"noopener\"><strong>Ultra-light deep MIR by trimming lottery ticket</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\n<p>The <a href=\"https://arxiv.org/pdf/1803.03635.pdf\" target=\"_blank\" rel=\"noopener\">lottery ticket hypothesis</a> paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as <strong>structured trimming</strong>, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.</p>\n<h2 id=\"8-Cover-Song-Detection\"><a href=\"#8-Cover-Song-Detection\" class=\"headerlink\" title=\"8 - Cover Song Detection\"></a>8 - Cover Song Detection</h2><p><a href=\"https://program.ismir2020.net/poster_2-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Combining musical features for cover detection</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\n<p>In previous cover song detection works, either the harmonic-related representation (e.g. <a href=\"https://www.upf.edu/web/mtg/hpcp\" target=\"_blank\" rel=\"noopener\">HPCP</a>, <a href=\"https://brianmcfee.net/papers/ismir2017_chord.pdf\" target=\"_blank\" rel=\"noopener\">cremaPCP</a>) or the melody-related representation (e.g. <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">dominant melody</a>, <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">multi-pitch</a>) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\(d_\\textrm{melody}\\) and \\(d_\\textrm{cremaPCP}\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Less is more: Faster and better music version identification with embedding distillation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\n<p>In <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">a previous work</a>, the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a <strong>latent space reconfiguration</strong> method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"1-Controllable-Symbolic-Music-Generation\"><a href=\"#1-Controllable-Symbolic-Music-Generation\" class=\"headerlink\" title=\"1 - Controllable Symbolic Music Generation\"></a>1 - Controllable Symbolic Music Generation</h2><p><a href=\"https://program.ismir2020.net/poster_5-06.html\" target=\"_blank\" rel=\"noopener\"><strong>Attributes-Aware Deep Music Transformation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\n<p>This work uses a very similar architecture like <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks</a> in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\(z\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in <a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\">Music FaderNets</a>, but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.<br>One interesting thing is that the authors do not compare results on linear correlation with <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE</a>, because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-08.html\" target=\"_blank\" rel=\"noopener\"><strong>BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\n<p>Congrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on <strong>personalization</strong>. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user’s taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes <strong>connective fusion</strong>, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraint</a> paper - firstly, we pretrain a VAE to learn latent code \\(z\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\(G\\) that generates latent code pair \\((z^\\prime_L, z^\\prime_R)\\) that is indistuingishable from the input pair\\((z_L, z_R)\\). During training, we also add in an additional style vector \\(s\\), hence also learning a style space which controls how the two sequences are connectively fused.<br>I was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\">His work last year</a> has also helped me a lot with Music FaderNets, so huge kudos to him!</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Generating Music with a Self-Correcting Non-Chronological Autoregressive Model</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\n<p>I spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using <strong>edit distance</strong>. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against <a href=\"https://www.jmlr.org/papers/volume17/16-272/16-272.pdf\" target=\"_blank\" rel=\"noopener\">orderless NADE</a> and <a href=\"https://arxiv.org/pdf/1903.07227.pdf\" target=\"_blank\" rel=\"noopener\">CoCoNet</a>, but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-06.html\" target=\"_blank\" rel=\"noopener\"><strong>PIANOTREE VAE: Structured Representation Learning for Polyphonic Music</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a <strong>tree-like structure</strong>, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.<br>I definitely think that PianoTree has the potential to be the <em>de facto</em> representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: <strong>the ease of usage</strong> (e.g. open source of encoder components and examples of usage), and whether <strong>the data structure is tightly coupled with the proposed VAE model</strong>. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Learning Interpretable Representation for Controllable Polyphonic Music Generation</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the <strong>disentanglement of chords and texture</strong> of a music piece. The architecture adopts a similar idea as their prior work called <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.<br>Meeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\"><strong>Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\n<p>My work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the <strong>insufficiency of labels</strong>, or the <strong>subjectiveness</strong> (and hence large variance) in human-annotated labels. We propose to use low-level features as “bridges” to between the music and the high level features. Hence, the model consists of:</p>\n<ul>\n<li><strong>faders</strong>, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement</li>\n<li><strong>presets</strong>, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.</li>\n</ul>\n<p>This method combines the advantages of <strong>rule-based methods</strong> and <strong>data-driven machine learning</strong>. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of <strong>semi-supervised learning</strong>. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-09.html\" target=\"_blank\" rel=\"noopener\"><strong>Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to “fill up the blanks” with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> as well). There are 3 separate components: <strong>SketchVAE</strong> for latent representation learning, <strong>SketchInpainter</strong> for predicting missing measures based on previous and future contexts, and <strong>SketchConnector</strong> which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-17.html\" target=\"_blank\" rel=\"noopener\"><strong>The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\n<p>This is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?</p>\n<p>For me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">this work</a> suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using <a href=\"https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html\" target=\"_blank\" rel=\"noopener\">fitness scape plot</a> to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.</p>\n<h2 id=\"2-Disentangled-Representation-Learning\"><a href=\"#2-Disentangled-Representation-Learning\" class=\"headerlink\" title=\"2 - Disentangled Representation Learning\"></a>2 - Disentangled Representation Learning</h2><p><a href=\"https://program.ismir2020.net/poster_5-10.html\" target=\"_blank\" rel=\"noopener\"><strong>Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\n<p>Work by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">pitch-timbre disentanglement</a> before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? </p>\n<p>This work employs a key idea: <strong>moderate pitch shiftings will not change timbre</strong>. Hence, even if we don’t have any labels annotated on pitch and timbre, we can still achieve disentanglement by <a href=\"https://paperswithcode.com/task/contrastive-learning\" target=\"_blank\" rel=\"noopener\">contrastive learning paradigms</a> - data augmentation by transposing the pitch, but enforce relations in \\(z_\\textrm{pitch}\\) and \\(z_\\textrm{timbre}\\). The authors propose 4 losses: regression loss, <a href=\"https://arxiv.org/pdf/2002.05709.pdf\" target=\"_blank\" rel=\"noopener\">contrastive loss</a>, <a href=\"https://arxiv.org/pdf/1703.10593v7.pdf\" target=\"_blank\" rel=\"noopener\">cycle consistency loss</a> and a new <strong>surrogate label loss</strong>. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Metric learning VS classification for disentangled music representation learning</strong></a></p>\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\n<p>This interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to <a href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf\" target=\"_blank\" rel=\"noopener\">this work</a>). Firstly, the authors connect classication and metric learning with <strong>proxy-based metric learning</strong>. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are<br>generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-15.html\" target=\"_blank\" rel=\"noopener\"><strong>dMelodies: A Music Dataset for Disentanglement Learning</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new dataset which resembles <a href=\"https://github.com/deepmind/dsprites-dataset\" target=\"_blank\" rel=\"noopener\">dSprites</a> in the computer vision domain, which is designed for learning and <strong>evaluating disentangled representation learning algorithms for music</strong>. The authors also ran benchmark experiments using common disentanglement methods (\\(\\beta\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.<br>I discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. <a href=\"https://ifdo.ca/~seymour/nottingham/nottingham.html\" target=\"_blank\" rel=\"noopener\">Nottingham dataset</a>) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis’ work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)</p>\n<h2 id=\"3-Singing-Voice-Conversion\"><a href=\"#3-Singing-Voice-Conversion\" class=\"headerlink\" title=\"3 - Singing Voice Conversion\"></a>3 - Singing Voice Conversion</h2><p><a href=\"https://program.ismir2020.net/poster_1-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Zero-Shot Singing Voice Conversion</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\n<p>The most interesting part of this work is the <strong>zero-shot</strong> part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a <a href=\"https://github.com/CorentinJ/Real-Time-Voice-Cloning\" target=\"_blank\" rel=\"noopener\">pretrained speaker embedding network</a>, and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the “pre-trained fine-tune” idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.</p>\n<h2 id=\"4-Audio-Synthesis\"><a href=\"#4-Audio-Synthesis\" class=\"headerlink\" title=\"4 - Audio Synthesis\"></a>4 - Audio Synthesis</h2><p><a href=\"https://program.ismir2020.net/poster_4-16.html\" target=\"_blank\" rel=\"noopener\"><strong>DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\n<p>Super cool and useful work (can’t wait to use the plugin as a producer)! This work uses a <strong>progressive growing GAN</strong> (similar to the idea in <a href=\"\">GANSynth</a>) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or <a href=\"http://proceedings.mlr.press/v80/adel18a/adel18a.pdf\">R-score in linear regressor</a>, which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in <a href=\"https://arxiv.org/pdf/1911.11853.pdf\" target=\"_blank\" rel=\"noopener\">a previous work using U-Net architecture</a>. This could probably shed light to a new spectrum of measurements in terms of factor controllability.</p>\n<p>Another interesting thing is that this work uses <strong>complex STFT spectrogram</strong> as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.</p>\n<h2 id=\"5-Music-Source-Separation\"><a href=\"#5-Music-Source-Separation\" class=\"headerlink\" title=\"5 - Music Source Separation\"></a>5 - Music Source Separation</h2><p><a href=\"https://program.ismir2020.net/poster_2-04.html\" target=\"_blank\" rel=\"noopener\"><strong>Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\n<p>U-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, <strong>Time-Distributed Blocks</strong> which does not have inter-frame operations, and <strong>Time-Frequency Blocks</strong> which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The <a href=\"https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab\" target=\"_blank\" rel=\"noopener\">demo</a> provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-07.html\" target=\"_blank\" rel=\"noopener\"><strong>Content based singing voice source separation via strong conditioning using aligned phonemes</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores <strong>informed source separation</strong> - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the <a href=\"https://arxiv.org/pdf/1709.07871.pdf\" target=\"_blank\" rel=\"noopener\">FiLM</a> layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Exploring Aligned Lyrics-informed Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\n<p>Similar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art <a href=\"https://sigsep.github.io/open-unmix/\" target=\"_blank\" rel=\"noopener\">Open Unmix</a> model, then the authors propose to use an additional <strong>lyric encoder</strong> to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from <a href=\"https://paperswithcode.com/task/text-to-speech-synthesis\" target=\"_blank\" rel=\"noopener\">text-to-speech</a> models, where the text information is encoded to condition on the speech synthesis component.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Multitask Learning for Instrument Activation Aware Music Source Separation</strong></a></p>\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\n<p>This work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use <strong>instrument activation detection</strong> as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.</p>\n<h2 id=\"6-Music-Transcription-Pitch-Estimation\"><a href=\"#6-Music-Transcription-Pitch-Estimation\" class=\"headerlink\" title=\"6 - Music Transcription / Pitch Estimation\"></a>6 - Music Transcription / Pitch Estimation</h2><p><a href=\"https://program.ismir2020.net/poster_2-18.html\" target=\"_blank\" rel=\"noopener\"><strong>Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks</strong></a></p>\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) <strong>phase information does help</strong> for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-01.html\" target=\"_blank\" rel=\"noopener\"><strong>Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams</strong></a></p>\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\n<p>This is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where <strong>deep clustering</strong> separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">existing multi-pitch estimator</a>) provide complementary information for timbre-based clustering and part separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-17.html\" target=\"_blank\" rel=\"noopener\"><strong>Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\n<p>This work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an <strong>autoregressive model</strong> by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their <a href=\"https://program.ismir2020.net/lbd_444.html\" target=\"_blank\" rel=\"noopener\">demo</a> is super excellent, such sleek and smooth visualization on real-time music transcription!</p>\n<h2 id=\"7-Model-Pruning\"><a href=\"#7-Model-Pruning\" class=\"headerlink\" title=\"7 - Model Pruning\"></a>7 - Model Pruning</h2><p><a href=\"https://program.ismir2020.net/poster_4-11.html\" target=\"_blank\" rel=\"noopener\"><strong>Ultra-light deep MIR by trimming lottery ticket</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\n<p>The <a href=\"https://arxiv.org/pdf/1803.03635.pdf\" target=\"_blank\" rel=\"noopener\">lottery ticket hypothesis</a> paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as <strong>structured trimming</strong>, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.</p>\n<h2 id=\"8-Cover-Song-Detection\"><a href=\"#8-Cover-Song-Detection\" class=\"headerlink\" title=\"8 - Cover Song Detection\"></a>8 - Cover Song Detection</h2><p><a href=\"https://program.ismir2020.net/poster_2-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Combining musical features for cover detection</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\n<p>In previous cover song detection works, either the harmonic-related representation (e.g. <a href=\"https://www.upf.edu/web/mtg/hpcp\" target=\"_blank\" rel=\"noopener\">HPCP</a>, <a href=\"https://brianmcfee.net/papers/ismir2017_chord.pdf\" target=\"_blank\" rel=\"noopener\">cremaPCP</a>) or the melody-related representation (e.g. <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">dominant melody</a>, <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">multi-pitch</a>) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\(d_\\textrm{melody}\\) and \\(d_\\textrm{cremaPCP}\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Less is more: Faster and better music version identification with embedding distillation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\n<p>In <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">a previous work</a>, the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a <strong>latent space reconfiguration</strong> method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.</p>\n"},{"title":"Parameterized Pooling Layers","date":"2020-11-25T02:24:37.000Z","estimatedReadTime":"~8 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Two parameterized pooling layers which aim to improve beyond average and max pooling\n2 - The techniques introduced are: [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf)\n<br/>\n\n## 1 - Introduction\n\nPooling layers in deep learning serve the purpose of **aggregating information** - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? \n\nThe very first encounter of most deep learning practitioners with pooling layers should be within the stack of \"conv - pool - relu\" block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to **downsample** the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on **temporal aggregation** for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\\\((d,)\\\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\\\((M, T)\\\\), where \\\\(T\\\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.\n\nThe most common pooling methods are either **average pooling** or **max pooling**. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion & democracy, whereas max pooling is more like tyranny & eliticism where only the best speaks.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\nThe also explains why average pooling produces **smoother, blurrer** feature maps, and max pooling produces **sharper, discontinuous** feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: **is there a way to exploit states between average and max pooling?** Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\\\(p\\\\), such that it gives us the best pooled output for our own application? This is the motivation of **parameterized / adaptive pooling** methods.\n\nBelow I will discuss two methods that I recently read up, which is [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf). Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.\n\n<br/>\n\n## 2 - AutoPool\n\nAutoPool, proposed by McFee et al, generalizes the pooling equation as below:\n\n$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(\\alpha \\in [0, \\infty)\\\\) is the trainable scalar parameter.\n\nWe can easily see that this equation takes the form of a **weighted sum** - each element \\\\(x\\\\), contributes to the final aggregated value \\\\(y\\\\), with a weight factor determined by function \\\\(w\\\\).\n\n1. when \\\\(\\alpha = 0\\\\), it is clear that \\\\(w(\\alpha, x) = \\frac{1}{|X|}\\\\) because \\\\(e^{\\alpha \\cdot x} = 1\\\\) and the denominator resembles the number of elements in \\\\(X\\\\). The corresponds to **average pooling**, and each value has equal contribution.\n\n2. when \\\\(\\alpha = 1\\\\), the authors term this as **softmax pooling**, as each value contributes with a factor of its softmax value.\n\n3. when \\\\(\\alpha \\to \\infty\\\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} = \n\\displaystyle\\lim_{\\alpha \\to \\infty}\n\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + ... }$$ Hence, by dividing \\\\(x_{max}\\\\) on both numerator and denominator, we can see that only if \\\\(x = x_{max}\\\\), then the limit equals to \\\\(1\\\\), or else the limit equals to \\\\(0\\\\). We can see that this corresponds to **max pooling**.\n\n<br/>\n\n## 2 - Generalized Mean Pooling (GeMPool)\n\nGeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:\n\n$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(p \\in [1, \\infty)\\\\) is the trainable scalar parameter.\n\n1. when \\\\(p = 1\\\\), this clearly corresponds to **average pooling**;\n\n2. when \\\\(p \\to \\infty\\\\), it corresponds to **max pooling**. A way to prove this is to calculate the following limit:\n$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + ...)^\\frac{1}{p} = x_{max}$$\n\n<br/>\n\n## 3 - Other Aggregating Mechanisms\n\nBoth methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the **exponent** of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that **the contributing factor of large values increase**, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn't been conducted before to the best of my knowledge.\n\nA more sophisticated method of aggregating values is to use **attention**, as a weightage is learnt for each value, known as **attention mask**, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.\n\n<br/>\n\n## 4 - Code Implementation\n\nI provide the portals to the original source code / reimplementation of the parameterized pooling methods:\n1. [AutoPool official implementation in Keras](https://github.com/marl/autopool/blob/master/autopool/autopool.py)\n2. [Generalized Mean Pooling reimplementation in PyTorch](https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py)\n3. [Github Gist on both pooling methods in TF2 Keras](https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01) \n\n\n\n\n\n","source":"_posts/param-pooling.md","raw":"---\ntitle: Parameterized Pooling Layers\ndate: 2020-11-25 10:24:37\ntags:\n    - Music Signal Processing\n    - Deep Learning\nestimatedReadTime: ~8 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Two parameterized pooling layers which aim to improve beyond average and max pooling\n2 - The techniques introduced are: [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf)\n<br/>\n\n## 1 - Introduction\n\nPooling layers in deep learning serve the purpose of **aggregating information** - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? \n\nThe very first encounter of most deep learning practitioners with pooling layers should be within the stack of \"conv - pool - relu\" block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to **downsample** the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on **temporal aggregation** for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\\\((d,)\\\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\\\((M, T)\\\\), where \\\\(T\\\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.\n\nThe most common pooling methods are either **average pooling** or **max pooling**. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion & democracy, whereas max pooling is more like tyranny & eliticism where only the best speaks.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\nThe also explains why average pooling produces **smoother, blurrer** feature maps, and max pooling produces **sharper, discontinuous** feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: **is there a way to exploit states between average and max pooling?** Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\\\(p\\\\), such that it gives us the best pooled output for our own application? This is the motivation of **parameterized / adaptive pooling** methods.\n\nBelow I will discuss two methods that I recently read up, which is [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf). Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.\n\n<br/>\n\n## 2 - AutoPool\n\nAutoPool, proposed by McFee et al, generalizes the pooling equation as below:\n\n$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(\\alpha \\in [0, \\infty)\\\\) is the trainable scalar parameter.\n\nWe can easily see that this equation takes the form of a **weighted sum** - each element \\\\(x\\\\), contributes to the final aggregated value \\\\(y\\\\), with a weight factor determined by function \\\\(w\\\\).\n\n1. when \\\\(\\alpha = 0\\\\), it is clear that \\\\(w(\\alpha, x) = \\frac{1}{|X|}\\\\) because \\\\(e^{\\alpha \\cdot x} = 1\\\\) and the denominator resembles the number of elements in \\\\(X\\\\). The corresponds to **average pooling**, and each value has equal contribution.\n\n2. when \\\\(\\alpha = 1\\\\), the authors term this as **softmax pooling**, as each value contributes with a factor of its softmax value.\n\n3. when \\\\(\\alpha \\to \\infty\\\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} = \n\\displaystyle\\lim_{\\alpha \\to \\infty}\n\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + ... }$$ Hence, by dividing \\\\(x_{max}\\\\) on both numerator and denominator, we can see that only if \\\\(x = x_{max}\\\\), then the limit equals to \\\\(1\\\\), or else the limit equals to \\\\(0\\\\). We can see that this corresponds to **max pooling**.\n\n<br/>\n\n## 2 - Generalized Mean Pooling (GeMPool)\n\nGeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:\n\n$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(p \\in [1, \\infty)\\\\) is the trainable scalar parameter.\n\n1. when \\\\(p = 1\\\\), this clearly corresponds to **average pooling**;\n\n2. when \\\\(p \\to \\infty\\\\), it corresponds to **max pooling**. A way to prove this is to calculate the following limit:\n$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + ...)^\\frac{1}{p} = x_{max}$$\n\n<br/>\n\n## 3 - Other Aggregating Mechanisms\n\nBoth methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the **exponent** of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that **the contributing factor of large values increase**, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn't been conducted before to the best of my knowledge.\n\nA more sophisticated method of aggregating values is to use **attention**, as a weightage is learnt for each value, known as **attention mask**, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.\n\n<br/>\n\n## 4 - Code Implementation\n\nI provide the portals to the original source code / reimplementation of the parameterized pooling methods:\n1. [AutoPool official implementation in Keras](https://github.com/marl/autopool/blob/master/autopool/autopool.py)\n2. [Generalized Mean Pooling reimplementation in PyTorch](https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py)\n3. [Github Gist on both pooling methods in TF2 Keras](https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01) \n\n\n\n\n\n","slug":"param-pooling","published":1,"updated":"2025-06-27T10:09:28.033Z","_id":"ckhwutnte00002f9kgkfwhopl","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Two parameterized pooling layers which aim to improve beyond average and max pooling<br>2 - The techniques introduced are: <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a><br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Pooling layers in deep learning serve the purpose of <strong>aggregating information</strong> - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? </p>\n<p>The very first encounter of most deep learning practitioners with pooling layers should be within the stack of “conv - pool - relu” block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to <strong>downsample</strong> the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on <strong>temporal aggregation</strong> for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\((d,)\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\((M, T)\\), where \\(T\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.</p>\n<p>The most common pooling methods are either <strong>average pooling</strong> or <strong>max pooling</strong>. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion &amp; democracy, whereas max pooling is more like tyranny &amp; eliticism where only the best speaks.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\n<p>The also explains why average pooling produces <strong>smoother, blurrer</strong> feature maps, and max pooling produces <strong>sharper, discontinuous</strong> feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: <strong>is there a way to exploit states between average and max pooling?</strong> Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\(p\\), such that it gives us the best pooled output for our own application? This is the motivation of <strong>parameterized / adaptive pooling</strong> methods.</p>\n<p>Below I will discuss two methods that I recently read up, which is <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a>. Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.</p>\n<br/>\n\n<h2 id=\"2-AutoPool\"><a href=\"#2-AutoPool\" class=\"headerlink\" title=\"2 - AutoPool\"></a>2 - AutoPool</h2><p>AutoPool, proposed by McFee et al, generalizes the pooling equation as below:</p>\n<p>$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(\\alpha \\in [0, \\infty)\\) is the trainable scalar parameter.</p>\n<p>We can easily see that this equation takes the form of a <strong>weighted sum</strong> - each element \\(x\\), contributes to the final aggregated value \\(y\\), with a weight factor determined by function \\(w\\).</p>\n<ol>\n<li><p>when \\(\\alpha = 0\\), it is clear that \\(w(\\alpha, x) = \\frac{1}{|X|}\\) because \\(e^{\\alpha \\cdot x} = 1\\) and the denominator resembles the number of elements in \\(X\\). The corresponds to <strong>average pooling</strong>, and each value has equal contribution.</p>\n</li>\n<li><p>when \\(\\alpha = 1\\), the authors term this as <strong>softmax pooling</strong>, as each value contributes with a factor of its softmax value.</p>\n</li>\n<li><p>when \\(\\alpha \\to \\infty\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} =<br>\\displaystyle\\lim_{\\alpha \\to \\infty}<br>\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + … }$$ Hence, by dividing \\(x_{max}\\) on both numerator and denominator, we can see that only if \\(x = x_{max}\\), then the limit equals to \\(1\\), or else the limit equals to \\(0\\). We can see that this corresponds to <strong>max pooling</strong>.</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"2-Generalized-Mean-Pooling-GeMPool\"><a href=\"#2-Generalized-Mean-Pooling-GeMPool\" class=\"headerlink\" title=\"2 - Generalized Mean Pooling (GeMPool)\"></a>2 - Generalized Mean Pooling (GeMPool)</h2><p>GeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:</p>\n<p>$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(p \\in [1, \\infty)\\) is the trainable scalar parameter.</p>\n<ol>\n<li><p>when \\(p = 1\\), this clearly corresponds to <strong>average pooling</strong>;</p>\n</li>\n<li><p>when \\(p \\to \\infty\\), it corresponds to <strong>max pooling</strong>. A way to prove this is to calculate the following limit:<br>$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + …)^\\frac{1}{p} = x_{max}$$</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"3-Other-Aggregating-Mechanisms\"><a href=\"#3-Other-Aggregating-Mechanisms\" class=\"headerlink\" title=\"3 - Other Aggregating Mechanisms\"></a>3 - Other Aggregating Mechanisms</h2><p>Both methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the <strong>exponent</strong> of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that <strong>the contributing factor of large values increase</strong>, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn’t been conducted before to the best of my knowledge.</p>\n<p>A more sophisticated method of aggregating values is to use <strong>attention</strong>, as a weightage is learnt for each value, known as <strong>attention mask</strong>, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.</p>\n<br/>\n\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>I provide the portals to the original source code / reimplementation of the parameterized pooling methods:</p>\n<ol>\n<li><a href=\"https://github.com/marl/autopool/blob/master/autopool/autopool.py\" target=\"_blank\" rel=\"noopener\">AutoPool official implementation in Keras</a></li>\n<li><a href=\"https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling reimplementation in PyTorch</a></li>\n<li><a href=\"https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01\" target=\"_blank\" rel=\"noopener\">Github Gist on both pooling methods in TF2 Keras</a> </li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Two parameterized pooling layers which aim to improve beyond average and max pooling<br>2 - The techniques introduced are: <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a><br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Pooling layers in deep learning serve the purpose of <strong>aggregating information</strong> - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? </p>\n<p>The very first encounter of most deep learning practitioners with pooling layers should be within the stack of “conv - pool - relu” block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to <strong>downsample</strong> the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on <strong>temporal aggregation</strong> for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\((d,)\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\((M, T)\\), where \\(T\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.</p>\n<p>The most common pooling methods are either <strong>average pooling</strong> or <strong>max pooling</strong>. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion &amp; democracy, whereas max pooling is more like tyranny &amp; eliticism where only the best speaks.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\n<p>The also explains why average pooling produces <strong>smoother, blurrer</strong> feature maps, and max pooling produces <strong>sharper, discontinuous</strong> feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: <strong>is there a way to exploit states between average and max pooling?</strong> Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\(p\\), such that it gives us the best pooled output for our own application? This is the motivation of <strong>parameterized / adaptive pooling</strong> methods.</p>\n<p>Below I will discuss two methods that I recently read up, which is <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a>. Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.</p>\n<br/>\n\n<h2 id=\"2-AutoPool\"><a href=\"#2-AutoPool\" class=\"headerlink\" title=\"2 - AutoPool\"></a>2 - AutoPool</h2><p>AutoPool, proposed by McFee et al, generalizes the pooling equation as below:</p>\n<p>$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(\\alpha \\in [0, \\infty)\\) is the trainable scalar parameter.</p>\n<p>We can easily see that this equation takes the form of a <strong>weighted sum</strong> - each element \\(x\\), contributes to the final aggregated value \\(y\\), with a weight factor determined by function \\(w\\).</p>\n<ol>\n<li><p>when \\(\\alpha = 0\\), it is clear that \\(w(\\alpha, x) = \\frac{1}{|X|}\\) because \\(e^{\\alpha \\cdot x} = 1\\) and the denominator resembles the number of elements in \\(X\\). The corresponds to <strong>average pooling</strong>, and each value has equal contribution.</p>\n</li>\n<li><p>when \\(\\alpha = 1\\), the authors term this as <strong>softmax pooling</strong>, as each value contributes with a factor of its softmax value.</p>\n</li>\n<li><p>when \\(\\alpha \\to \\infty\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} =<br>\\displaystyle\\lim_{\\alpha \\to \\infty}<br>\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + … }$$ Hence, by dividing \\(x_{max}\\) on both numerator and denominator, we can see that only if \\(x = x_{max}\\), then the limit equals to \\(1\\), or else the limit equals to \\(0\\). We can see that this corresponds to <strong>max pooling</strong>.</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"2-Generalized-Mean-Pooling-GeMPool\"><a href=\"#2-Generalized-Mean-Pooling-GeMPool\" class=\"headerlink\" title=\"2 - Generalized Mean Pooling (GeMPool)\"></a>2 - Generalized Mean Pooling (GeMPool)</h2><p>GeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:</p>\n<p>$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(p \\in [1, \\infty)\\) is the trainable scalar parameter.</p>\n<ol>\n<li><p>when \\(p = 1\\), this clearly corresponds to <strong>average pooling</strong>;</p>\n</li>\n<li><p>when \\(p \\to \\infty\\), it corresponds to <strong>max pooling</strong>. A way to prove this is to calculate the following limit:<br>$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + …)^\\frac{1}{p} = x_{max}$$</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"3-Other-Aggregating-Mechanisms\"><a href=\"#3-Other-Aggregating-Mechanisms\" class=\"headerlink\" title=\"3 - Other Aggregating Mechanisms\"></a>3 - Other Aggregating Mechanisms</h2><p>Both methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the <strong>exponent</strong> of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that <strong>the contributing factor of large values increase</strong>, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn’t been conducted before to the best of my knowledge.</p>\n<p>A more sophisticated method of aggregating values is to use <strong>attention</strong>, as a weightage is learnt for each value, known as <strong>attention mask</strong>, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.</p>\n<br/>\n\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>I provide the portals to the original source code / reimplementation of the parameterized pooling methods:</p>\n<ol>\n<li><a href=\"https://github.com/marl/autopool/blob/master/autopool/autopool.py\" target=\"_blank\" rel=\"noopener\">AutoPool official implementation in Keras</a></li>\n<li><a href=\"https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling reimplementation in PyTorch</a></li>\n<li><a href=\"https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01\" target=\"_blank\" rel=\"noopener\">Github Gist on both pooling methods in TF2 Keras</a> </li>\n</ol>\n"},{"title":"Challenges in Productionizing Cover Detection Systems","date":"2021-02-25T11:12:09.000Z","estimatedReadTime":"~12 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief survey on recent cover song detection systems\n2 - Challenges in deploying cover song detection systems to production\n\n\n## 1 - Introduction\n\nRecently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. \n\nThe experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed \"baby\" - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a **very, very, very long way** from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.\n\nI could now understand clearly why the **model is often not the primary concern within the stack**, especially when the team is resource limited. More resources can be directed to R&D afterwards, but a **seamlessly served model**, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n## 2 - A Very Brief Survey on Recent Advances in Cover Song Detection\n\nCover song detection, to the music industry, is the **potential \"upgraded\" version of audio fingerprinting systems**, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to **identify \"music of any version, in any performance / cover, in any form\"**.\n\nTo the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.\n\n### Dominant Melody-Based\n\nThe idea is to match the **dominant melody** of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by [Doras et al. 2019](https://arxiv.org/pdf/1907.01824.pdf) and [Doras et al. 2020](https://arxiv.org/pdf/1910.09862.pdf), which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include [Sailer et al.](https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf) and [Tsai et al.](https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758), which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\nHowever, the performance of dominant melody based solutions is tightly coupled with **the accuracy of the extracted dominant melody** (see [this recent work](https://brianmcfee.net/papers/ismir2017_salience.pdf) on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) \"wobbly\" pitch glides due to singing techniques. For alignment-based methods, since we often need **pitch intervals** to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.\n\n### Harmonic-Based\n\nThe idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by [MOVE](https://arxiv.org/pdf/1910.12551.pdf) and [Re-MOVE](https://arxiv.org/pdf/2010.03284.pdf) which uses [cremaPCP](https://github.com/bmcfee/crema) as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, [Serra et al.](https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf)'s method using [HPCP](https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles) as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\nThe potential problem in harmonic-based methods is that there can be **more false positives** in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in **quadratic time**, which imposes a limit on detection speed and hence harder to scale.\n\n### Hybrid Methods\n\nThe most recent hybrid attempt is by [Yesiler and Doras](https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y) which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include [MFCC and HPCP fusion](https://arxiv.org/pdf/1707.04680.pdf), with improvements using [ensemble-based comparison](https://arxiv.org/pdf/1905.11700.pdf).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\nFrom a system standpoint, hybrid methods **add levels of complexity** when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.\n\nThere is also an important work which introduces the representation of **2D Fourier Transform** (2DFT) for CSD by [Seetharaman et al.](https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf). 2DFT (see [this video](https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen) for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on \"faithful covers\", but failed when the cover has a larger extent of variation. \n\n### End-to-End Based\n\nEnd-to-end based systems are often favoured due to its **simplicity** for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including [CQTNet](https://arxiv.org/pdf/1911.00334.pdf), [TPPNet](https://www.ijcai.org/Proceedings/2019/0673.pdf), and the recent [ByteCover](https://arxiv.org/pdf/2010.14022.pdf) lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a [person re-identification problem](https://paperswithcode.com/task/person-re-identification), and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n## 3 - Thoughts and Discussion regarding CSD in Production\n\nI would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.\n\n### Snippet Detection\n\nBecause CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only **few seconds of recording**. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.\n\nBut, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) **doesn't show resemblance** / **marginally resembles** with the reference (irrelevant sections chosen); or more often (ii) **resembles more with other references** depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don't generalize well to snippet forms of query - alignment based methods are dependent on query & reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by [Zalkow et al.](https://www.mdpi.com/2076-3417/10/1/19/pdf) which works on \"shingles\" in classical music.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\nResearch work also did not focus on **which section (where)** in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW & Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. \n\n### Benchmark Results May Not Transfer To Other Datasets\n\nThe performance of CSD algorithms are highly dependent on **what kind of corpus you are comparing against**. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many \"competitive candidates\" for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. \n\nAnother note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if **transfer learning** (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.\n\n### Metrics Used May Not Reflect Practical Needs\n\nFor a very long time, CSD has been formulated as an **information retrieval** problem - \"given a song, can you retrieve the most similar cover tracks?\" This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like \"given a track (original / cover), can you tell me which work it belongs to?\", which is more relevant to an **identification problem** (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.\n\n### Computer Vision-Based Models Perform Best?\n\nByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see [this re-ID strong baseline paper](https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf)). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed **\"musically-aware\"** network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, [pooling methods](https://gudgud96.github.io/2020/11/25/param-pooling/) etc.)? This would be a question that I would love to seek answer for.\n\n## 4 - Conclusion\n\nCSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.\n\n## 5 - Further References\n1 - Yesiler et al. - [Version Identification in the 20s - ISMIR2020](https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49), ISMIR 2020 Tutorial.\n2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - [link](https://medias.ircam.fr/x9f5132)\n<br/>","source":"_posts/challenge-csd.md","raw":"---\ntitle: Challenges in Productionizing Cover Detection Systems\ndate: 2021-02-25 19:12:09\ntags:\n    - Music Signal Processing\n    - ML in Production\nestimatedReadTime: ~12 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief survey on recent cover song detection systems\n2 - Challenges in deploying cover song detection systems to production\n\n\n## 1 - Introduction\n\nRecently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. \n\nThe experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed \"baby\" - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a **very, very, very long way** from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.\n\nI could now understand clearly why the **model is often not the primary concern within the stack**, especially when the team is resource limited. More resources can be directed to R&D afterwards, but a **seamlessly served model**, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n## 2 - A Very Brief Survey on Recent Advances in Cover Song Detection\n\nCover song detection, to the music industry, is the **potential \"upgraded\" version of audio fingerprinting systems**, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to **identify \"music of any version, in any performance / cover, in any form\"**.\n\nTo the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.\n\n### Dominant Melody-Based\n\nThe idea is to match the **dominant melody** of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by [Doras et al. 2019](https://arxiv.org/pdf/1907.01824.pdf) and [Doras et al. 2020](https://arxiv.org/pdf/1910.09862.pdf), which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include [Sailer et al.](https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf) and [Tsai et al.](https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758), which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\nHowever, the performance of dominant melody based solutions is tightly coupled with **the accuracy of the extracted dominant melody** (see [this recent work](https://brianmcfee.net/papers/ismir2017_salience.pdf) on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) \"wobbly\" pitch glides due to singing techniques. For alignment-based methods, since we often need **pitch intervals** to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.\n\n### Harmonic-Based\n\nThe idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by [MOVE](https://arxiv.org/pdf/1910.12551.pdf) and [Re-MOVE](https://arxiv.org/pdf/2010.03284.pdf) which uses [cremaPCP](https://github.com/bmcfee/crema) as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, [Serra et al.](https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf)'s method using [HPCP](https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles) as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\nThe potential problem in harmonic-based methods is that there can be **more false positives** in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in **quadratic time**, which imposes a limit on detection speed and hence harder to scale.\n\n### Hybrid Methods\n\nThe most recent hybrid attempt is by [Yesiler and Doras](https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y) which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include [MFCC and HPCP fusion](https://arxiv.org/pdf/1707.04680.pdf), with improvements using [ensemble-based comparison](https://arxiv.org/pdf/1905.11700.pdf).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\nFrom a system standpoint, hybrid methods **add levels of complexity** when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.\n\nThere is also an important work which introduces the representation of **2D Fourier Transform** (2DFT) for CSD by [Seetharaman et al.](https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf). 2DFT (see [this video](https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen) for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on \"faithful covers\", but failed when the cover has a larger extent of variation. \n\n### End-to-End Based\n\nEnd-to-end based systems are often favoured due to its **simplicity** for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including [CQTNet](https://arxiv.org/pdf/1911.00334.pdf), [TPPNet](https://www.ijcai.org/Proceedings/2019/0673.pdf), and the recent [ByteCover](https://arxiv.org/pdf/2010.14022.pdf) lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a [person re-identification problem](https://paperswithcode.com/task/person-re-identification), and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n## 3 - Thoughts and Discussion regarding CSD in Production\n\nI would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.\n\n### Snippet Detection\n\nBecause CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only **few seconds of recording**. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.\n\nBut, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) **doesn't show resemblance** / **marginally resembles** with the reference (irrelevant sections chosen); or more often (ii) **resembles more with other references** depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don't generalize well to snippet forms of query - alignment based methods are dependent on query & reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by [Zalkow et al.](https://www.mdpi.com/2076-3417/10/1/19/pdf) which works on \"shingles\" in classical music.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\nResearch work also did not focus on **which section (where)** in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW & Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. \n\n### Benchmark Results May Not Transfer To Other Datasets\n\nThe performance of CSD algorithms are highly dependent on **what kind of corpus you are comparing against**. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many \"competitive candidates\" for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. \n\nAnother note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if **transfer learning** (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.\n\n### Metrics Used May Not Reflect Practical Needs\n\nFor a very long time, CSD has been formulated as an **information retrieval** problem - \"given a song, can you retrieve the most similar cover tracks?\" This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like \"given a track (original / cover), can you tell me which work it belongs to?\", which is more relevant to an **identification problem** (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.\n\n### Computer Vision-Based Models Perform Best?\n\nByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see [this re-ID strong baseline paper](https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf)). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed **\"musically-aware\"** network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, [pooling methods](https://gudgud96.github.io/2020/11/25/param-pooling/) etc.)? This would be a question that I would love to seek answer for.\n\n## 4 - Conclusion\n\nCSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.\n\n## 5 - Further References\n1 - Yesiler et al. - [Version Identification in the 20s - ISMIR2020](https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49), ISMIR 2020 Tutorial.\n2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - [link](https://medias.ircam.fr/x9f5132)\n<br/>","slug":"challenge-csd","published":1,"updated":"2025-06-27T10:08:45.832Z","_id":"cklqf68yn0000y59k0kv6fs2z","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief survey on recent cover song detection systems<br>2 - Challenges in deploying cover song detection systems to production</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. </p>\n<p>The experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed “baby” - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a <strong>very, very, very long way</strong> from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.</p>\n<p>I could now understand clearly why the <strong>model is often not the primary concern within the stack</strong>, especially when the team is resource limited. More resources can be directed to R&amp;D afterwards, but a <strong>seamlessly served model</strong>, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n<h2 id=\"2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\"><a href=\"#2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\" class=\"headerlink\" title=\"2 - A Very Brief Survey on Recent Advances in Cover Song Detection\"></a>2 - A Very Brief Survey on Recent Advances in Cover Song Detection</h2><p>Cover song detection, to the music industry, is the <strong>potential “upgraded” version of audio fingerprinting systems</strong>, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to <strong>identify “music of any version, in any performance / cover, in any form”</strong>.</p>\n<p>To the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.</p>\n<h3 id=\"Dominant-Melody-Based\"><a href=\"#Dominant-Melody-Based\" class=\"headerlink\" title=\"Dominant Melody-Based\"></a>Dominant Melody-Based</h3><p>The idea is to match the <strong>dominant melody</strong> of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2019</a> and <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2020</a>, which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include <a href=\"https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf\" target=\"_blank\" rel=\"noopener\">Sailer et al.</a> and <a href=\"https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758\" target=\"_blank\" rel=\"noopener\">Tsai et al.</a>, which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\n<p>However, the performance of dominant melody based solutions is tightly coupled with <strong>the accuracy of the extracted dominant melody</strong> (see <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">this recent work</a> on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) “wobbly” pitch glides due to singing techniques. For alignment-based methods, since we often need <strong>pitch intervals</strong> to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.</p>\n<h3 id=\"Harmonic-Based\"><a href=\"#Harmonic-Based\" class=\"headerlink\" title=\"Harmonic-Based\"></a>Harmonic-Based</h3><p>The idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">MOVE</a> and <a href=\"https://arxiv.org/pdf/2010.03284.pdf\" target=\"_blank\" rel=\"noopener\">Re-MOVE</a> which uses <a href=\"https://github.com/bmcfee/crema\" target=\"_blank\" rel=\"noopener\">cremaPCP</a> as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, <a href=\"https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf\" target=\"_blank\" rel=\"noopener\">Serra et al.</a>‘s method using <a href=\"https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles\" target=\"_blank\" rel=\"noopener\">HPCP</a> as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\n<p>The potential problem in harmonic-based methods is that there can be <strong>more false positives</strong> in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in <strong>quadratic time</strong>, which imposes a limit on detection speed and hence harder to scale.</p>\n<h3 id=\"Hybrid-Methods\"><a href=\"#Hybrid-Methods\" class=\"headerlink\" title=\"Hybrid Methods\"></a>Hybrid Methods</h3><p>The most recent hybrid attempt is by <a href=\"https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y\" target=\"_blank\" rel=\"noopener\">Yesiler and Doras</a> which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include <a href=\"https://arxiv.org/pdf/1707.04680.pdf\" target=\"_blank\" rel=\"noopener\">MFCC and HPCP fusion</a>, with improvements using <a href=\"https://arxiv.org/pdf/1905.11700.pdf\" target=\"_blank\" rel=\"noopener\">ensemble-based comparison</a>.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\n<p>From a system standpoint, hybrid methods <strong>add levels of complexity</strong> when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.</p>\n<p>There is also an important work which introduces the representation of <strong>2D Fourier Transform</strong> (2DFT) for CSD by <a href=\"https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf\" target=\"_blank\" rel=\"noopener\">Seetharaman et al.</a>. 2DFT (see <a href=\"https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen\" target=\"_blank\" rel=\"noopener\">this video</a> for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on “faithful covers”, but failed when the cover has a larger extent of variation. </p>\n<h3 id=\"End-to-End-Based\"><a href=\"#End-to-End-Based\" class=\"headerlink\" title=\"End-to-End Based\"></a>End-to-End Based</h3><p>End-to-end based systems are often favoured due to its <strong>simplicity</strong> for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including <a href=\"https://arxiv.org/pdf/1911.00334.pdf\" target=\"_blank\" rel=\"noopener\">CQTNet</a>, <a href=\"https://www.ijcai.org/Proceedings/2019/0673.pdf\" target=\"_blank\" rel=\"noopener\">TPPNet</a>, and the recent <a href=\"https://arxiv.org/pdf/2010.14022.pdf\" target=\"_blank\" rel=\"noopener\">ByteCover</a> lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a <a href=\"https://paperswithcode.com/task/person-re-identification\" target=\"_blank\" rel=\"noopener\">person re-identification problem</a>, and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n<h2 id=\"3-Thoughts-and-Discussion-regarding-CSD-in-Production\"><a href=\"#3-Thoughts-and-Discussion-regarding-CSD-in-Production\" class=\"headerlink\" title=\"3 - Thoughts and Discussion regarding CSD in Production\"></a>3 - Thoughts and Discussion regarding CSD in Production</h2><p>I would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.</p>\n<h3 id=\"Snippet-Detection\"><a href=\"#Snippet-Detection\" class=\"headerlink\" title=\"Snippet Detection\"></a>Snippet Detection</h3><p>Because CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only <strong>few seconds of recording</strong>. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.</p>\n<p>But, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) <strong>doesn’t show resemblance</strong> / <strong>marginally resembles</strong> with the reference (irrelevant sections chosen); or more often (ii) <strong>resembles more with other references</strong> depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don’t generalize well to snippet forms of query - alignment based methods are dependent on query &amp; reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by <a href=\"https://www.mdpi.com/2076-3417/10/1/19/pdf\" target=\"_blank\" rel=\"noopener\">Zalkow et al.</a> which works on “shingles” in classical music.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\n<p>Research work also did not focus on <strong>which section (where)</strong> in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW &amp; Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. </p>\n<h3 id=\"Benchmark-Results-May-Not-Transfer-To-Other-Datasets\"><a href=\"#Benchmark-Results-May-Not-Transfer-To-Other-Datasets\" class=\"headerlink\" title=\"Benchmark Results May Not Transfer To Other Datasets\"></a>Benchmark Results May Not Transfer To Other Datasets</h3><p>The performance of CSD algorithms are highly dependent on <strong>what kind of corpus you are comparing against</strong>. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many “competitive candidates” for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. </p>\n<p>Another note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if <strong>transfer learning</strong> (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.</p>\n<h3 id=\"Metrics-Used-May-Not-Reflect-Practical-Needs\"><a href=\"#Metrics-Used-May-Not-Reflect-Practical-Needs\" class=\"headerlink\" title=\"Metrics Used May Not Reflect Practical Needs\"></a>Metrics Used May Not Reflect Practical Needs</h3><p>For a very long time, CSD has been formulated as an <strong>information retrieval</strong> problem - “given a song, can you retrieve the most similar cover tracks?” This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like “given a track (original / cover), can you tell me which work it belongs to?”, which is more relevant to an <strong>identification problem</strong> (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.</p>\n<h3 id=\"Computer-Vision-Based-Models-Perform-Best\"><a href=\"#Computer-Vision-Based-Models-Perform-Best\" class=\"headerlink\" title=\"Computer Vision-Based Models Perform Best?\"></a>Computer Vision-Based Models Perform Best?</h3><p>ByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see <a href=\"https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\" target=\"_blank\" rel=\"noopener\">this re-ID strong baseline paper</a>). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed <strong>“musically-aware”</strong> network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, <a href=\"https://gudgud96.github.io/2020/11/25/param-pooling/\">pooling methods</a> etc.)? This would be a question that I would love to seek answer for.</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4 - Conclusion\"></a>4 - Conclusion</h2><p>CSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.</p>\n<h2 id=\"5-Further-References\"><a href=\"#5-Further-References\" class=\"headerlink\" title=\"5 - Further References\"></a>5 - Further References</h2><p>1 - Yesiler et al. - <a href=\"https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49\" target=\"_blank\" rel=\"noopener\">Version Identification in the 20s - ISMIR2020</a>, ISMIR 2020 Tutorial.<br>2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - <a href=\"https://medias.ircam.fr/x9f5132\" target=\"_blank\" rel=\"noopener\">link</a><br><br/></p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief survey on recent cover song detection systems<br>2 - Challenges in deploying cover song detection systems to production</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. </p>\n<p>The experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed “baby” - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a <strong>very, very, very long way</strong> from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.</p>\n<p>I could now understand clearly why the <strong>model is often not the primary concern within the stack</strong>, especially when the team is resource limited. More resources can be directed to R&amp;D afterwards, but a <strong>seamlessly served model</strong>, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n<h2 id=\"2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\"><a href=\"#2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\" class=\"headerlink\" title=\"2 - A Very Brief Survey on Recent Advances in Cover Song Detection\"></a>2 - A Very Brief Survey on Recent Advances in Cover Song Detection</h2><p>Cover song detection, to the music industry, is the <strong>potential “upgraded” version of audio fingerprinting systems</strong>, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to <strong>identify “music of any version, in any performance / cover, in any form”</strong>.</p>\n<p>To the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.</p>\n<h3 id=\"Dominant-Melody-Based\"><a href=\"#Dominant-Melody-Based\" class=\"headerlink\" title=\"Dominant Melody-Based\"></a>Dominant Melody-Based</h3><p>The idea is to match the <strong>dominant melody</strong> of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2019</a> and <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2020</a>, which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include <a href=\"https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf\" target=\"_blank\" rel=\"noopener\">Sailer et al.</a> and <a href=\"https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758\" target=\"_blank\" rel=\"noopener\">Tsai et al.</a>, which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\n<p>However, the performance of dominant melody based solutions is tightly coupled with <strong>the accuracy of the extracted dominant melody</strong> (see <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">this recent work</a> on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) “wobbly” pitch glides due to singing techniques. For alignment-based methods, since we often need <strong>pitch intervals</strong> to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.</p>\n<h3 id=\"Harmonic-Based\"><a href=\"#Harmonic-Based\" class=\"headerlink\" title=\"Harmonic-Based\"></a>Harmonic-Based</h3><p>The idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">MOVE</a> and <a href=\"https://arxiv.org/pdf/2010.03284.pdf\" target=\"_blank\" rel=\"noopener\">Re-MOVE</a> which uses <a href=\"https://github.com/bmcfee/crema\" target=\"_blank\" rel=\"noopener\">cremaPCP</a> as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, <a href=\"https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf\" target=\"_blank\" rel=\"noopener\">Serra et al.</a>‘s method using <a href=\"https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles\" target=\"_blank\" rel=\"noopener\">HPCP</a> as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\n<p>The potential problem in harmonic-based methods is that there can be <strong>more false positives</strong> in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in <strong>quadratic time</strong>, which imposes a limit on detection speed and hence harder to scale.</p>\n<h3 id=\"Hybrid-Methods\"><a href=\"#Hybrid-Methods\" class=\"headerlink\" title=\"Hybrid Methods\"></a>Hybrid Methods</h3><p>The most recent hybrid attempt is by <a href=\"https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y\" target=\"_blank\" rel=\"noopener\">Yesiler and Doras</a> which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include <a href=\"https://arxiv.org/pdf/1707.04680.pdf\" target=\"_blank\" rel=\"noopener\">MFCC and HPCP fusion</a>, with improvements using <a href=\"https://arxiv.org/pdf/1905.11700.pdf\" target=\"_blank\" rel=\"noopener\">ensemble-based comparison</a>.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\n<p>From a system standpoint, hybrid methods <strong>add levels of complexity</strong> when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.</p>\n<p>There is also an important work which introduces the representation of <strong>2D Fourier Transform</strong> (2DFT) for CSD by <a href=\"https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf\" target=\"_blank\" rel=\"noopener\">Seetharaman et al.</a>. 2DFT (see <a href=\"https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen\" target=\"_blank\" rel=\"noopener\">this video</a> for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on “faithful covers”, but failed when the cover has a larger extent of variation. </p>\n<h3 id=\"End-to-End-Based\"><a href=\"#End-to-End-Based\" class=\"headerlink\" title=\"End-to-End Based\"></a>End-to-End Based</h3><p>End-to-end based systems are often favoured due to its <strong>simplicity</strong> for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including <a href=\"https://arxiv.org/pdf/1911.00334.pdf\" target=\"_blank\" rel=\"noopener\">CQTNet</a>, <a href=\"https://www.ijcai.org/Proceedings/2019/0673.pdf\" target=\"_blank\" rel=\"noopener\">TPPNet</a>, and the recent <a href=\"https://arxiv.org/pdf/2010.14022.pdf\" target=\"_blank\" rel=\"noopener\">ByteCover</a> lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a <a href=\"https://paperswithcode.com/task/person-re-identification\" target=\"_blank\" rel=\"noopener\">person re-identification problem</a>, and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n<h2 id=\"3-Thoughts-and-Discussion-regarding-CSD-in-Production\"><a href=\"#3-Thoughts-and-Discussion-regarding-CSD-in-Production\" class=\"headerlink\" title=\"3 - Thoughts and Discussion regarding CSD in Production\"></a>3 - Thoughts and Discussion regarding CSD in Production</h2><p>I would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.</p>\n<h3 id=\"Snippet-Detection\"><a href=\"#Snippet-Detection\" class=\"headerlink\" title=\"Snippet Detection\"></a>Snippet Detection</h3><p>Because CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only <strong>few seconds of recording</strong>. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.</p>\n<p>But, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) <strong>doesn’t show resemblance</strong> / <strong>marginally resembles</strong> with the reference (irrelevant sections chosen); or more often (ii) <strong>resembles more with other references</strong> depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don’t generalize well to snippet forms of query - alignment based methods are dependent on query &amp; reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by <a href=\"https://www.mdpi.com/2076-3417/10/1/19/pdf\" target=\"_blank\" rel=\"noopener\">Zalkow et al.</a> which works on “shingles” in classical music.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\n<p>Research work also did not focus on <strong>which section (where)</strong> in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW &amp; Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. </p>\n<h3 id=\"Benchmark-Results-May-Not-Transfer-To-Other-Datasets\"><a href=\"#Benchmark-Results-May-Not-Transfer-To-Other-Datasets\" class=\"headerlink\" title=\"Benchmark Results May Not Transfer To Other Datasets\"></a>Benchmark Results May Not Transfer To Other Datasets</h3><p>The performance of CSD algorithms are highly dependent on <strong>what kind of corpus you are comparing against</strong>. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many “competitive candidates” for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. </p>\n<p>Another note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if <strong>transfer learning</strong> (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.</p>\n<h3 id=\"Metrics-Used-May-Not-Reflect-Practical-Needs\"><a href=\"#Metrics-Used-May-Not-Reflect-Practical-Needs\" class=\"headerlink\" title=\"Metrics Used May Not Reflect Practical Needs\"></a>Metrics Used May Not Reflect Practical Needs</h3><p>For a very long time, CSD has been formulated as an <strong>information retrieval</strong> problem - “given a song, can you retrieve the most similar cover tracks?” This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like “given a track (original / cover), can you tell me which work it belongs to?”, which is more relevant to an <strong>identification problem</strong> (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.</p>\n<h3 id=\"Computer-Vision-Based-Models-Perform-Best\"><a href=\"#Computer-Vision-Based-Models-Perform-Best\" class=\"headerlink\" title=\"Computer Vision-Based Models Perform Best?\"></a>Computer Vision-Based Models Perform Best?</h3><p>ByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see <a href=\"https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\" target=\"_blank\" rel=\"noopener\">this re-ID strong baseline paper</a>). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed <strong>“musically-aware”</strong> network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, <a href=\"https://gudgud96.github.io/2020/11/25/param-pooling/\">pooling methods</a> etc.)? This would be a question that I would love to seek answer for.</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4 - Conclusion\"></a>4 - Conclusion</h2><p>CSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.</p>\n<h2 id=\"5-Further-References\"><a href=\"#5-Further-References\" class=\"headerlink\" title=\"5 - Further References\"></a>5 - Further References</h2><p>1 - Yesiler et al. - <a href=\"https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49\" target=\"_blank\" rel=\"noopener\">Version Identification in the 20s - ISMIR2020</a>, ISMIR 2020 Tutorial.<br>2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - <a href=\"https://medias.ircam.fr/x9f5132\" target=\"_blank\" rel=\"noopener\">link</a><br><br/></p>\n"},{"title":"MIR Papers 2021 (and ISMIR)","date":"2022-02-19T15:47:54.000Z","estimatedReadTime":"~20 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 1 - Introduction\n\nIt's been almost a year since my last post, and of course a lot has changed. I am no longer working on cover song detection anymore, nor am I training models to solve MIR problems at the moment. I still do, like [trying something for MediaEval](https://github.com/gudgud96/noisy-student-emotion-training) or [reproducing papers I like](https://github.com/gudgud96/diff-wave-synth), but mostly after work which is, you know, not a lot of time left.\n\nWhich is also why I haven't been writing for long, because writing is slow and given the time available, I rather spend more time coding to maximize output (or procrastinate to minimize it...). But inspired by my recent co-worker, I still find blogging necessary to record and clear your mind, and most importantly, ISMIR 2021 is like N months ago and the idea to write a post about it was always dangling (which I really should have finished this N months ago...)\n\nSo here it comes!\n\n## 2 - Differentiable Digital Signal Processing\n\nI think 2021 is very much a year on DDSP and its applications, and it has also been my recent interest. Just to name a few works that I follow slightly closer:\n\n[**DDSP: Differentiable Digital Signal Processing**](https://arxiv.org/abs/2001.04643)\n\nThe idea is neat - using WaveNet to synthesize audio is costing way too many parameters, so maybe we need to instill some domain knowledge as inductive bias. Perhaps we can make use of **synthesizers** (let's take additive synthesizers as an example). To learn the parameters of a synth, the synth itself should be \"differentiable\" so that gradients can be back-propagated. Hence each DSP component within a synth (F0, loudness, filter design, reverb, etc.) can be implemented as differentiable functions, to allow the learning of parameters via optimizing a (multi-level) spectrogram loss. The parameters needed is simply **10x lesser** to achieve high fidelity synthesis.\n\n[**Differentiable Signal Processing with Black-Box Audio Effects**](https://arxiv.org/abs/2105.04752)\n\nA neat idea with direct application usage. Given that third party audio FX plugins are non-differentiable, one most obvious solution is to bring in gradient approximation solutions (if you don't want to re-implement the differentiable version of the entire thing). This paper uses a stochastic gradient approximation method called [*simultaneous permutation stochastic approximation*](https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation) for the gradient from black box -> deep encoder. Various applications can span, e.g. automatic mixing / mastering / EQ etc.\n\n*Side note*: the authors mention about the training process - it needs to be parallelized to a single-GPU, multi-CPU setup, so that model training runs on GPU but FX plugins run on CPU. This is pretty much the same training setup as in reinforcement learning (GPU for model, CPU for simulation). I am recently reading about [Ray](https://github.com/ray-project/ray), which is precisely designed for this kind of setup, and I think it might be a suitable use case.\n\n[**Automatic Multitrack Mixing with A Differentiable Mixing Console of Neural Audio Effects**](https://arxiv.org/abs/2010.10291)\n\nAlso on auto-mixing, this paper basically trains a (TCN) model to learn the parameters on the mixing console, which also needs the components on the mixing console (EQ, Compressor, Reverb) to be differentiable. Also, a stereo loss function is introduced because spatialization is important for mixing.\n\n[**Synthesizer Soundmatching with Differentiable DSP**](https://archives.ismir.net/ismir2021/paper/000053.pdf)\n\nThis work focuses more on estimating synth parameters (presets). It needs an estimator network (for the params) and a differentiable (additive/subtractive) synth. Two mode of trainings can be supported - either *supervised* with synth parameter labels,  or *unsupervised* by reproducing input waveform.\n\n[**Differentiable Wavetable Synthesis**](https://arxiv.org/abs/2111.10003#:~:text=Differentiable%20Wavetable%20Synthesis%20(DWTS)%20is,end%2Dto%2Dend%20training.)\n\nSimply implementing the similar idea of DDSP on wavetable synthesis. Since I am a Serum user, wavetable synthesis have always been my favourite, so I tried to [reproduce this paper](https://github.com/gudgud96/diff-wave-synth) as well, with my own implementation of wavetable synth in PyTorch.\n\n[**MIDI-DDSP**](https://arxiv.org/abs/2112.09312)\n\nI worked in this direction a short while before (on piano synthesis, [short paper](https://github.com/gudgud96/piano-synthesis) here), and I only managed to model dynamics and articulation, and I have to use WaveGlow for synthesis which doesn't perform very well at that time, not to mention the amount of time it takes to train WaveGlow. This work extends to model *timbre* and *vibrato*, as well as using DDSP for the synthesis layer. Much more extensive and optimized work.\n\n\n## 3 - Sound Synthesis\n\n[**RAVE: A variational autoencoder for fast and high-quality neural audio synthesis**](https://arxiv.org/abs/2111.05011)\n\nA VAE + adversarial fine tuning method for audio synthesis, main advantage is that it can synthesize on CPU + **20x** faster than before. VAE + adversarial fine tuning is a pretty common training technique to improve audio synthesis quality, one paper that I read before is [Latent Constraints](https://arxiv.org/abs/1711.05772). \n\n[**Neural Waveshaping Synthesis**](https://arxiv.org/abs/2107.05050)\n\nOne of my favourite papers in ISMIR. To achieve efficient CPU synthesis, this work proposes *Neural Waveshaping Unit* (and a fast version) to produce complex timbre evolution. Combining with a differentiable noise synthesizer, only 260k model parameters are needed. I probably will write a bit more to delve into this work further in future.\n\n[**Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations**](https://arxiv.org/abs/2111.08462)\n\nA starting work on using implicit neural representations for audio synthesis. The advantage is that INR is itself a compact representation, hence it can learn faster and generally produce quantitatively better audio reconstructions with equal parameter counts (I think there's much more value in this direction and I should probably delve deeper)\n\n## 4 - General Audio Representations\n\n[**Towards Learning Universal Audio Representations**](https://arxiv.org/abs/2111.12124)\n\nA very ambitious work to provide universal audio representation for various downstream tasks. The architecture used is a [*Slow-Fast*](https://arxiv.org/abs/1812.03982) [*Normalizer-Free*](https://arxiv.org/abs/2102.06171) *F0 Net* given observations in previous work, and transfers well among a wide range of audio-reated tasks.\n\n[**MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation**](https://archives.ismir.net/ismir2021/paper/000090.pdf)\n\nConvert symbolic music into note-level token representation, and apply BERT-like pretraining. The model is capable of texture generation, chord analysis, accompaniment refinement.\n\n[**Contrastive Learning of Musical Representations**](https://arxiv.org/abs/2103.09410)\n\n[SimCLR](https://arxiv.org/abs/2002.05709) on raw music audio, hence the augmentations are specific to music (which the author provides a [repo](https://github.com/Spijkervet/torchaudio-augmentations) on it). This self-supervised method can work well on MTAT and MSD tagging.\n\n\n## 5 - On Music Tagging\n\n[**Semi-supervised Music Tagging Transformer**](https://arxiv.org/abs/2111.13457)\n\nProbably one of the hottest paper during ISMIR. The idea is pretty simple - which is to apply semi-supervised learning on music tagging, and it is super reasonable because tagged music data is super scarce. I tried a [similar idea](https://github.com/gudgud96/noisy-student-emotion-training) during the same time for MediaEval, hence working with emotion data, and noisy student training does not give very conclusive results in the end.\n\n## 6 - Some music open source projects\n\nThe most familiar one I know would still be [nnAudio](https://github.com/KinWaiCheuk/nnAudio) by Kin Wai Cheuk, it's a cool PyTorch audio processing library with sweet CQT implementations. I helped a little on Griffin-Lim and VQT, and I probably would try inverse-CQT next.\n\nRecently I am also looking into [pedalboard](https://github.com/spotify/pedalboard) by Spotify, which is quite a cool project on chaining audio effects - it's simple and it just works. I also find [DawDreamer](https://github.com/DBraun/DawDreamer) an interesting one, basically it's a DAW on Python. The backend still uses JUCE but it provides a Python user interface.\n\n\n\n\n\n","source":"_posts/ismir-2021.md","raw":"---\ntitle: MIR Papers 2021 (and ISMIR)\ndate: 2022-02-19 23:47:54\ntags:\n - Music Information Retrieval\nestimatedReadTime: ~20 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 1 - Introduction\n\nIt's been almost a year since my last post, and of course a lot has changed. I am no longer working on cover song detection anymore, nor am I training models to solve MIR problems at the moment. I still do, like [trying something for MediaEval](https://github.com/gudgud96/noisy-student-emotion-training) or [reproducing papers I like](https://github.com/gudgud96/diff-wave-synth), but mostly after work which is, you know, not a lot of time left.\n\nWhich is also why I haven't been writing for long, because writing is slow and given the time available, I rather spend more time coding to maximize output (or procrastinate to minimize it...). But inspired by my recent co-worker, I still find blogging necessary to record and clear your mind, and most importantly, ISMIR 2021 is like N months ago and the idea to write a post about it was always dangling (which I really should have finished this N months ago...)\n\nSo here it comes!\n\n## 2 - Differentiable Digital Signal Processing\n\nI think 2021 is very much a year on DDSP and its applications, and it has also been my recent interest. Just to name a few works that I follow slightly closer:\n\n[**DDSP: Differentiable Digital Signal Processing**](https://arxiv.org/abs/2001.04643)\n\nThe idea is neat - using WaveNet to synthesize audio is costing way too many parameters, so maybe we need to instill some domain knowledge as inductive bias. Perhaps we can make use of **synthesizers** (let's take additive synthesizers as an example). To learn the parameters of a synth, the synth itself should be \"differentiable\" so that gradients can be back-propagated. Hence each DSP component within a synth (F0, loudness, filter design, reverb, etc.) can be implemented as differentiable functions, to allow the learning of parameters via optimizing a (multi-level) spectrogram loss. The parameters needed is simply **10x lesser** to achieve high fidelity synthesis.\n\n[**Differentiable Signal Processing with Black-Box Audio Effects**](https://arxiv.org/abs/2105.04752)\n\nA neat idea with direct application usage. Given that third party audio FX plugins are non-differentiable, one most obvious solution is to bring in gradient approximation solutions (if you don't want to re-implement the differentiable version of the entire thing). This paper uses a stochastic gradient approximation method called [*simultaneous permutation stochastic approximation*](https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation) for the gradient from black box -> deep encoder. Various applications can span, e.g. automatic mixing / mastering / EQ etc.\n\n*Side note*: the authors mention about the training process - it needs to be parallelized to a single-GPU, multi-CPU setup, so that model training runs on GPU but FX plugins run on CPU. This is pretty much the same training setup as in reinforcement learning (GPU for model, CPU for simulation). I am recently reading about [Ray](https://github.com/ray-project/ray), which is precisely designed for this kind of setup, and I think it might be a suitable use case.\n\n[**Automatic Multitrack Mixing with A Differentiable Mixing Console of Neural Audio Effects**](https://arxiv.org/abs/2010.10291)\n\nAlso on auto-mixing, this paper basically trains a (TCN) model to learn the parameters on the mixing console, which also needs the components on the mixing console (EQ, Compressor, Reverb) to be differentiable. Also, a stereo loss function is introduced because spatialization is important for mixing.\n\n[**Synthesizer Soundmatching with Differentiable DSP**](https://archives.ismir.net/ismir2021/paper/000053.pdf)\n\nThis work focuses more on estimating synth parameters (presets). It needs an estimator network (for the params) and a differentiable (additive/subtractive) synth. Two mode of trainings can be supported - either *supervised* with synth parameter labels,  or *unsupervised* by reproducing input waveform.\n\n[**Differentiable Wavetable Synthesis**](https://arxiv.org/abs/2111.10003#:~:text=Differentiable%20Wavetable%20Synthesis%20(DWTS)%20is,end%2Dto%2Dend%20training.)\n\nSimply implementing the similar idea of DDSP on wavetable synthesis. Since I am a Serum user, wavetable synthesis have always been my favourite, so I tried to [reproduce this paper](https://github.com/gudgud96/diff-wave-synth) as well, with my own implementation of wavetable synth in PyTorch.\n\n[**MIDI-DDSP**](https://arxiv.org/abs/2112.09312)\n\nI worked in this direction a short while before (on piano synthesis, [short paper](https://github.com/gudgud96/piano-synthesis) here), and I only managed to model dynamics and articulation, and I have to use WaveGlow for synthesis which doesn't perform very well at that time, not to mention the amount of time it takes to train WaveGlow. This work extends to model *timbre* and *vibrato*, as well as using DDSP for the synthesis layer. Much more extensive and optimized work.\n\n\n## 3 - Sound Synthesis\n\n[**RAVE: A variational autoencoder for fast and high-quality neural audio synthesis**](https://arxiv.org/abs/2111.05011)\n\nA VAE + adversarial fine tuning method for audio synthesis, main advantage is that it can synthesize on CPU + **20x** faster than before. VAE + adversarial fine tuning is a pretty common training technique to improve audio synthesis quality, one paper that I read before is [Latent Constraints](https://arxiv.org/abs/1711.05772). \n\n[**Neural Waveshaping Synthesis**](https://arxiv.org/abs/2107.05050)\n\nOne of my favourite papers in ISMIR. To achieve efficient CPU synthesis, this work proposes *Neural Waveshaping Unit* (and a fast version) to produce complex timbre evolution. Combining with a differentiable noise synthesizer, only 260k model parameters are needed. I probably will write a bit more to delve into this work further in future.\n\n[**Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations**](https://arxiv.org/abs/2111.08462)\n\nA starting work on using implicit neural representations for audio synthesis. The advantage is that INR is itself a compact representation, hence it can learn faster and generally produce quantitatively better audio reconstructions with equal parameter counts (I think there's much more value in this direction and I should probably delve deeper)\n\n## 4 - General Audio Representations\n\n[**Towards Learning Universal Audio Representations**](https://arxiv.org/abs/2111.12124)\n\nA very ambitious work to provide universal audio representation for various downstream tasks. The architecture used is a [*Slow-Fast*](https://arxiv.org/abs/1812.03982) [*Normalizer-Free*](https://arxiv.org/abs/2102.06171) *F0 Net* given observations in previous work, and transfers well among a wide range of audio-reated tasks.\n\n[**MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation**](https://archives.ismir.net/ismir2021/paper/000090.pdf)\n\nConvert symbolic music into note-level token representation, and apply BERT-like pretraining. The model is capable of texture generation, chord analysis, accompaniment refinement.\n\n[**Contrastive Learning of Musical Representations**](https://arxiv.org/abs/2103.09410)\n\n[SimCLR](https://arxiv.org/abs/2002.05709) on raw music audio, hence the augmentations are specific to music (which the author provides a [repo](https://github.com/Spijkervet/torchaudio-augmentations) on it). This self-supervised method can work well on MTAT and MSD tagging.\n\n\n## 5 - On Music Tagging\n\n[**Semi-supervised Music Tagging Transformer**](https://arxiv.org/abs/2111.13457)\n\nProbably one of the hottest paper during ISMIR. The idea is pretty simple - which is to apply semi-supervised learning on music tagging, and it is super reasonable because tagged music data is super scarce. I tried a [similar idea](https://github.com/gudgud96/noisy-student-emotion-training) during the same time for MediaEval, hence working with emotion data, and noisy student training does not give very conclusive results in the end.\n\n## 6 - Some music open source projects\n\nThe most familiar one I know would still be [nnAudio](https://github.com/KinWaiCheuk/nnAudio) by Kin Wai Cheuk, it's a cool PyTorch audio processing library with sweet CQT implementations. I helped a little on Griffin-Lim and VQT, and I probably would try inverse-CQT next.\n\nRecently I am also looking into [pedalboard](https://github.com/spotify/pedalboard) by Spotify, which is quite a cool project on chaining audio effects - it's simple and it just works. I also find [DawDreamer](https://github.com/DBraun/DawDreamer) an interesting one, basically it's a DAW on Python. The backend still uses JUCE but it provides a Python user interface.\n\n\n\n\n\n","slug":"ismir-2021","published":1,"updated":"2025-06-27T10:09:21.405Z","_id":"ckzu0m6n80000qg9khdg3busn","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>It’s been almost a year since my last post, and of course a lot has changed. I am no longer working on cover song detection anymore, nor am I training models to solve MIR problems at the moment. I still do, like <a href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">trying something for MediaEval</a> or <a href=\"https://github.com/gudgud96/diff-wave-synth\" target=\"_blank\" rel=\"noopener\">reproducing papers I like</a>, but mostly after work which is, you know, not a lot of time left.</p>\n<p>Which is also why I haven’t been writing for long, because writing is slow and given the time available, I rather spend more time coding to maximize output (or procrastinate to minimize it…). But inspired by my recent co-worker, I still find blogging necessary to record and clear your mind, and most importantly, ISMIR 2021 is like N months ago and the idea to write a post about it was always dangling (which I really should have finished this N months ago…)</p>\n<p>So here it comes!</p>\n<h2 id=\"2-Differentiable-Digital-Signal-Processing\"><a href=\"#2-Differentiable-Digital-Signal-Processing\" class=\"headerlink\" title=\"2 - Differentiable Digital Signal Processing\"></a>2 - Differentiable Digital Signal Processing</h2><p>I think 2021 is very much a year on DDSP and its applications, and it has also been my recent interest. Just to name a few works that I follow slightly closer:</p>\n<p><a href=\"https://arxiv.org/abs/2001.04643\" target=\"_blank\" rel=\"noopener\"><strong>DDSP: Differentiable Digital Signal Processing</strong></a></p>\n<p>The idea is neat - using WaveNet to synthesize audio is costing way too many parameters, so maybe we need to instill some domain knowledge as inductive bias. Perhaps we can make use of <strong>synthesizers</strong> (let’s take additive synthesizers as an example). To learn the parameters of a synth, the synth itself should be “differentiable” so that gradients can be back-propagated. Hence each DSP component within a synth (F0, loudness, filter design, reverb, etc.) can be implemented as differentiable functions, to allow the learning of parameters via optimizing a (multi-level) spectrogram loss. The parameters needed is simply <strong>10x lesser</strong> to achieve high fidelity synthesis.</p>\n<p><a href=\"https://arxiv.org/abs/2105.04752\" target=\"_blank\" rel=\"noopener\"><strong>Differentiable Signal Processing with Black-Box Audio Effects</strong></a></p>\n<p>A neat idea with direct application usage. Given that third party audio FX plugins are non-differentiable, one most obvious solution is to bring in gradient approximation solutions (if you don’t want to re-implement the differentiable version of the entire thing). This paper uses a stochastic gradient approximation method called <a href=\"https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation\" target=\"_blank\" rel=\"noopener\"><em>simultaneous permutation stochastic approximation</em></a> for the gradient from black box -&gt; deep encoder. Various applications can span, e.g. automatic mixing / mastering / EQ etc.</p>\n<p><em>Side note</em>: the authors mention about the training process - it needs to be parallelized to a single-GPU, multi-CPU setup, so that model training runs on GPU but FX plugins run on CPU. This is pretty much the same training setup as in reinforcement learning (GPU for model, CPU for simulation). I am recently reading about <a href=\"https://github.com/ray-project/ray\" target=\"_blank\" rel=\"noopener\">Ray</a>, which is precisely designed for this kind of setup, and I think it might be a suitable use case.</p>\n<p><a href=\"https://arxiv.org/abs/2010.10291\" target=\"_blank\" rel=\"noopener\"><strong>Automatic Multitrack Mixing with A Differentiable Mixing Console of Neural Audio Effects</strong></a></p>\n<p>Also on auto-mixing, this paper basically trains a (TCN) model to learn the parameters on the mixing console, which also needs the components on the mixing console (EQ, Compressor, Reverb) to be differentiable. Also, a stereo loss function is introduced because spatialization is important for mixing.</p>\n<p><a href=\"https://archives.ismir.net/ismir2021/paper/000053.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Synthesizer Soundmatching with Differentiable DSP</strong></a></p>\n<p>This work focuses more on estimating synth parameters (presets). It needs an estimator network (for the params) and a differentiable (additive/subtractive) synth. Two mode of trainings can be supported - either <em>supervised</em> with synth parameter labels,  or <em>unsupervised</em> by reproducing input waveform.</p>\n<p><a href=\"https://arxiv.org/abs/2111.10003#:~:text=Differentiable%20Wavetable%20Synthesis%20(DWTS)%20is,end%2Dto%2Dend%20training.\" target=\"_blank\" rel=\"noopener\"><strong>Differentiable Wavetable Synthesis</strong></a></p>\n<p>Simply implementing the similar idea of DDSP on wavetable synthesis. Since I am a Serum user, wavetable synthesis have always been my favourite, so I tried to <a href=\"https://github.com/gudgud96/diff-wave-synth\" target=\"_blank\" rel=\"noopener\">reproduce this paper</a> as well, with my own implementation of wavetable synth in PyTorch.</p>\n<p><a href=\"https://arxiv.org/abs/2112.09312\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-DDSP</strong></a></p>\n<p>I worked in this direction a short while before (on piano synthesis, <a href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">short paper</a> here), and I only managed to model dynamics and articulation, and I have to use WaveGlow for synthesis which doesn’t perform very well at that time, not to mention the amount of time it takes to train WaveGlow. This work extends to model <em>timbre</em> and <em>vibrato</em>, as well as using DDSP for the synthesis layer. Much more extensive and optimized work.</p>\n<h2 id=\"3-Sound-Synthesis\"><a href=\"#3-Sound-Synthesis\" class=\"headerlink\" title=\"3 - Sound Synthesis\"></a>3 - Sound Synthesis</h2><p><a href=\"https://arxiv.org/abs/2111.05011\" target=\"_blank\" rel=\"noopener\"><strong>RAVE: A variational autoencoder for fast and high-quality neural audio synthesis</strong></a></p>\n<p>A VAE + adversarial fine tuning method for audio synthesis, main advantage is that it can synthesize on CPU + <strong>20x</strong> faster than before. VAE + adversarial fine tuning is a pretty common training technique to improve audio synthesis quality, one paper that I read before is <a href=\"https://arxiv.org/abs/1711.05772\" target=\"_blank\" rel=\"noopener\">Latent Constraints</a>. </p>\n<p><a href=\"https://arxiv.org/abs/2107.05050\" target=\"_blank\" rel=\"noopener\"><strong>Neural Waveshaping Synthesis</strong></a></p>\n<p>One of my favourite papers in ISMIR. To achieve efficient CPU synthesis, this work proposes <em>Neural Waveshaping Unit</em> (and a fast version) to produce complex timbre evolution. Combining with a differentiable noise synthesizer, only 260k model parameters are needed. I probably will write a bit more to delve into this work further in future.</p>\n<p><a href=\"https://arxiv.org/abs/2111.08462\" target=\"_blank\" rel=\"noopener\"><strong>Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations</strong></a></p>\n<p>A starting work on using implicit neural representations for audio synthesis. The advantage is that INR is itself a compact representation, hence it can learn faster and generally produce quantitatively better audio reconstructions with equal parameter counts (I think there’s much more value in this direction and I should probably delve deeper)</p>\n<h2 id=\"4-General-Audio-Representations\"><a href=\"#4-General-Audio-Representations\" class=\"headerlink\" title=\"4 - General Audio Representations\"></a>4 - General Audio Representations</h2><p><a href=\"https://arxiv.org/abs/2111.12124\" target=\"_blank\" rel=\"noopener\"><strong>Towards Learning Universal Audio Representations</strong></a></p>\n<p>A very ambitious work to provide universal audio representation for various downstream tasks. The architecture used is a <a href=\"https://arxiv.org/abs/1812.03982\" target=\"_blank\" rel=\"noopener\"><em>Slow-Fast</em></a> <a href=\"https://arxiv.org/abs/2102.06171\" target=\"_blank\" rel=\"noopener\"><em>Normalizer-Free</em></a> <em>F0 Net</em> given observations in previous work, and transfers well among a wide range of audio-reated tasks.</p>\n<p><a href=\"https://archives.ismir.net/ismir2021/paper/000090.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation</strong></a></p>\n<p>Convert symbolic music into note-level token representation, and apply BERT-like pretraining. The model is capable of texture generation, chord analysis, accompaniment refinement.</p>\n<p><a href=\"https://arxiv.org/abs/2103.09410\" target=\"_blank\" rel=\"noopener\"><strong>Contrastive Learning of Musical Representations</strong></a></p>\n<p><a href=\"https://arxiv.org/abs/2002.05709\" target=\"_blank\" rel=\"noopener\">SimCLR</a> on raw music audio, hence the augmentations are specific to music (which the author provides a <a href=\"https://github.com/Spijkervet/torchaudio-augmentations\" target=\"_blank\" rel=\"noopener\">repo</a> on it). This self-supervised method can work well on MTAT and MSD tagging.</p>\n<h2 id=\"5-On-Music-Tagging\"><a href=\"#5-On-Music-Tagging\" class=\"headerlink\" title=\"5 - On Music Tagging\"></a>5 - On Music Tagging</h2><p><a href=\"https://arxiv.org/abs/2111.13457\" target=\"_blank\" rel=\"noopener\"><strong>Semi-supervised Music Tagging Transformer</strong></a></p>\n<p>Probably one of the hottest paper during ISMIR. The idea is pretty simple - which is to apply semi-supervised learning on music tagging, and it is super reasonable because tagged music data is super scarce. I tried a <a href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">similar idea</a> during the same time for MediaEval, hence working with emotion data, and noisy student training does not give very conclusive results in the end.</p>\n<h2 id=\"6-Some-music-open-source-projects\"><a href=\"#6-Some-music-open-source-projects\" class=\"headerlink\" title=\"6 - Some music open source projects\"></a>6 - Some music open source projects</h2><p>The most familiar one I know would still be <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> by Kin Wai Cheuk, it’s a cool PyTorch audio processing library with sweet CQT implementations. I helped a little on Griffin-Lim and VQT, and I probably would try inverse-CQT next.</p>\n<p>Recently I am also looking into <a href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noopener\">pedalboard</a> by Spotify, which is quite a cool project on chaining audio effects - it’s simple and it just works. I also find <a href=\"https://github.com/DBraun/DawDreamer\" target=\"_blank\" rel=\"noopener\">DawDreamer</a> an interesting one, basically it’s a DAW on Python. The backend still uses JUCE but it provides a Python user interface.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>It’s been almost a year since my last post, and of course a lot has changed. I am no longer working on cover song detection anymore, nor am I training models to solve MIR problems at the moment. I still do, like <a href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">trying something for MediaEval</a> or <a href=\"https://github.com/gudgud96/diff-wave-synth\" target=\"_blank\" rel=\"noopener\">reproducing papers I like</a>, but mostly after work which is, you know, not a lot of time left.</p>\n<p>Which is also why I haven’t been writing for long, because writing is slow and given the time available, I rather spend more time coding to maximize output (or procrastinate to minimize it…). But inspired by my recent co-worker, I still find blogging necessary to record and clear your mind, and most importantly, ISMIR 2021 is like N months ago and the idea to write a post about it was always dangling (which I really should have finished this N months ago…)</p>\n<p>So here it comes!</p>\n<h2 id=\"2-Differentiable-Digital-Signal-Processing\"><a href=\"#2-Differentiable-Digital-Signal-Processing\" class=\"headerlink\" title=\"2 - Differentiable Digital Signal Processing\"></a>2 - Differentiable Digital Signal Processing</h2><p>I think 2021 is very much a year on DDSP and its applications, and it has also been my recent interest. Just to name a few works that I follow slightly closer:</p>\n<p><a href=\"https://arxiv.org/abs/2001.04643\" target=\"_blank\" rel=\"noopener\"><strong>DDSP: Differentiable Digital Signal Processing</strong></a></p>\n<p>The idea is neat - using WaveNet to synthesize audio is costing way too many parameters, so maybe we need to instill some domain knowledge as inductive bias. Perhaps we can make use of <strong>synthesizers</strong> (let’s take additive synthesizers as an example). To learn the parameters of a synth, the synth itself should be “differentiable” so that gradients can be back-propagated. Hence each DSP component within a synth (F0, loudness, filter design, reverb, etc.) can be implemented as differentiable functions, to allow the learning of parameters via optimizing a (multi-level) spectrogram loss. The parameters needed is simply <strong>10x lesser</strong> to achieve high fidelity synthesis.</p>\n<p><a href=\"https://arxiv.org/abs/2105.04752\" target=\"_blank\" rel=\"noopener\"><strong>Differentiable Signal Processing with Black-Box Audio Effects</strong></a></p>\n<p>A neat idea with direct application usage. Given that third party audio FX plugins are non-differentiable, one most obvious solution is to bring in gradient approximation solutions (if you don’t want to re-implement the differentiable version of the entire thing). This paper uses a stochastic gradient approximation method called <a href=\"https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation\" target=\"_blank\" rel=\"noopener\"><em>simultaneous permutation stochastic approximation</em></a> for the gradient from black box -&gt; deep encoder. Various applications can span, e.g. automatic mixing / mastering / EQ etc.</p>\n<p><em>Side note</em>: the authors mention about the training process - it needs to be parallelized to a single-GPU, multi-CPU setup, so that model training runs on GPU but FX plugins run on CPU. This is pretty much the same training setup as in reinforcement learning (GPU for model, CPU for simulation). I am recently reading about <a href=\"https://github.com/ray-project/ray\" target=\"_blank\" rel=\"noopener\">Ray</a>, which is precisely designed for this kind of setup, and I think it might be a suitable use case.</p>\n<p><a href=\"https://arxiv.org/abs/2010.10291\" target=\"_blank\" rel=\"noopener\"><strong>Automatic Multitrack Mixing with A Differentiable Mixing Console of Neural Audio Effects</strong></a></p>\n<p>Also on auto-mixing, this paper basically trains a (TCN) model to learn the parameters on the mixing console, which also needs the components on the mixing console (EQ, Compressor, Reverb) to be differentiable. Also, a stereo loss function is introduced because spatialization is important for mixing.</p>\n<p><a href=\"https://archives.ismir.net/ismir2021/paper/000053.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Synthesizer Soundmatching with Differentiable DSP</strong></a></p>\n<p>This work focuses more on estimating synth parameters (presets). It needs an estimator network (for the params) and a differentiable (additive/subtractive) synth. Two mode of trainings can be supported - either <em>supervised</em> with synth parameter labels,  or <em>unsupervised</em> by reproducing input waveform.</p>\n<p><a href=\"https://arxiv.org/abs/2111.10003#:~:text=Differentiable%20Wavetable%20Synthesis%20(DWTS)%20is,end%2Dto%2Dend%20training.\" target=\"_blank\" rel=\"noopener\"><strong>Differentiable Wavetable Synthesis</strong></a></p>\n<p>Simply implementing the similar idea of DDSP on wavetable synthesis. Since I am a Serum user, wavetable synthesis have always been my favourite, so I tried to <a href=\"https://github.com/gudgud96/diff-wave-synth\" target=\"_blank\" rel=\"noopener\">reproduce this paper</a> as well, with my own implementation of wavetable synth in PyTorch.</p>\n<p><a href=\"https://arxiv.org/abs/2112.09312\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-DDSP</strong></a></p>\n<p>I worked in this direction a short while before (on piano synthesis, <a href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">short paper</a> here), and I only managed to model dynamics and articulation, and I have to use WaveGlow for synthesis which doesn’t perform very well at that time, not to mention the amount of time it takes to train WaveGlow. This work extends to model <em>timbre</em> and <em>vibrato</em>, as well as using DDSP for the synthesis layer. Much more extensive and optimized work.</p>\n<h2 id=\"3-Sound-Synthesis\"><a href=\"#3-Sound-Synthesis\" class=\"headerlink\" title=\"3 - Sound Synthesis\"></a>3 - Sound Synthesis</h2><p><a href=\"https://arxiv.org/abs/2111.05011\" target=\"_blank\" rel=\"noopener\"><strong>RAVE: A variational autoencoder for fast and high-quality neural audio synthesis</strong></a></p>\n<p>A VAE + adversarial fine tuning method for audio synthesis, main advantage is that it can synthesize on CPU + <strong>20x</strong> faster than before. VAE + adversarial fine tuning is a pretty common training technique to improve audio synthesis quality, one paper that I read before is <a href=\"https://arxiv.org/abs/1711.05772\" target=\"_blank\" rel=\"noopener\">Latent Constraints</a>. </p>\n<p><a href=\"https://arxiv.org/abs/2107.05050\" target=\"_blank\" rel=\"noopener\"><strong>Neural Waveshaping Synthesis</strong></a></p>\n<p>One of my favourite papers in ISMIR. To achieve efficient CPU synthesis, this work proposes <em>Neural Waveshaping Unit</em> (and a fast version) to produce complex timbre evolution. Combining with a differentiable noise synthesizer, only 260k model parameters are needed. I probably will write a bit more to delve into this work further in future.</p>\n<p><a href=\"https://arxiv.org/abs/2111.08462\" target=\"_blank\" rel=\"noopener\"><strong>Towards Lightweight Controllable Audio Synthesis with Conditional Implicit Neural Representations</strong></a></p>\n<p>A starting work on using implicit neural representations for audio synthesis. The advantage is that INR is itself a compact representation, hence it can learn faster and generally produce quantitatively better audio reconstructions with equal parameter counts (I think there’s much more value in this direction and I should probably delve deeper)</p>\n<h2 id=\"4-General-Audio-Representations\"><a href=\"#4-General-Audio-Representations\" class=\"headerlink\" title=\"4 - General Audio Representations\"></a>4 - General Audio Representations</h2><p><a href=\"https://arxiv.org/abs/2111.12124\" target=\"_blank\" rel=\"noopener\"><strong>Towards Learning Universal Audio Representations</strong></a></p>\n<p>A very ambitious work to provide universal audio representation for various downstream tasks. The architecture used is a <a href=\"https://arxiv.org/abs/1812.03982\" target=\"_blank\" rel=\"noopener\"><em>Slow-Fast</em></a> <a href=\"https://arxiv.org/abs/2102.06171\" target=\"_blank\" rel=\"noopener\"><em>Normalizer-Free</em></a> <em>F0 Net</em> given observations in previous work, and transfers well among a wide range of audio-reated tasks.</p>\n<p><a href=\"https://archives.ismir.net/ismir2021/paper/000090.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation</strong></a></p>\n<p>Convert symbolic music into note-level token representation, and apply BERT-like pretraining. The model is capable of texture generation, chord analysis, accompaniment refinement.</p>\n<p><a href=\"https://arxiv.org/abs/2103.09410\" target=\"_blank\" rel=\"noopener\"><strong>Contrastive Learning of Musical Representations</strong></a></p>\n<p><a href=\"https://arxiv.org/abs/2002.05709\" target=\"_blank\" rel=\"noopener\">SimCLR</a> on raw music audio, hence the augmentations are specific to music (which the author provides a <a href=\"https://github.com/Spijkervet/torchaudio-augmentations\" target=\"_blank\" rel=\"noopener\">repo</a> on it). This self-supervised method can work well on MTAT and MSD tagging.</p>\n<h2 id=\"5-On-Music-Tagging\"><a href=\"#5-On-Music-Tagging\" class=\"headerlink\" title=\"5 - On Music Tagging\"></a>5 - On Music Tagging</h2><p><a href=\"https://arxiv.org/abs/2111.13457\" target=\"_blank\" rel=\"noopener\"><strong>Semi-supervised Music Tagging Transformer</strong></a></p>\n<p>Probably one of the hottest paper during ISMIR. The idea is pretty simple - which is to apply semi-supervised learning on music tagging, and it is super reasonable because tagged music data is super scarce. I tried a <a href=\"https://github.com/gudgud96/noisy-student-emotion-training\" target=\"_blank\" rel=\"noopener\">similar idea</a> during the same time for MediaEval, hence working with emotion data, and noisy student training does not give very conclusive results in the end.</p>\n<h2 id=\"6-Some-music-open-source-projects\"><a href=\"#6-Some-music-open-source-projects\" class=\"headerlink\" title=\"6 - Some music open source projects\"></a>6 - Some music open source projects</h2><p>The most familiar one I know would still be <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> by Kin Wai Cheuk, it’s a cool PyTorch audio processing library with sweet CQT implementations. I helped a little on Griffin-Lim and VQT, and I probably would try inverse-CQT next.</p>\n<p>Recently I am also looking into <a href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noopener\">pedalboard</a> by Spotify, which is quite a cool project on chaining audio effects - it’s simple and it just works. I also find <a href=\"https://github.com/DBraun/DawDreamer\" target=\"_blank\" rel=\"noopener\">DawDreamer</a> an interesting one, basically it’s a DAW on Python. The backend still uses JUCE but it provides a Python user interface.</p>\n"},{"title":"On The Generative AI Boom","date":"2023-04-24T08:52:50.000Z","estimatedReadTime":"~12 minutes","_content":"\nIn 2018, I wrote [my first blog post](/2018/09/24/ai-music-direction/) when I first worked on AI music generation. It was a snapshot of what I personally think to be the right positoning of AI music generation at the time. \n\nFive years passed, and I am glad that generative AI, as a whole, is having its blossoming moment today. At this juncture, I would like to write a few points regarding generative AI and LLMs, as a snapshot of my current opinions.\n\n## AI and Job Market\n\n**1** - The right way I think going forward with AI is definitely the **\"co-pilot\"** mode, complementing human at doing \"baseline-level work\". However creative and cool one's work is, there are still portions of work that are mundane and time-consuming - polishing edges in PS for art designers, track labelling for music producers, writing emails for programmers, etc. That's where AI come to the rescue.\n\n**2** - Of course, what defines \"baseline-level work\" changes from time to time - data-entry + processing work is cool during IBM mainframe days; today [ChatGPT can even plot graphs and do basic data analysis](https://beebom.com/how-draw-graphs-charts-diagrams-chatgpt/). That means humans are, and should be, constantly upskilling themselves. The challenge is then to keep the speed of upskilling at the same rate as AI adoption; if not, even slowing AI adoption for the sake of upskilling. I personally am more optimistic that, there will first be a major understanding and a re-position of job skills to adapt to the AI wave, and then the widespread of AI adoption in various sectors happens, instead of the opposite.\n\n**3** - My optimism is based on two points: firstly, I don't think AI will be adopted that quickly to make jobs vanish. Secondly, I believe that there will be enough jobs created, or obsolete skills transferred, during the process of AI adoption. I personally agree with the [lump of labor fallacy](https://en.wikipedia.org/wiki/Lump_of_labour_fallacy). Demands are ever-growing, in fact, new technology gives birth to new demands (\"hey, now with this, we can do that!\"). That explains why [unemployment rate does not rise when previous major tech advancement happens](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence). Generative AI already gives birth to a middle layer of \"AI engineers\" (now in the form of \"prompt engineers\") - not that hard-core to know the ML math, but more skillful to help those who only know basic usage of generative AI products. \n\n**4** - There is a worry of AI automating jobs that even require creative and critical thinking, like copywriters, telemarketers, even interior designers. I argue two points. Firstly, even for humans, there are too many reasons for not hiring a more technically competent person - most likely because we can foresee, or already have, a better working relationship with an alternative candidate, albeit less competent technically. Secondly, humans are better at doing \"out-of-distribution\" stuffs - sometimes that's called \"uniqueness\" or \"style\". Facing the AI wave, it becomes more important to find out our \"out-of-distribution\" areas, areas which we can provide the most unique value. This is not something that we should do only when we are facing the AI wave - we should already been doing it the whole time in an ever-competitive job market.\n\n**5** - To elaborate on why AI won't be adopted that quickly: at first, there is [inertia for any kind of technological impact](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence), especially in this case, given our worry of the correctness and reliability of LLMs. Of course, LLMs will get better, but I think the **insecurity** of relying fully on an LLM forms a stronger inertia to slow the widespread of AI adoption.\n\n**6** - What AI can't further replace is the **\"context\"** of our jobs. \"Context\" is an intangible concept, but it covers every subtle detail in our job scope outside of the domain hard knowledge - relationships you own, habits you form while collaborating, ways to cut corners to ship things faster, etc. There are just way too many things other than domain knowledge for one to perform well in their job, just like a good software engineer never just know how to code well. These \"contexts\" are the things at risk if one proposes to replace a human operator with AI. The knowledge of \"context\" can hardly be transferred and quantified, and this is why I think a total replacement of human by AI for a job role might actually be counterproductive.\n\n**7** - All these points reiterate the \"co-pilot\" mode: **we don't replace existing jobs with AI**. We might reduce vacancies due to AI, as we liberate more manpower which uses AI tools. But as demands surge, the liberated manpower will be re-invested into exploring new interests, attending to new demands, and starting new business directions. These will then create more job oppurtunities.\n\n## About the AI tech\n\n**8** - I think why LLM products are powerful is because they solve \"the last mile problem\". As compared to search engines, it understands your intent, sieves through massive amount of data, tailors the response to your use case, and somehow shows the magic of \"human touch\". But I don't think it replaces search engines. A lot of times I don't need that personal touch, and I enjoy the speedy response with an abundant of sources from the search engine. Hence, I think the true value of LLMs lies in specialized areas (imagine a legal contract drafter, or a healthcare counsellor), instead of being a general-purpose chatbot. But at the same time, I feel the \"last mile problem\" is much harder to solve in these scenarios.\n\n**9** - I am not optimistic about prompt engineering. \"Searching for prompts that works best for a generative model\" is model-specific, or even product-specific. Specific prompts will go obsolete (like \"Let's think step-by-step\"), and I believe models will improve to serve prompts that are more and more general. Also, I think prompts are primitive, and not the best UI/UX. Who wants to type a whole paragraph of essay to perform a task, and if it goes wrong, type in longer paragraphs? There will be better solutions, and I personally admire what [DragGAN](https://github.com/XingangPan/DragGAN) has done - spot-on demonstration of the power of controllable generation, without prompts.\n\n**10** - I don't think AGI / strong AI is coming in the near future - it is more like a [stochastic parrot](https://en.wikipedia.org/wiki/Stochastic_parrot). I don't see any theoretical grounds in current machine learning field that can make programs \"think that they exist\" (some \"cogito ergo sum\" resemblance here). Even not to mention the inefficiency of machine \"learning\" - given 1.4T parameters and the world wide web, GPT-4 is still far from what a 86-billion-neuron human brain can do. The risks implicated by the misuse of AI far outweigh the risk of CPUs conquering the world.\n\n**11** - It seems like AI can already do a lot, but there are a few areas that I think will be extremely interesting. First off, I still think deep learning on audio / music is behind its counterparts on image and language. Engineering challenges of on-device ML inference, which guarantees better data privacy and mobility, will be a big problem worth solving. Decades-old research problems on how to leverage massive amount of unlabelled data, and explaning learnt representations of deep learning models, remains valuable questions to answer.\n\n## AI and Content Creation\n\n**12** - IP in generative AI is an interesting subject. In the musical context, covers and remixes have already brought a great deal of storm to the concept of IP infringement around 10 - 20 years ago, especially when UGC platforms popped out. It will be even harder moving forward, as the line between \"original\" and \"derivative\" will be blurred even further. \n\n**13** - Personally, I think the trend will be to first impose a strict ban on any kind of generation that directly threatens identity. In that case, stuffs like voice cloning, deepfake videos will have very limited room to grow, albeit for entertainment purposes.\n\n**14** - Then there comes big-data facilitated, AI-generated content. Personally, I believe the pro-AI side will win eventually, because analogous to streaming models, AI-generated / assisted content will be a major part of the content market, given the ease of use of the AI tools. I also don't see any reason to oppose, for example, [Secret Invasion using AI-generated clips in its opening credits](https://ew.com/tv/secret-invasion-marvel-ai-generated-intro-controversy/#:~:text=In%20an%20interview%20with%20Polygon,morphing%20into%20green%20alien%20Skrulls.), I find it an interesting artistic choice, and very well suited with the topic of deception in the series. I also think platforms like [Runway](https://runwayml.com/) enhance creativity and explore possibilities, instead of stifling it.\n\n**15** - The key concern will then be how to use data ethically, in a way that every stakeholder is less unhappy. I believe regions with opposing stance on using data for AI training will soon find themselves losing competitive advantage down the road. Although the journey will be all bumpy and grudgy, filled with ugly lawsuits and gruesome bloodbath, I think it will lead to adoption instead of opposition eventually. (With shallow knowledge, I think the [WGA Hollywood strike](https://en.wikipedia.org/wiki/2023_Writers_Guild_of_America_strike) is more of a debate on fair compensation than a debate on AI. If it is an a debate of AI, I would refer to my arguments in point 4).\n\n## Conclusion\n\n**16** - To close, I want to put in [Francois Chollet's great quote](https://twitter.com/fchollet/status/1640126042433290240) which came just in time when the generative AI boom starts: \"Fall in love with the problem, not the method. Trend-chasing is counter-productive\". AI is a shiny method with almost magical potentials, but fundamentals shall prevail, that we should first build products that people need, and solve problems that we are interested in.","source":"_posts/generative-ai-direction.md","raw":"---\ntitle: On The Generative AI Boom\ndate: 2023-04-24 16:52:50\ntags:\n    - General Thoughts\nestimatedReadTime: ~12 minutes\n---\n\nIn 2018, I wrote [my first blog post](/2018/09/24/ai-music-direction/) when I first worked on AI music generation. It was a snapshot of what I personally think to be the right positoning of AI music generation at the time. \n\nFive years passed, and I am glad that generative AI, as a whole, is having its blossoming moment today. At this juncture, I would like to write a few points regarding generative AI and LLMs, as a snapshot of my current opinions.\n\n## AI and Job Market\n\n**1** - The right way I think going forward with AI is definitely the **\"co-pilot\"** mode, complementing human at doing \"baseline-level work\". However creative and cool one's work is, there are still portions of work that are mundane and time-consuming - polishing edges in PS for art designers, track labelling for music producers, writing emails for programmers, etc. That's where AI come to the rescue.\n\n**2** - Of course, what defines \"baseline-level work\" changes from time to time - data-entry + processing work is cool during IBM mainframe days; today [ChatGPT can even plot graphs and do basic data analysis](https://beebom.com/how-draw-graphs-charts-diagrams-chatgpt/). That means humans are, and should be, constantly upskilling themselves. The challenge is then to keep the speed of upskilling at the same rate as AI adoption; if not, even slowing AI adoption for the sake of upskilling. I personally am more optimistic that, there will first be a major understanding and a re-position of job skills to adapt to the AI wave, and then the widespread of AI adoption in various sectors happens, instead of the opposite.\n\n**3** - My optimism is based on two points: firstly, I don't think AI will be adopted that quickly to make jobs vanish. Secondly, I believe that there will be enough jobs created, or obsolete skills transferred, during the process of AI adoption. I personally agree with the [lump of labor fallacy](https://en.wikipedia.org/wiki/Lump_of_labour_fallacy). Demands are ever-growing, in fact, new technology gives birth to new demands (\"hey, now with this, we can do that!\"). That explains why [unemployment rate does not rise when previous major tech advancement happens](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence). Generative AI already gives birth to a middle layer of \"AI engineers\" (now in the form of \"prompt engineers\") - not that hard-core to know the ML math, but more skillful to help those who only know basic usage of generative AI products. \n\n**4** - There is a worry of AI automating jobs that even require creative and critical thinking, like copywriters, telemarketers, even interior designers. I argue two points. Firstly, even for humans, there are too many reasons for not hiring a more technically competent person - most likely because we can foresee, or already have, a better working relationship with an alternative candidate, albeit less competent technically. Secondly, humans are better at doing \"out-of-distribution\" stuffs - sometimes that's called \"uniqueness\" or \"style\". Facing the AI wave, it becomes more important to find out our \"out-of-distribution\" areas, areas which we can provide the most unique value. This is not something that we should do only when we are facing the AI wave - we should already been doing it the whole time in an ever-competitive job market.\n\n**5** - To elaborate on why AI won't be adopted that quickly: at first, there is [inertia for any kind of technological impact](https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence), especially in this case, given our worry of the correctness and reliability of LLMs. Of course, LLMs will get better, but I think the **insecurity** of relying fully on an LLM forms a stronger inertia to slow the widespread of AI adoption.\n\n**6** - What AI can't further replace is the **\"context\"** of our jobs. \"Context\" is an intangible concept, but it covers every subtle detail in our job scope outside of the domain hard knowledge - relationships you own, habits you form while collaborating, ways to cut corners to ship things faster, etc. There are just way too many things other than domain knowledge for one to perform well in their job, just like a good software engineer never just know how to code well. These \"contexts\" are the things at risk if one proposes to replace a human operator with AI. The knowledge of \"context\" can hardly be transferred and quantified, and this is why I think a total replacement of human by AI for a job role might actually be counterproductive.\n\n**7** - All these points reiterate the \"co-pilot\" mode: **we don't replace existing jobs with AI**. We might reduce vacancies due to AI, as we liberate more manpower which uses AI tools. But as demands surge, the liberated manpower will be re-invested into exploring new interests, attending to new demands, and starting new business directions. These will then create more job oppurtunities.\n\n## About the AI tech\n\n**8** - I think why LLM products are powerful is because they solve \"the last mile problem\". As compared to search engines, it understands your intent, sieves through massive amount of data, tailors the response to your use case, and somehow shows the magic of \"human touch\". But I don't think it replaces search engines. A lot of times I don't need that personal touch, and I enjoy the speedy response with an abundant of sources from the search engine. Hence, I think the true value of LLMs lies in specialized areas (imagine a legal contract drafter, or a healthcare counsellor), instead of being a general-purpose chatbot. But at the same time, I feel the \"last mile problem\" is much harder to solve in these scenarios.\n\n**9** - I am not optimistic about prompt engineering. \"Searching for prompts that works best for a generative model\" is model-specific, or even product-specific. Specific prompts will go obsolete (like \"Let's think step-by-step\"), and I believe models will improve to serve prompts that are more and more general. Also, I think prompts are primitive, and not the best UI/UX. Who wants to type a whole paragraph of essay to perform a task, and if it goes wrong, type in longer paragraphs? There will be better solutions, and I personally admire what [DragGAN](https://github.com/XingangPan/DragGAN) has done - spot-on demonstration of the power of controllable generation, without prompts.\n\n**10** - I don't think AGI / strong AI is coming in the near future - it is more like a [stochastic parrot](https://en.wikipedia.org/wiki/Stochastic_parrot). I don't see any theoretical grounds in current machine learning field that can make programs \"think that they exist\" (some \"cogito ergo sum\" resemblance here). Even not to mention the inefficiency of machine \"learning\" - given 1.4T parameters and the world wide web, GPT-4 is still far from what a 86-billion-neuron human brain can do. The risks implicated by the misuse of AI far outweigh the risk of CPUs conquering the world.\n\n**11** - It seems like AI can already do a lot, but there are a few areas that I think will be extremely interesting. First off, I still think deep learning on audio / music is behind its counterparts on image and language. Engineering challenges of on-device ML inference, which guarantees better data privacy and mobility, will be a big problem worth solving. Decades-old research problems on how to leverage massive amount of unlabelled data, and explaning learnt representations of deep learning models, remains valuable questions to answer.\n\n## AI and Content Creation\n\n**12** - IP in generative AI is an interesting subject. In the musical context, covers and remixes have already brought a great deal of storm to the concept of IP infringement around 10 - 20 years ago, especially when UGC platforms popped out. It will be even harder moving forward, as the line between \"original\" and \"derivative\" will be blurred even further. \n\n**13** - Personally, I think the trend will be to first impose a strict ban on any kind of generation that directly threatens identity. In that case, stuffs like voice cloning, deepfake videos will have very limited room to grow, albeit for entertainment purposes.\n\n**14** - Then there comes big-data facilitated, AI-generated content. Personally, I believe the pro-AI side will win eventually, because analogous to streaming models, AI-generated / assisted content will be a major part of the content market, given the ease of use of the AI tools. I also don't see any reason to oppose, for example, [Secret Invasion using AI-generated clips in its opening credits](https://ew.com/tv/secret-invasion-marvel-ai-generated-intro-controversy/#:~:text=In%20an%20interview%20with%20Polygon,morphing%20into%20green%20alien%20Skrulls.), I find it an interesting artistic choice, and very well suited with the topic of deception in the series. I also think platforms like [Runway](https://runwayml.com/) enhance creativity and explore possibilities, instead of stifling it.\n\n**15** - The key concern will then be how to use data ethically, in a way that every stakeholder is less unhappy. I believe regions with opposing stance on using data for AI training will soon find themselves losing competitive advantage down the road. Although the journey will be all bumpy and grudgy, filled with ugly lawsuits and gruesome bloodbath, I think it will lead to adoption instead of opposition eventually. (With shallow knowledge, I think the [WGA Hollywood strike](https://en.wikipedia.org/wiki/2023_Writers_Guild_of_America_strike) is more of a debate on fair compensation than a debate on AI. If it is an a debate of AI, I would refer to my arguments in point 4).\n\n## Conclusion\n\n**16** - To close, I want to put in [Francois Chollet's great quote](https://twitter.com/fchollet/status/1640126042433290240) which came just in time when the generative AI boom starts: \"Fall in love with the problem, not the method. Trend-chasing is counter-productive\". AI is a shiny method with almost magical potentials, but fundamentals shall prevail, that we should first build products that people need, and solve problems that we are interested in.","slug":"generative-ai-direction","published":1,"updated":"2025-06-27T10:09:07.215Z","_id":"clgs5b6k30000qyc91zdc0k6b","comments":1,"layout":"post","photos":[],"link":"","content":"<p>In 2018, I wrote <a href=\"/2018/09/24/ai-music-direction/\">my first blog post</a> when I first worked on AI music generation. It was a snapshot of what I personally think to be the right positoning of AI music generation at the time. </p>\n<p>Five years passed, and I am glad that generative AI, as a whole, is having its blossoming moment today. At this juncture, I would like to write a few points regarding generative AI and LLMs, as a snapshot of my current opinions.</p>\n<h2 id=\"AI-and-Job-Market\"><a href=\"#AI-and-Job-Market\" class=\"headerlink\" title=\"AI and Job Market\"></a>AI and Job Market</h2><p><strong>1</strong> - The right way I think going forward with AI is definitely the <strong>“co-pilot”</strong> mode, complementing human at doing “baseline-level work”. However creative and cool one’s work is, there are still portions of work that are mundane and time-consuming - polishing edges in PS for art designers, track labelling for music producers, writing emails for programmers, etc. That’s where AI come to the rescue.</p>\n<p><strong>2</strong> - Of course, what defines “baseline-level work” changes from time to time - data-entry + processing work is cool during IBM mainframe days; today <a href=\"https://beebom.com/how-draw-graphs-charts-diagrams-chatgpt/\" target=\"_blank\" rel=\"noopener\">ChatGPT can even plot graphs and do basic data analysis</a>. That means humans are, and should be, constantly upskilling themselves. The challenge is then to keep the speed of upskilling at the same rate as AI adoption; if not, even slowing AI adoption for the sake of upskilling. I personally am more optimistic that, there will first be a major understanding and a re-position of job skills to adapt to the AI wave, and then the widespread of AI adoption in various sectors happens, instead of the opposite.</p>\n<p><strong>3</strong> - My optimism is based on two points: firstly, I don’t think AI will be adopted that quickly to make jobs vanish. Secondly, I believe that there will be enough jobs created, or obsolete skills transferred, during the process of AI adoption. I personally agree with the <a href=\"https://en.wikipedia.org/wiki/Lump_of_labour_fallacy\" target=\"_blank\" rel=\"noopener\">lump of labor fallacy</a>. Demands are ever-growing, in fact, new technology gives birth to new demands (“hey, now with this, we can do that!”). That explains why <a href=\"https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence\" target=\"_blank\" rel=\"noopener\">unemployment rate does not rise when previous major tech advancement happens</a>. Generative AI already gives birth to a middle layer of “AI engineers” (now in the form of “prompt engineers”) - not that hard-core to know the ML math, but more skillful to help those who only know basic usage of generative AI products. </p>\n<p><strong>4</strong> - There is a worry of AI automating jobs that even require creative and critical thinking, like copywriters, telemarketers, even interior designers. I argue two points. Firstly, even for humans, there are too many reasons for not hiring a more technically competent person - most likely because we can foresee, or already have, a better working relationship with an alternative candidate, albeit less competent technically. Secondly, humans are better at doing “out-of-distribution” stuffs - sometimes that’s called “uniqueness” or “style”. Facing the AI wave, it becomes more important to find out our “out-of-distribution” areas, areas which we can provide the most unique value. This is not something that we should do only when we are facing the AI wave - we should already been doing it the whole time in an ever-competitive job market.</p>\n<p><strong>5</strong> - To elaborate on why AI won’t be adopted that quickly: at first, there is <a href=\"https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence\" target=\"_blank\" rel=\"noopener\">inertia for any kind of technological impact</a>, especially in this case, given our worry of the correctness and reliability of LLMs. Of course, LLMs will get better, but I think the <strong>insecurity</strong> of relying fully on an LLM forms a stronger inertia to slow the widespread of AI adoption.</p>\n<p><strong>6</strong> - What AI can’t further replace is the <strong>“context”</strong> of our jobs. “Context” is an intangible concept, but it covers every subtle detail in our job scope outside of the domain hard knowledge - relationships you own, habits you form while collaborating, ways to cut corners to ship things faster, etc. There are just way too many things other than domain knowledge for one to perform well in their job, just like a good software engineer never just know how to code well. These “contexts” are the things at risk if one proposes to replace a human operator with AI. The knowledge of “context” can hardly be transferred and quantified, and this is why I think a total replacement of human by AI for a job role might actually be counterproductive.</p>\n<p><strong>7</strong> - All these points reiterate the “co-pilot” mode: <strong>we don’t replace existing jobs with AI</strong>. We might reduce vacancies due to AI, as we liberate more manpower which uses AI tools. But as demands surge, the liberated manpower will be re-invested into exploring new interests, attending to new demands, and starting new business directions. These will then create more job oppurtunities.</p>\n<h2 id=\"About-the-AI-tech\"><a href=\"#About-the-AI-tech\" class=\"headerlink\" title=\"About the AI tech\"></a>About the AI tech</h2><p><strong>8</strong> - I think why LLM products are powerful is because they solve “the last mile problem”. As compared to search engines, it understands your intent, sieves through massive amount of data, tailors the response to your use case, and somehow shows the magic of “human touch”. But I don’t think it replaces search engines. A lot of times I don’t need that personal touch, and I enjoy the speedy response with an abundant of sources from the search engine. Hence, I think the true value of LLMs lies in specialized areas (imagine a legal contract drafter, or a healthcare counsellor), instead of being a general-purpose chatbot. But at the same time, I feel the “last mile problem” is much harder to solve in these scenarios.</p>\n<p><strong>9</strong> - I am not optimistic about prompt engineering. “Searching for prompts that works best for a generative model” is model-specific, or even product-specific. Specific prompts will go obsolete (like “Let’s think step-by-step”), and I believe models will improve to serve prompts that are more and more general. Also, I think prompts are primitive, and not the best UI/UX. Who wants to type a whole paragraph of essay to perform a task, and if it goes wrong, type in longer paragraphs? There will be better solutions, and I personally admire what <a href=\"https://github.com/XingangPan/DragGAN\" target=\"_blank\" rel=\"noopener\">DragGAN</a> has done - spot-on demonstration of the power of controllable generation, without prompts.</p>\n<p><strong>10</strong> - I don’t think AGI / strong AI is coming in the near future - it is more like a <a href=\"https://en.wikipedia.org/wiki/Stochastic_parrot\" target=\"_blank\" rel=\"noopener\">stochastic parrot</a>. I don’t see any theoretical grounds in current machine learning field that can make programs “think that they exist” (some “cogito ergo sum” resemblance here). Even not to mention the inefficiency of machine “learning” - given 1.4T parameters and the world wide web, GPT-4 is still far from what a 86-billion-neuron human brain can do. The risks implicated by the misuse of AI far outweigh the risk of CPUs conquering the world.</p>\n<p><strong>11</strong> - It seems like AI can already do a lot, but there are a few areas that I think will be extremely interesting. First off, I still think deep learning on audio / music is behind its counterparts on image and language. Engineering challenges of on-device ML inference, which guarantees better data privacy and mobility, will be a big problem worth solving. Decades-old research problems on how to leverage massive amount of unlabelled data, and explaning learnt representations of deep learning models, remains valuable questions to answer.</p>\n<h2 id=\"AI-and-Content-Creation\"><a href=\"#AI-and-Content-Creation\" class=\"headerlink\" title=\"AI and Content Creation\"></a>AI and Content Creation</h2><p><strong>12</strong> - IP in generative AI is an interesting subject. In the musical context, covers and remixes have already brought a great deal of storm to the concept of IP infringement around 10 - 20 years ago, especially when UGC platforms popped out. It will be even harder moving forward, as the line between “original” and “derivative” will be blurred even further. </p>\n<p><strong>13</strong> - Personally, I think the trend will be to first impose a strict ban on any kind of generation that directly threatens identity. In that case, stuffs like voice cloning, deepfake videos will have very limited room to grow, albeit for entertainment purposes.</p>\n<p><strong>14</strong> - Then there comes big-data facilitated, AI-generated content. Personally, I believe the pro-AI side will win eventually, because analogous to streaming models, AI-generated / assisted content will be a major part of the content market, given the ease of use of the AI tools. I also don’t see any reason to oppose, for example, <a href=\"https://ew.com/tv/secret-invasion-marvel-ai-generated-intro-controversy/#:~:text=In%20an%20interview%20with%20Polygon,morphing%20into%20green%20alien%20Skrulls.\" target=\"_blank\" rel=\"noopener\">Secret Invasion using AI-generated clips in its opening credits</a>, I find it an interesting artistic choice, and very well suited with the topic of deception in the series. I also think platforms like <a href=\"https://runwayml.com/\" target=\"_blank\" rel=\"noopener\">Runway</a> enhance creativity and explore possibilities, instead of stifling it.</p>\n<p><strong>15</strong> - The key concern will then be how to use data ethically, in a way that every stakeholder is less unhappy. I believe regions with opposing stance on using data for AI training will soon find themselves losing competitive advantage down the road. Although the journey will be all bumpy and grudgy, filled with ugly lawsuits and gruesome bloodbath, I think it will lead to adoption instead of opposition eventually. (With shallow knowledge, I think the <a href=\"https://en.wikipedia.org/wiki/2023_Writers_Guild_of_America_strike\" target=\"_blank\" rel=\"noopener\">WGA Hollywood strike</a> is more of a debate on fair compensation than a debate on AI. If it is an a debate of AI, I would refer to my arguments in point 4).</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p><strong>16</strong> - To close, I want to put in <a href=\"https://twitter.com/fchollet/status/1640126042433290240\" target=\"_blank\" rel=\"noopener\">Francois Chollet’s great quote</a> which came just in time when the generative AI boom starts: “Fall in love with the problem, not the method. Trend-chasing is counter-productive”. AI is a shiny method with almost magical potentials, but fundamentals shall prevail, that we should first build products that people need, and solve problems that we are interested in.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>In 2018, I wrote <a href=\"/2018/09/24/ai-music-direction/\">my first blog post</a> when I first worked on AI music generation. It was a snapshot of what I personally think to be the right positoning of AI music generation at the time. </p>\n<p>Five years passed, and I am glad that generative AI, as a whole, is having its blossoming moment today. At this juncture, I would like to write a few points regarding generative AI and LLMs, as a snapshot of my current opinions.</p>\n<h2 id=\"AI-and-Job-Market\"><a href=\"#AI-and-Job-Market\" class=\"headerlink\" title=\"AI and Job Market\"></a>AI and Job Market</h2><p><strong>1</strong> - The right way I think going forward with AI is definitely the <strong>“co-pilot”</strong> mode, complementing human at doing “baseline-level work”. However creative and cool one’s work is, there are still portions of work that are mundane and time-consuming - polishing edges in PS for art designers, track labelling for music producers, writing emails for programmers, etc. That’s where AI come to the rescue.</p>\n<p><strong>2</strong> - Of course, what defines “baseline-level work” changes from time to time - data-entry + processing work is cool during IBM mainframe days; today <a href=\"https://beebom.com/how-draw-graphs-charts-diagrams-chatgpt/\" target=\"_blank\" rel=\"noopener\">ChatGPT can even plot graphs and do basic data analysis</a>. That means humans are, and should be, constantly upskilling themselves. The challenge is then to keep the speed of upskilling at the same rate as AI adoption; if not, even slowing AI adoption for the sake of upskilling. I personally am more optimistic that, there will first be a major understanding and a re-position of job skills to adapt to the AI wave, and then the widespread of AI adoption in various sectors happens, instead of the opposite.</p>\n<p><strong>3</strong> - My optimism is based on two points: firstly, I don’t think AI will be adopted that quickly to make jobs vanish. Secondly, I believe that there will be enough jobs created, or obsolete skills transferred, during the process of AI adoption. I personally agree with the <a href=\"https://en.wikipedia.org/wiki/Lump_of_labour_fallacy\" target=\"_blank\" rel=\"noopener\">lump of labor fallacy</a>. Demands are ever-growing, in fact, new technology gives birth to new demands (“hey, now with this, we can do that!”). That explains why <a href=\"https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence\" target=\"_blank\" rel=\"noopener\">unemployment rate does not rise when previous major tech advancement happens</a>. Generative AI already gives birth to a middle layer of “AI engineers” (now in the form of “prompt engineers”) - not that hard-core to know the ML math, but more skillful to help those who only know basic usage of generative AI products. </p>\n<p><strong>4</strong> - There is a worry of AI automating jobs that even require creative and critical thinking, like copywriters, telemarketers, even interior designers. I argue two points. Firstly, even for humans, there are too many reasons for not hiring a more technically competent person - most likely because we can foresee, or already have, a better working relationship with an alternative candidate, albeit less competent technically. Secondly, humans are better at doing “out-of-distribution” stuffs - sometimes that’s called “uniqueness” or “style”. Facing the AI wave, it becomes more important to find out our “out-of-distribution” areas, areas which we can provide the most unique value. This is not something that we should do only when we are facing the AI wave - we should already been doing it the whole time in an ever-competitive job market.</p>\n<p><strong>5</strong> - To elaborate on why AI won’t be adopted that quickly: at first, there is <a href=\"https://www.economist.com/finance-and-economics/2023/05/07/your-job-is-probably-safe-from-artificial-intelligence\" target=\"_blank\" rel=\"noopener\">inertia for any kind of technological impact</a>, especially in this case, given our worry of the correctness and reliability of LLMs. Of course, LLMs will get better, but I think the <strong>insecurity</strong> of relying fully on an LLM forms a stronger inertia to slow the widespread of AI adoption.</p>\n<p><strong>6</strong> - What AI can’t further replace is the <strong>“context”</strong> of our jobs. “Context” is an intangible concept, but it covers every subtle detail in our job scope outside of the domain hard knowledge - relationships you own, habits you form while collaborating, ways to cut corners to ship things faster, etc. There are just way too many things other than domain knowledge for one to perform well in their job, just like a good software engineer never just know how to code well. These “contexts” are the things at risk if one proposes to replace a human operator with AI. The knowledge of “context” can hardly be transferred and quantified, and this is why I think a total replacement of human by AI for a job role might actually be counterproductive.</p>\n<p><strong>7</strong> - All these points reiterate the “co-pilot” mode: <strong>we don’t replace existing jobs with AI</strong>. We might reduce vacancies due to AI, as we liberate more manpower which uses AI tools. But as demands surge, the liberated manpower will be re-invested into exploring new interests, attending to new demands, and starting new business directions. These will then create more job oppurtunities.</p>\n<h2 id=\"About-the-AI-tech\"><a href=\"#About-the-AI-tech\" class=\"headerlink\" title=\"About the AI tech\"></a>About the AI tech</h2><p><strong>8</strong> - I think why LLM products are powerful is because they solve “the last mile problem”. As compared to search engines, it understands your intent, sieves through massive amount of data, tailors the response to your use case, and somehow shows the magic of “human touch”. But I don’t think it replaces search engines. A lot of times I don’t need that personal touch, and I enjoy the speedy response with an abundant of sources from the search engine. Hence, I think the true value of LLMs lies in specialized areas (imagine a legal contract drafter, or a healthcare counsellor), instead of being a general-purpose chatbot. But at the same time, I feel the “last mile problem” is much harder to solve in these scenarios.</p>\n<p><strong>9</strong> - I am not optimistic about prompt engineering. “Searching for prompts that works best for a generative model” is model-specific, or even product-specific. Specific prompts will go obsolete (like “Let’s think step-by-step”), and I believe models will improve to serve prompts that are more and more general. Also, I think prompts are primitive, and not the best UI/UX. Who wants to type a whole paragraph of essay to perform a task, and if it goes wrong, type in longer paragraphs? There will be better solutions, and I personally admire what <a href=\"https://github.com/XingangPan/DragGAN\" target=\"_blank\" rel=\"noopener\">DragGAN</a> has done - spot-on demonstration of the power of controllable generation, without prompts.</p>\n<p><strong>10</strong> - I don’t think AGI / strong AI is coming in the near future - it is more like a <a href=\"https://en.wikipedia.org/wiki/Stochastic_parrot\" target=\"_blank\" rel=\"noopener\">stochastic parrot</a>. I don’t see any theoretical grounds in current machine learning field that can make programs “think that they exist” (some “cogito ergo sum” resemblance here). Even not to mention the inefficiency of machine “learning” - given 1.4T parameters and the world wide web, GPT-4 is still far from what a 86-billion-neuron human brain can do. The risks implicated by the misuse of AI far outweigh the risk of CPUs conquering the world.</p>\n<p><strong>11</strong> - It seems like AI can already do a lot, but there are a few areas that I think will be extremely interesting. First off, I still think deep learning on audio / music is behind its counterparts on image and language. Engineering challenges of on-device ML inference, which guarantees better data privacy and mobility, will be a big problem worth solving. Decades-old research problems on how to leverage massive amount of unlabelled data, and explaning learnt representations of deep learning models, remains valuable questions to answer.</p>\n<h2 id=\"AI-and-Content-Creation\"><a href=\"#AI-and-Content-Creation\" class=\"headerlink\" title=\"AI and Content Creation\"></a>AI and Content Creation</h2><p><strong>12</strong> - IP in generative AI is an interesting subject. In the musical context, covers and remixes have already brought a great deal of storm to the concept of IP infringement around 10 - 20 years ago, especially when UGC platforms popped out. It will be even harder moving forward, as the line between “original” and “derivative” will be blurred even further. </p>\n<p><strong>13</strong> - Personally, I think the trend will be to first impose a strict ban on any kind of generation that directly threatens identity. In that case, stuffs like voice cloning, deepfake videos will have very limited room to grow, albeit for entertainment purposes.</p>\n<p><strong>14</strong> - Then there comes big-data facilitated, AI-generated content. Personally, I believe the pro-AI side will win eventually, because analogous to streaming models, AI-generated / assisted content will be a major part of the content market, given the ease of use of the AI tools. I also don’t see any reason to oppose, for example, <a href=\"https://ew.com/tv/secret-invasion-marvel-ai-generated-intro-controversy/#:~:text=In%20an%20interview%20with%20Polygon,morphing%20into%20green%20alien%20Skrulls.\" target=\"_blank\" rel=\"noopener\">Secret Invasion using AI-generated clips in its opening credits</a>, I find it an interesting artistic choice, and very well suited with the topic of deception in the series. I also think platforms like <a href=\"https://runwayml.com/\" target=\"_blank\" rel=\"noopener\">Runway</a> enhance creativity and explore possibilities, instead of stifling it.</p>\n<p><strong>15</strong> - The key concern will then be how to use data ethically, in a way that every stakeholder is less unhappy. I believe regions with opposing stance on using data for AI training will soon find themselves losing competitive advantage down the road. Although the journey will be all bumpy and grudgy, filled with ugly lawsuits and gruesome bloodbath, I think it will lead to adoption instead of opposition eventually. (With shallow knowledge, I think the <a href=\"https://en.wikipedia.org/wiki/2023_Writers_Guild_of_America_strike\" target=\"_blank\" rel=\"noopener\">WGA Hollywood strike</a> is more of a debate on fair compensation than a debate on AI. If it is an a debate of AI, I would refer to my arguments in point 4).</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p><strong>16</strong> - To close, I want to put in <a href=\"https://twitter.com/fchollet/status/1640126042433290240\" target=\"_blank\" rel=\"noopener\">Francois Chollet’s great quote</a> which came just in time when the generative AI boom starts: “Fall in love with the problem, not the method. Trend-chasing is counter-productive”. AI is a shiny method with almost magical potentials, but fundamentals shall prevail, that we should first build products that people need, and solve problems that we are interested in.</p>\n"},{"title":"Understanding RVC - Retrieval-based Voice Conversion","date":"2024-09-26T14:53:51.000Z","estimatedReadTime":"~15 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Technical concepts in the RVC project\n2 - Individual modules, such as VITS, RMVPE, HuBERT\n3 - The `top-k` retrieval module, and how does it improve generation quality\n\n<div class=\"post-table-of-contents\">\n    <h3>Table of Contents</h3>\n    <a href=\"#\">1 - Introduction</a><br/>\n    <a href=\"#\">2 - Architecture</a><br/>\n    <a href=\"#\">3 - Deep Dive</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.1 - Content Feature Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.2 - Pitch Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.3 - Acoustic Model (VITS)</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.4 - Retrieval Module</a><br/>\n    <a href=\"#\">4 - Inference</a><br/>\n    <a href=\"#\">5 - Related Work</a><br/>\n</div>\n\n## 1 - Introduction\n\n**AI cover songs** have taken the internet by storm alongside the recent generative AI boom ignited by ChatGPT. If you still haven't had any experience on this piece of heavenly magic, have a listen to [Fake Drake](https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html), [AI Frank Sinatra](https://www.youtube.com/watch?v=HWsb7zTKplc&ab_channel=AiCovers), or [Stephanie Sun](https://www.youtube.com/watch?v=uPMXn7IbdXw&ab_channel=%E5%8D%8E%E8%AF%ADAI%E7%BF%BB%E5%94%B1) (who later posted a [gloomy response](https://www.straitstimes.com/life/entertainment/how-do-i-fight-with-that-stefanie-sun-issues-gloomy-response-to-popularity-of-ai-stefanie-sun) on the matter). To be honest, it's weird to feel exciting and scary at the same time after listening to them. \n\nIn this blog post, I would like to provide a breakdown on (arguably) one of the most popular voice conversion project, which is the [**RVC project**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI). I personally think that RVC is one of the reasons why AI covers have gained huge momentum, given that RVC provides [a rather permissive license](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE) to use its source code and pretrained models, and an easy-to-use, beginner-friendly Web UI for fine-tuning. \n\nI have come across many non-technical tutorials / videos about how to fine-tune RVC, but have yet to read an in-depth breakdown on the technical side of things, hence the motivation of writing this blog post. I will re-draw some of the diagrams based on my understanding, and (try to) justify the important steps in the RVC model. \n\nAnother thing to note is that, RVC shares a lot of common concepts with its \"predecessor\", the [**So-VITS project**](https://github.com/svc-develop-team/so-vits-svc/tree/4.1-Stable), so I hope that this post provides enough details to help readers understand both projects. My crude understanding is that the main difference between So-VITS and RVC is the `top-k` retrieval module, so although the choices of some parts of the modules might be different, but the overall framework should stay similar.\n\n## 2 - Architecture \n\nVoice conversion is essentially a disentanglement task which aims to separate the content and the speaker information. Generally, a voice conversion model consists of a **content encoder** that extracts speaker-invariant content information (such as phonemes, text, intonation), and a **acoustic model** that reconstructs the target voice based on the given content. \n\nWe can break-down RVC into the following modules:\n- A **content feature extractor** to extract information such as phonemes, intonation, etc. from the source audio. Here, RVC chooses [**HuBERT**](), or more precisely, a variant of [**ContentVec**]() - this choice is similar to the early versions of So-VITS. \n- A **pitch extractor** to get the coarse-level and fine-level F0 estimation. Pitch is an important part of the content information, especially in the context of singing voice. Here, RVC chooses [**RMVPE**]().\n- A **conditional acoustic model** to generate the target audio based on given conditions (i.e. speaker ID & content information). Here, RVC chooses [**VITS**](https://arxiv.org/pdf/2106.06103) as its generation framework, with some noticeble influence by [HiFi-GAN](https://arxiv.org/abs/2010.05646) on the [vocoder](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan) - this choice is also largely inherited from So-VITS.\n- A **retrieval module** newly introduced by RVC. Here, RVC stores all content features of the same speaker into a vector index, which can be used later for similarity search during inference. With this, since the content features are from the training set instead of being extracted solely from the source audio, it could introduce more information about the target speaker, further helping the reconstruction to sound more like the target speaker.\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-train.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: The architecture of RVC for training.</figcaption>\n</figure>\n<br/>\n\n## 3.1 - Content Feature Extraction\n\n**HuBERT** is one of the most popular self-supervised speech representation, commonly used in speech and audio related tasks. First, audio chunks are converted into a series of tokens via an offline clustering method (e.g. a K-Means clustering on the MFCCs over a large dataset). Then, some tokens within a sequence are masked, and a Transformer is trained to predict the masked tokens, i.e. the **masked language modelling** method (there is a reason why BERT appears in the name). The features learnt seems to help reduce word-error rate in speech recognition tasks, showing its superiority in encoding content-related information. The clustering / masked-language-modelling parts are sometimes called the *teacher* / *student* modules.\n\nBoth RVC and So-VITS uses the same [hubert-base](https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt) model, but despite its name, I don't think it's a purely pretrained HuBERT model, because from the [**ContentVec**](https://proceedings.mlr.press/v162/qian22b/qian22b.pdf) paper, it claims that HuBERT features could still achieve fairly good results on speaker identification, contradicting with the aim to disentangle speaker information. From the clues provided in So-VITS, I am more convinced that this \"HuBERT\" model has a higher resemblance with **ContentVec** (check out their [video](https://www.youtube.com/watch?v=aiGp1g-dCY4&ab_channel=YangZhang)), which requires more steps to reduce the source speaker information. ContentVec is basically HuBERT, with a few speaker-invariant tweaks:\n- During offline-clustering, before converting the audio into HuBERT features, the audio is randomly converted into other speaker's voices;\n- Add speaker-related transformation (e.g. formant transform) as augmentation when training the speech representation network, and impose a contrastive loss to enforce invariance in timbre changes;\n- During masked label prediction, feed speaker info as condition to the student network to remove any need for further encoding speaker info. \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/contentvec.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: ContentVec training, diagram from the <a href=\"https://arxiv.org/pdf/2106.06103\">original paper.</a></figcaption>\n</figure>\n<br/>\n\n## 3.2 - Pitch Extraction\n\nPitch (or more precisely, fundamental frequency) extraction is crucial for singing voice transfer. A popular pitch extractor used previously is [crepe](https://github.com/marl/crepe), which is a convolutional neural network trained in a supervised manner. RVC chooses a recent work called [RMVPE](https://arxiv.org/pdf/2306.15412), which is based on a U-Net like architecture. The key improvement is that RMVPE performs well even when the input audio is not a clean vocal track, so users can extract vocal pitch directly for polyphonic music, removing the need of running through a source separation model (probably the reason why the authors choose a U-Net). They also observe accuracy improvements over other pitch extraction models. \n\n## 3.3 - Acoustic Model - VITS\n\nThe gist of RVC and So-VITS is their acoustic model, which is based on [VITS](https://arxiv.org/pdf/2106.06103). VITS is essentially a **conditional VAE**, augmented with **normalizing flows** and an **adversarial training** process. You can observe these 3 parts under the VITS module in Figure 1.\n\nFirst, let's frame the voice conversion problem as a *conditional generation* problem. Let \\\\(p(y | c)\\\\) denote the likelihood function that represents the voice conversion process, where \\\\(y\\\\) is the output audio and \\\\(c\\\\) is the content information condition (i.e. the HuBERT features and extracted pitch). We can approximate this intractable likelihood function by maximizing the following ELBO function, following the conditional VAE formulation:\n\n$$E_{z\\sim q(z|y)}[\\log p(y|z, c)] - \\mathcal{D}_{KL}(q(z|y) || p(z|c))$$\n\nFrom here we need 3 neural networks to parameterize the posterior encoder \\\\(q(z|y)\\\\), the prior encoder \\\\(p(z|c)\\\\), and a decoder \\\\(p(y|z, c)\\\\). The first term in the ELBO is the reconstruction loss, and the second term is the KL loss between the posterior and prior distribution. This pretty much sums up the conditional VAE formulation.\n\nTo further improve on this, the authors find that increasing the expressiveness of the prior distribution is important for generating realistic samples. So, a **normalizing (invertible) flow** \\\\(f_\\theta\\\\) is added to allow a transformation on the prior into a more complex distribution. Since the flow is invertible, during training the posterior is passed into the flow and compute the KL loss with the prior - this additional transform will help to bridge the \"gap\" between the posterior and the prior. Later during inference, the prior is passed into the inverse flow \\\\(f^{-1}_{\\theta}\\\\) to map it to a more complex distribution for generation. This highly resembles the technique proposed in the [variational inference with normalizing flows](https://arxiv.org/pdf/1505.05770) paper.\n\nLastly, as seen in most of the literature on high-fidelity singing voice synthesis or audio synthesis, a **[HiFi-GAN]()-style adversarial training** is used on the decoder to improve generation quality. You can observe this technique used in e.g. [DiffSinger](), [RAVE](), [Encodec](), [Stable Audio](), etc. The adversarial training introduces a (multi-period) discriminator that tells if the generation is ground truth or generated output. A few losses are introduced:\n\n- For the discriminator, to better distinguish ground truth / generated output: \\\\(L_{\\textrm{adv}}(D) = E_{y,z}((D(y) - 1)^2 + (D(G(z)))^2)\\\\) \n- For the generator (VITS decoder), to better \"confuse\" the discriminator: \\\\(L_{\\textrm{adv}}(G) = E_{z}(D(G(z) - 1)^2)\\\\) \n- Feature matching loss from [Larsen et al., 2016](https://arxiv.org/pdf/1512.09300), [Kumar et al., 2019](https://arxiv.org/pdf/1910.06711), which is essentially the L1 loss of the discriminator's intermediate states, when taking \\\\(y\\\\) and \\\\(G(z)\\\\) as input respectively: \\\\(L_{\\textrm{FM}}(G) = E_{y,z}(\\sum_{l} ||D_l(y) - D_l(G(z))||_1)\\\\)\n\nA little more about the decoder used (or more commonly known as **vocoder** here), it is the [NSF-HiFiGAN]() from DiffSinger. Instead of using mel-spectrogram as input like HiFi-GAN, [NSF](https://arxiv.org/pdf/1810.11946) (neural source-filter) takes in the learnt \"spectral\" features \\\\(z\\\\) and F0. The F0 is used to generate the excitation signal using a harmonic + noise *source module*. After that, the excitation signal and \\\\(z\\\\) are passed through the *neural filter module*, which consists of a series of convolutional layers and residual blocks across several upsampling resolutions, to \"filter\" the excitation signal in order to obtain the output audio. For the detailed implementation, kindly refer to the source code [here](https://github.com/openvpi/DiffSinger/blob/refactor/modules/nsf_hifigan/models.py#L253).\n\n<figure>\n  <img style=\"width:100%; margin-top:30px; margin-top:30px; margin-top:30px;\" src=\"/img/nsf.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Neural source filter model, diagram from ths <a href=\"https://arxiv.org/pdf/1810.11946\">NSF paper.</a> (exact details might differ from DiffSinger's implementation)</figcaption>\n</figure>\n<br/>\n\nOne more point to add is that, RVC chooses to use the *coarse* version of F0 (which is basically being [discretized into a fixed range of integers](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/train/extract/extract_f0_print.py#L95)) in the prior encoder, instead of the continuous-value fine-grain F0. I suppose the reason might be that the discretized \"F0 tokens\" are much easier to be used together with the HuBERT features, in this case, the discretized F0s are passed through an embedding layer and added to a projection of the HuBERT features, hence obtaining the condition signal for the prior encoder (see the source code [here](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L64)).\n\n## 3.4 - Retrieval Module\n\nThe above sections should cover most of the high-level details in So-VITS. So, what's new in RVC? The key idea is to store the ContentVec features for each speaker during training, hoping that they can be reused during inference, to add more target-speaker-related information in the generation pipeline. The aim is to generate output audio that could capture more detailed timbre and nuances of the target speaker, and reduce timbre leakage from the source speaker.\n\nTo do this, we need to store a **vector index** for each speaker. During training, all HuBERT feature vectors corresponding to the same speaker are saved. As later during inference, we want to be able to quickly search for the nearest vectors, given the HuBERT feature vectors from the source audio, so that we can either (i) use these nearest vectors as substitutes, or (ii) fuse them linearly with the source feature vectors. Therefore, we need to store a vector index to facilitate approximate nearest neighbour (ANN) search. Here RVC chooses to store an [inverted index file](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVF.html#structfaiss_1_1IndexIVF) (IVF), which first requires partitioning the vectors (e.g. through clustering), and then create an index that maps each cluster (centroid) to the data points (vectors) that belong to that cluster. During ANN search, we first identify the cluster centroid, and only searches the vectors in the cluster - this normally gives us a good enough approximate candidate. ANN is a super interesting topic - kindly refer to [faiss](https://github.com/facebookresearch/faiss/wiki)'s wiki if you are interested in other advanced indexing and search techniques, such as the popular [IVF-PQ](https://www.pinecone.io/learn/series/faiss/product-quantization/) and [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/) for large-scale vector search.\n\n## 4 - Inference \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-infer.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: RVC for inference.</figcaption>\n</figure>\n<br/>\n\nFor inference, let's first discuss the retrieval module. Given the source HuBERT feature vectors and a selected target speaker, we search for top-\\\\(K\\\\) vectors that are most similar to the source vectors (RVC chooses \\\\(K = 8\\\\)) from the vector index. RVC also introduces an `index_rate` parameter, \\\\(\\alpha\\\\), which decides how much of the target speaker feature vectors should be linearly fused with the source vectors (refer to Figure 2). The intuition here is that, although ContentVec is supposed to output speaker-invariant source feature vectors, in practice it might still have some source speaker-related information, so swapping it out with the target speaker features should better reduce the \"timbre leak\" from the source speaker. You can refer to the authors' notes on [how to tune the index rate](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/docs/en/faq_en.md#q11what-is-the-index-rate-for-and-how-to-adjust-it) - in most of the cases in RVC \\\\(\\alpha = 0.3\\\\).\n\nThe rest of the inference part in the VITS module is straightforward - a latent variable \\\\(z_p\\\\) is sampled from the prior encoder, which is conditioned on F0 and the (swapped) HuBERT features. As discussed, \\\\(z_p\\\\) is passed through an inverse flow \\\\(f^{-1}\\\\) to increase its distribution complexity, and it is fed into the NSF-HiFiGAN vocoder, together with the F0, to generate the output.\n\n## 5 - Related Work\n\nIt's truly an exciting time for singing voice conversion. First, there is a [singing voice conversion challenge](https://www.vc-challenge.org/) last year, and you can observe various new systems proposed with innovations on different content feature extractor, pitch extractor, and vocoder. Diffusion models, such as [DiffSVC](https://arxiv.org/pdf/2105.13871), are a popular choice recently, which formulates the conversion task as a denoising task using denoising diffusion probabilitic models (DDPM), conditioned by content features, F0s and loudness. To further speed up the inference speed, which is a commonly known issue for DDPMs, there is already a recent work that uses [consistency models](https://arxiv.org/pdf/2401.01792). There are also various attempts to make SVC fast and resource-efficient: [Nercessian et al. 2023](https://www.dafx.de/paper-archive/2023/DAFx23_paper_21.pdf) runs SVC as a plugin in real-time, [FastSVC](https://arxiv.org/pdf/2011.05731) achieves for a real-time factor of ~0.25 (1min to convert 4min of singing) on CPUs. \n\nFor a quick summary of the recent singing voice conversion methods, I recommend you to check out [this awesome article](https://medium.com/qosmo-lab/state-of-the-art-singing-voice-conversion-methods-12f01b35405b) by Naotake Masuda from Neutone. Also, check out a [comparative study paper](https://arxiv.org/pdf/2310.05203) on the systems submitted to the recent singing voice conversion challenge.\n\n## 6 - Code Implementation\n\nSharing a few code portals that point to the important modules in RVC:\n- [Pitch extraction](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract/extract_f0_rmvpe.py#L33)\n- [Content feature extraction](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract_feature_print.py#L80)\n- [VITS module](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L621)\n- [NSF-HiFiGAN vocoder](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L467)\n- Retrieval module [indexing](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer-web.py#L616) and [ANN retrieval](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/vc/pipeline.py#L235)","source":"_posts/annotated-rvc.md","raw":"---\ntitle: Understanding RVC - Retrieval-based Voice Conversion\ndate: 2024-09-26 22:53:51\ntags:\n    - Music Signal Processing\n    - Deep Learning\nestimatedReadTime: ~15 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Technical concepts in the RVC project\n2 - Individual modules, such as VITS, RMVPE, HuBERT\n3 - The `top-k` retrieval module, and how does it improve generation quality\n\n<div class=\"post-table-of-contents\">\n    <h3>Table of Contents</h3>\n    <a href=\"#\">1 - Introduction</a><br/>\n    <a href=\"#\">2 - Architecture</a><br/>\n    <a href=\"#\">3 - Deep Dive</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.1 - Content Feature Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.2 - Pitch Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.3 - Acoustic Model (VITS)</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.4 - Retrieval Module</a><br/>\n    <a href=\"#\">4 - Inference</a><br/>\n    <a href=\"#\">5 - Related Work</a><br/>\n</div>\n\n## 1 - Introduction\n\n**AI cover songs** have taken the internet by storm alongside the recent generative AI boom ignited by ChatGPT. If you still haven't had any experience on this piece of heavenly magic, have a listen to [Fake Drake](https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html), [AI Frank Sinatra](https://www.youtube.com/watch?v=HWsb7zTKplc&ab_channel=AiCovers), or [Stephanie Sun](https://www.youtube.com/watch?v=uPMXn7IbdXw&ab_channel=%E5%8D%8E%E8%AF%ADAI%E7%BF%BB%E5%94%B1) (who later posted a [gloomy response](https://www.straitstimes.com/life/entertainment/how-do-i-fight-with-that-stefanie-sun-issues-gloomy-response-to-popularity-of-ai-stefanie-sun) on the matter). To be honest, it's weird to feel exciting and scary at the same time after listening to them. \n\nIn this blog post, I would like to provide a breakdown on (arguably) one of the most popular voice conversion project, which is the [**RVC project**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI). I personally think that RVC is one of the reasons why AI covers have gained huge momentum, given that RVC provides [a rather permissive license](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE) to use its source code and pretrained models, and an easy-to-use, beginner-friendly Web UI for fine-tuning. \n\nI have come across many non-technical tutorials / videos about how to fine-tune RVC, but have yet to read an in-depth breakdown on the technical side of things, hence the motivation of writing this blog post. I will re-draw some of the diagrams based on my understanding, and (try to) justify the important steps in the RVC model. \n\nAnother thing to note is that, RVC shares a lot of common concepts with its \"predecessor\", the [**So-VITS project**](https://github.com/svc-develop-team/so-vits-svc/tree/4.1-Stable), so I hope that this post provides enough details to help readers understand both projects. My crude understanding is that the main difference between So-VITS and RVC is the `top-k` retrieval module, so although the choices of some parts of the modules might be different, but the overall framework should stay similar.\n\n## 2 - Architecture \n\nVoice conversion is essentially a disentanglement task which aims to separate the content and the speaker information. Generally, a voice conversion model consists of a **content encoder** that extracts speaker-invariant content information (such as phonemes, text, intonation), and a **acoustic model** that reconstructs the target voice based on the given content. \n\nWe can break-down RVC into the following modules:\n- A **content feature extractor** to extract information such as phonemes, intonation, etc. from the source audio. Here, RVC chooses [**HuBERT**](), or more precisely, a variant of [**ContentVec**]() - this choice is similar to the early versions of So-VITS. \n- A **pitch extractor** to get the coarse-level and fine-level F0 estimation. Pitch is an important part of the content information, especially in the context of singing voice. Here, RVC chooses [**RMVPE**]().\n- A **conditional acoustic model** to generate the target audio based on given conditions (i.e. speaker ID & content information). Here, RVC chooses [**VITS**](https://arxiv.org/pdf/2106.06103) as its generation framework, with some noticeble influence by [HiFi-GAN](https://arxiv.org/abs/2010.05646) on the [vocoder](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan) - this choice is also largely inherited from So-VITS.\n- A **retrieval module** newly introduced by RVC. Here, RVC stores all content features of the same speaker into a vector index, which can be used later for similarity search during inference. With this, since the content features are from the training set instead of being extracted solely from the source audio, it could introduce more information about the target speaker, further helping the reconstruction to sound more like the target speaker.\n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-train.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: The architecture of RVC for training.</figcaption>\n</figure>\n<br/>\n\n## 3.1 - Content Feature Extraction\n\n**HuBERT** is one of the most popular self-supervised speech representation, commonly used in speech and audio related tasks. First, audio chunks are converted into a series of tokens via an offline clustering method (e.g. a K-Means clustering on the MFCCs over a large dataset). Then, some tokens within a sequence are masked, and a Transformer is trained to predict the masked tokens, i.e. the **masked language modelling** method (there is a reason why BERT appears in the name). The features learnt seems to help reduce word-error rate in speech recognition tasks, showing its superiority in encoding content-related information. The clustering / masked-language-modelling parts are sometimes called the *teacher* / *student* modules.\n\nBoth RVC and So-VITS uses the same [hubert-base](https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt) model, but despite its name, I don't think it's a purely pretrained HuBERT model, because from the [**ContentVec**](https://proceedings.mlr.press/v162/qian22b/qian22b.pdf) paper, it claims that HuBERT features could still achieve fairly good results on speaker identification, contradicting with the aim to disentangle speaker information. From the clues provided in So-VITS, I am more convinced that this \"HuBERT\" model has a higher resemblance with **ContentVec** (check out their [video](https://www.youtube.com/watch?v=aiGp1g-dCY4&ab_channel=YangZhang)), which requires more steps to reduce the source speaker information. ContentVec is basically HuBERT, with a few speaker-invariant tweaks:\n- During offline-clustering, before converting the audio into HuBERT features, the audio is randomly converted into other speaker's voices;\n- Add speaker-related transformation (e.g. formant transform) as augmentation when training the speech representation network, and impose a contrastive loss to enforce invariance in timbre changes;\n- During masked label prediction, feed speaker info as condition to the student network to remove any need for further encoding speaker info. \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/contentvec.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: ContentVec training, diagram from the <a href=\"https://arxiv.org/pdf/2106.06103\">original paper.</a></figcaption>\n</figure>\n<br/>\n\n## 3.2 - Pitch Extraction\n\nPitch (or more precisely, fundamental frequency) extraction is crucial for singing voice transfer. A popular pitch extractor used previously is [crepe](https://github.com/marl/crepe), which is a convolutional neural network trained in a supervised manner. RVC chooses a recent work called [RMVPE](https://arxiv.org/pdf/2306.15412), which is based on a U-Net like architecture. The key improvement is that RMVPE performs well even when the input audio is not a clean vocal track, so users can extract vocal pitch directly for polyphonic music, removing the need of running through a source separation model (probably the reason why the authors choose a U-Net). They also observe accuracy improvements over other pitch extraction models. \n\n## 3.3 - Acoustic Model - VITS\n\nThe gist of RVC and So-VITS is their acoustic model, which is based on [VITS](https://arxiv.org/pdf/2106.06103). VITS is essentially a **conditional VAE**, augmented with **normalizing flows** and an **adversarial training** process. You can observe these 3 parts under the VITS module in Figure 1.\n\nFirst, let's frame the voice conversion problem as a *conditional generation* problem. Let \\\\(p(y | c)\\\\) denote the likelihood function that represents the voice conversion process, where \\\\(y\\\\) is the output audio and \\\\(c\\\\) is the content information condition (i.e. the HuBERT features and extracted pitch). We can approximate this intractable likelihood function by maximizing the following ELBO function, following the conditional VAE formulation:\n\n$$E_{z\\sim q(z|y)}[\\log p(y|z, c)] - \\mathcal{D}_{KL}(q(z|y) || p(z|c))$$\n\nFrom here we need 3 neural networks to parameterize the posterior encoder \\\\(q(z|y)\\\\), the prior encoder \\\\(p(z|c)\\\\), and a decoder \\\\(p(y|z, c)\\\\). The first term in the ELBO is the reconstruction loss, and the second term is the KL loss between the posterior and prior distribution. This pretty much sums up the conditional VAE formulation.\n\nTo further improve on this, the authors find that increasing the expressiveness of the prior distribution is important for generating realistic samples. So, a **normalizing (invertible) flow** \\\\(f_\\theta\\\\) is added to allow a transformation on the prior into a more complex distribution. Since the flow is invertible, during training the posterior is passed into the flow and compute the KL loss with the prior - this additional transform will help to bridge the \"gap\" between the posterior and the prior. Later during inference, the prior is passed into the inverse flow \\\\(f^{-1}_{\\theta}\\\\) to map it to a more complex distribution for generation. This highly resembles the technique proposed in the [variational inference with normalizing flows](https://arxiv.org/pdf/1505.05770) paper.\n\nLastly, as seen in most of the literature on high-fidelity singing voice synthesis or audio synthesis, a **[HiFi-GAN]()-style adversarial training** is used on the decoder to improve generation quality. You can observe this technique used in e.g. [DiffSinger](), [RAVE](), [Encodec](), [Stable Audio](), etc. The adversarial training introduces a (multi-period) discriminator that tells if the generation is ground truth or generated output. A few losses are introduced:\n\n- For the discriminator, to better distinguish ground truth / generated output: \\\\(L_{\\textrm{adv}}(D) = E_{y,z}((D(y) - 1)^2 + (D(G(z)))^2)\\\\) \n- For the generator (VITS decoder), to better \"confuse\" the discriminator: \\\\(L_{\\textrm{adv}}(G) = E_{z}(D(G(z) - 1)^2)\\\\) \n- Feature matching loss from [Larsen et al., 2016](https://arxiv.org/pdf/1512.09300), [Kumar et al., 2019](https://arxiv.org/pdf/1910.06711), which is essentially the L1 loss of the discriminator's intermediate states, when taking \\\\(y\\\\) and \\\\(G(z)\\\\) as input respectively: \\\\(L_{\\textrm{FM}}(G) = E_{y,z}(\\sum_{l} ||D_l(y) - D_l(G(z))||_1)\\\\)\n\nA little more about the decoder used (or more commonly known as **vocoder** here), it is the [NSF-HiFiGAN]() from DiffSinger. Instead of using mel-spectrogram as input like HiFi-GAN, [NSF](https://arxiv.org/pdf/1810.11946) (neural source-filter) takes in the learnt \"spectral\" features \\\\(z\\\\) and F0. The F0 is used to generate the excitation signal using a harmonic + noise *source module*. After that, the excitation signal and \\\\(z\\\\) are passed through the *neural filter module*, which consists of a series of convolutional layers and residual blocks across several upsampling resolutions, to \"filter\" the excitation signal in order to obtain the output audio. For the detailed implementation, kindly refer to the source code [here](https://github.com/openvpi/DiffSinger/blob/refactor/modules/nsf_hifigan/models.py#L253).\n\n<figure>\n  <img style=\"width:100%; margin-top:30px; margin-top:30px; margin-top:30px;\" src=\"/img/nsf.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Neural source filter model, diagram from ths <a href=\"https://arxiv.org/pdf/1810.11946\">NSF paper.</a> (exact details might differ from DiffSinger's implementation)</figcaption>\n</figure>\n<br/>\n\nOne more point to add is that, RVC chooses to use the *coarse* version of F0 (which is basically being [discretized into a fixed range of integers](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/train/extract/extract_f0_print.py#L95)) in the prior encoder, instead of the continuous-value fine-grain F0. I suppose the reason might be that the discretized \"F0 tokens\" are much easier to be used together with the HuBERT features, in this case, the discretized F0s are passed through an embedding layer and added to a projection of the HuBERT features, hence obtaining the condition signal for the prior encoder (see the source code [here](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L64)).\n\n## 3.4 - Retrieval Module\n\nThe above sections should cover most of the high-level details in So-VITS. So, what's new in RVC? The key idea is to store the ContentVec features for each speaker during training, hoping that they can be reused during inference, to add more target-speaker-related information in the generation pipeline. The aim is to generate output audio that could capture more detailed timbre and nuances of the target speaker, and reduce timbre leakage from the source speaker.\n\nTo do this, we need to store a **vector index** for each speaker. During training, all HuBERT feature vectors corresponding to the same speaker are saved. As later during inference, we want to be able to quickly search for the nearest vectors, given the HuBERT feature vectors from the source audio, so that we can either (i) use these nearest vectors as substitutes, or (ii) fuse them linearly with the source feature vectors. Therefore, we need to store a vector index to facilitate approximate nearest neighbour (ANN) search. Here RVC chooses to store an [inverted index file](https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVF.html#structfaiss_1_1IndexIVF) (IVF), which first requires partitioning the vectors (e.g. through clustering), and then create an index that maps each cluster (centroid) to the data points (vectors) that belong to that cluster. During ANN search, we first identify the cluster centroid, and only searches the vectors in the cluster - this normally gives us a good enough approximate candidate. ANN is a super interesting topic - kindly refer to [faiss](https://github.com/facebookresearch/faiss/wiki)'s wiki if you are interested in other advanced indexing and search techniques, such as the popular [IVF-PQ](https://www.pinecone.io/learn/series/faiss/product-quantization/) and [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/) for large-scale vector search.\n\n## 4 - Inference \n\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-infer.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: RVC for inference.</figcaption>\n</figure>\n<br/>\n\nFor inference, let's first discuss the retrieval module. Given the source HuBERT feature vectors and a selected target speaker, we search for top-\\\\(K\\\\) vectors that are most similar to the source vectors (RVC chooses \\\\(K = 8\\\\)) from the vector index. RVC also introduces an `index_rate` parameter, \\\\(\\alpha\\\\), which decides how much of the target speaker feature vectors should be linearly fused with the source vectors (refer to Figure 2). The intuition here is that, although ContentVec is supposed to output speaker-invariant source feature vectors, in practice it might still have some source speaker-related information, so swapping it out with the target speaker features should better reduce the \"timbre leak\" from the source speaker. You can refer to the authors' notes on [how to tune the index rate](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/docs/en/faq_en.md#q11what-is-the-index-rate-for-and-how-to-adjust-it) - in most of the cases in RVC \\\\(\\alpha = 0.3\\\\).\n\nThe rest of the inference part in the VITS module is straightforward - a latent variable \\\\(z_p\\\\) is sampled from the prior encoder, which is conditioned on F0 and the (swapped) HuBERT features. As discussed, \\\\(z_p\\\\) is passed through an inverse flow \\\\(f^{-1}\\\\) to increase its distribution complexity, and it is fed into the NSF-HiFiGAN vocoder, together with the F0, to generate the output.\n\n## 5 - Related Work\n\nIt's truly an exciting time for singing voice conversion. First, there is a [singing voice conversion challenge](https://www.vc-challenge.org/) last year, and you can observe various new systems proposed with innovations on different content feature extractor, pitch extractor, and vocoder. Diffusion models, such as [DiffSVC](https://arxiv.org/pdf/2105.13871), are a popular choice recently, which formulates the conversion task as a denoising task using denoising diffusion probabilitic models (DDPM), conditioned by content features, F0s and loudness. To further speed up the inference speed, which is a commonly known issue for DDPMs, there is already a recent work that uses [consistency models](https://arxiv.org/pdf/2401.01792). There are also various attempts to make SVC fast and resource-efficient: [Nercessian et al. 2023](https://www.dafx.de/paper-archive/2023/DAFx23_paper_21.pdf) runs SVC as a plugin in real-time, [FastSVC](https://arxiv.org/pdf/2011.05731) achieves for a real-time factor of ~0.25 (1min to convert 4min of singing) on CPUs. \n\nFor a quick summary of the recent singing voice conversion methods, I recommend you to check out [this awesome article](https://medium.com/qosmo-lab/state-of-the-art-singing-voice-conversion-methods-12f01b35405b) by Naotake Masuda from Neutone. Also, check out a [comparative study paper](https://arxiv.org/pdf/2310.05203) on the systems submitted to the recent singing voice conversion challenge.\n\n## 6 - Code Implementation\n\nSharing a few code portals that point to the important modules in RVC:\n- [Pitch extraction](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract/extract_f0_rmvpe.py#L33)\n- [Content feature extraction](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract_feature_print.py#L80)\n- [VITS module](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L621)\n- [NSF-HiFiGAN vocoder](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L467)\n- Retrieval module [indexing](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer-web.py#L616) and [ANN retrieval](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/vc/pipeline.py#L235)","slug":"annotated-rvc","published":1,"updated":"2025-06-27T09:15:32.473Z","_id":"cmbae7c0s0000809kgkv13omt","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Technical concepts in the RVC project<br>2 - Individual modules, such as VITS, RMVPE, HuBERT<br>3 - The <code>top-k</code> retrieval module, and how does it improve generation quality</p>\n<div class=\"post-table-of-contents\">\n    <h3>Table of Contents</h3>\n    <a href=\"#\">1 - Introduction</a><br/>\n    <a href=\"#\">2 - Architecture</a><br/>\n    <a href=\"#\">3 - Deep Dive</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.1 - Content Feature Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.2 - Pitch Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.3 - Acoustic Model (VITS)</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.4 - Retrieval Module</a><br/>\n    <a href=\"#\">4 - Inference</a><br/>\n    <a href=\"#\">5 - Related Work</a><br/>\n</div>\n\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p><strong>AI cover songs</strong> have taken the internet by storm alongside the recent generative AI boom ignited by ChatGPT. If you still haven’t had any experience on this piece of heavenly magic, have a listen to <a href=\"https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html\" target=\"_blank\" rel=\"noopener\">Fake Drake</a>, <a href=\"https://www.youtube.com/watch?v=HWsb7zTKplc&ab_channel=AiCovers\" target=\"_blank\" rel=\"noopener\">AI Frank Sinatra</a>, or <a href=\"https://www.youtube.com/watch?v=uPMXn7IbdXw&ab_channel=%E5%8D%8E%E8%AF%ADAI%E7%BF%BB%E5%94%B1\" target=\"_blank\" rel=\"noopener\">Stephanie Sun</a> (who later posted a <a href=\"https://www.straitstimes.com/life/entertainment/how-do-i-fight-with-that-stefanie-sun-issues-gloomy-response-to-popularity-of-ai-stefanie-sun\" target=\"_blank\" rel=\"noopener\">gloomy response</a> on the matter). To be honest, it’s weird to feel exciting and scary at the same time after listening to them. </p>\n<p>In this blog post, I would like to provide a breakdown on (arguably) one of the most popular voice conversion project, which is the <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI\" target=\"_blank\" rel=\"noopener\"><strong>RVC project</strong></a>. I personally think that RVC is one of the reasons why AI covers have gained huge momentum, given that RVC provides <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE\" target=\"_blank\" rel=\"noopener\">a rather permissive license</a> to use its source code and pretrained models, and an easy-to-use, beginner-friendly Web UI for fine-tuning. </p>\n<p>I have come across many non-technical tutorials / videos about how to fine-tune RVC, but have yet to read an in-depth breakdown on the technical side of things, hence the motivation of writing this blog post. I will re-draw some of the diagrams based on my understanding, and (try to) justify the important steps in the RVC model. </p>\n<p>Another thing to note is that, RVC shares a lot of common concepts with its “predecessor”, the <a href=\"https://github.com/svc-develop-team/so-vits-svc/tree/4.1-Stable\" target=\"_blank\" rel=\"noopener\"><strong>So-VITS project</strong></a>, so I hope that this post provides enough details to help readers understand both projects. My crude understanding is that the main difference between So-VITS and RVC is the <code>top-k</code> retrieval module, so although the choices of some parts of the modules might be different, but the overall framework should stay similar.</p>\n<h2 id=\"2-Architecture\"><a href=\"#2-Architecture\" class=\"headerlink\" title=\"2 - Architecture\"></a>2 - Architecture</h2><p>Voice conversion is essentially a disentanglement task which aims to separate the content and the speaker information. Generally, a voice conversion model consists of a <strong>content encoder</strong> that extracts speaker-invariant content information (such as phonemes, text, intonation), and a <strong>acoustic model</strong> that reconstructs the target voice based on the given content. </p>\n<p>We can break-down RVC into the following modules:</p>\n<ul>\n<li>A <strong>content feature extractor</strong> to extract information such as phonemes, intonation, etc. from the source audio. Here, RVC chooses <a href=\"\"><strong>HuBERT</strong></a>, or more precisely, a variant of <a href=\"\"><strong>ContentVec</strong></a> - this choice is similar to the early versions of So-VITS. </li>\n<li>A <strong>pitch extractor</strong> to get the coarse-level and fine-level F0 estimation. Pitch is an important part of the content information, especially in the context of singing voice. Here, RVC chooses <a href=\"\"><strong>RMVPE</strong></a>.</li>\n<li>A <strong>conditional acoustic model</strong> to generate the target audio based on given conditions (i.e. speaker ID &amp; content information). Here, RVC chooses <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\"><strong>VITS</strong></a> as its generation framework, with some noticeble influence by <a href=\"https://arxiv.org/abs/2010.05646\" target=\"_blank\" rel=\"noopener\">HiFi-GAN</a> on the <a href=\"https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan\" target=\"_blank\" rel=\"noopener\">vocoder</a> - this choice is also largely inherited from So-VITS.</li>\n<li>A <strong>retrieval module</strong> newly introduced by RVC. Here, RVC stores all content features of the same speaker into a vector index, which can be used later for similarity search during inference. With this, since the content features are from the training set instead of being extracted solely from the source audio, it could introduce more information about the target speaker, further helping the reconstruction to sound more like the target speaker.</li>\n</ul>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-train.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: The architecture of RVC for training.</figcaption>\n</figure>\n<br/>\n\n<h2 id=\"3-1-Content-Feature-Extraction\"><a href=\"#3-1-Content-Feature-Extraction\" class=\"headerlink\" title=\"3.1 - Content Feature Extraction\"></a>3.1 - Content Feature Extraction</h2><p><strong>HuBERT</strong> is one of the most popular self-supervised speech representation, commonly used in speech and audio related tasks. First, audio chunks are converted into a series of tokens via an offline clustering method (e.g. a K-Means clustering on the MFCCs over a large dataset). Then, some tokens within a sequence are masked, and a Transformer is trained to predict the masked tokens, i.e. the <strong>masked language modelling</strong> method (there is a reason why BERT appears in the name). The features learnt seems to help reduce word-error rate in speech recognition tasks, showing its superiority in encoding content-related information. The clustering / masked-language-modelling parts are sometimes called the <em>teacher</em> / <em>student</em> modules.</p>\n<p>Both RVC and So-VITS uses the same <a href=\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt\" target=\"_blank\" rel=\"noopener\">hubert-base</a> model, but despite its name, I don’t think it’s a purely pretrained HuBERT model, because from the <a href=\"https://proceedings.mlr.press/v162/qian22b/qian22b.pdf\" target=\"_blank\" rel=\"noopener\"><strong>ContentVec</strong></a> paper, it claims that HuBERT features could still achieve fairly good results on speaker identification, contradicting with the aim to disentangle speaker information. From the clues provided in So-VITS, I am more convinced that this “HuBERT” model has a higher resemblance with <strong>ContentVec</strong> (check out their <a href=\"https://www.youtube.com/watch?v=aiGp1g-dCY4&ab_channel=YangZhang\" target=\"_blank\" rel=\"noopener\">video</a>), which requires more steps to reduce the source speaker information. ContentVec is basically HuBERT, with a few speaker-invariant tweaks:</p>\n<ul>\n<li>During offline-clustering, before converting the audio into HuBERT features, the audio is randomly converted into other speaker’s voices;</li>\n<li>Add speaker-related transformation (e.g. formant transform) as augmentation when training the speech representation network, and impose a contrastive loss to enforce invariance in timbre changes;</li>\n<li>During masked label prediction, feed speaker info as condition to the student network to remove any need for further encoding speaker info. </li>\n</ul>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/contentvec.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: ContentVec training, diagram from the <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\">original paper.</a></figcaption>\n</figure>\n<br/>\n\n<h2 id=\"3-2-Pitch-Extraction\"><a href=\"#3-2-Pitch-Extraction\" class=\"headerlink\" title=\"3.2 - Pitch Extraction\"></a>3.2 - Pitch Extraction</h2><p>Pitch (or more precisely, fundamental frequency) extraction is crucial for singing voice transfer. A popular pitch extractor used previously is <a href=\"https://github.com/marl/crepe\" target=\"_blank\" rel=\"noopener\">crepe</a>, which is a convolutional neural network trained in a supervised manner. RVC chooses a recent work called <a href=\"https://arxiv.org/pdf/2306.15412\" target=\"_blank\" rel=\"noopener\">RMVPE</a>, which is based on a U-Net like architecture. The key improvement is that RMVPE performs well even when the input audio is not a clean vocal track, so users can extract vocal pitch directly for polyphonic music, removing the need of running through a source separation model (probably the reason why the authors choose a U-Net). They also observe accuracy improvements over other pitch extraction models. </p>\n<h2 id=\"3-3-Acoustic-Model-VITS\"><a href=\"#3-3-Acoustic-Model-VITS\" class=\"headerlink\" title=\"3.3 - Acoustic Model - VITS\"></a>3.3 - Acoustic Model - VITS</h2><p>The gist of RVC and So-VITS is their acoustic model, which is based on <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\">VITS</a>. VITS is essentially a <strong>conditional VAE</strong>, augmented with <strong>normalizing flows</strong> and an <strong>adversarial training</strong> process. You can observe these 3 parts under the VITS module in Figure 1.</p>\n<p>First, let’s frame the voice conversion problem as a <em>conditional generation</em> problem. Let \\(p(y | c)\\) denote the likelihood function that represents the voice conversion process, where \\(y\\) is the output audio and \\(c\\) is the content information condition (i.e. the HuBERT features and extracted pitch). We can approximate this intractable likelihood function by maximizing the following ELBO function, following the conditional VAE formulation:</p>\n<p>$$E_{z\\sim q(z|y)}[\\log p(y|z, c)] - \\mathcal{D}_{KL}(q(z|y) || p(z|c))$$</p>\n<p>From here we need 3 neural networks to parameterize the posterior encoder \\(q(z|y)\\), the prior encoder \\(p(z|c)\\), and a decoder \\(p(y|z, c)\\). The first term in the ELBO is the reconstruction loss, and the second term is the KL loss between the posterior and prior distribution. This pretty much sums up the conditional VAE formulation.</p>\n<p>To further improve on this, the authors find that increasing the expressiveness of the prior distribution is important for generating realistic samples. So, a <strong>normalizing (invertible) flow</strong> \\(f_\\theta\\) is added to allow a transformation on the prior into a more complex distribution. Since the flow is invertible, during training the posterior is passed into the flow and compute the KL loss with the prior - this additional transform will help to bridge the “gap” between the posterior and the prior. Later during inference, the prior is passed into the inverse flow \\(f^{-1}_{\\theta}\\) to map it to a more complex distribution for generation. This highly resembles the technique proposed in the <a href=\"https://arxiv.org/pdf/1505.05770\" target=\"_blank\" rel=\"noopener\">variational inference with normalizing flows</a> paper.</p>\n<p>Lastly, as seen in most of the literature on high-fidelity singing voice synthesis or audio synthesis, a <strong><a href=\"\">HiFi-GAN</a>-style adversarial training</strong> is used on the decoder to improve generation quality. You can observe this technique used in e.g. <a href=\"\">DiffSinger</a>, <a href=\"\">RAVE</a>, <a href=\"\">Encodec</a>, <a href=\"\">Stable Audio</a>, etc. The adversarial training introduces a (multi-period) discriminator that tells if the generation is ground truth or generated output. A few losses are introduced:</p>\n<ul>\n<li>For the discriminator, to better distinguish ground truth / generated output: \\(L_{\\textrm{adv}}(D) = E_{y,z}((D(y) - 1)^2 + (D(G(z)))^2)\\) </li>\n<li>For the generator (VITS decoder), to better “confuse” the discriminator: \\(L_{\\textrm{adv}}(G) = E_{z}(D(G(z) - 1)^2)\\) </li>\n<li>Feature matching loss from <a href=\"https://arxiv.org/pdf/1512.09300\" target=\"_blank\" rel=\"noopener\">Larsen et al., 2016</a>, <a href=\"https://arxiv.org/pdf/1910.06711\" target=\"_blank\" rel=\"noopener\">Kumar et al., 2019</a>, which is essentially the L1 loss of the discriminator’s intermediate states, when taking \\(y\\) and \\(G(z)\\) as input respectively: \\(L_{\\textrm{FM}}(G) = E_{y,z}(\\sum_{l} ||D_l(y) - D_l(G(z))||_1)\\)</li>\n</ul>\n<p>A little more about the decoder used (or more commonly known as <strong>vocoder</strong> here), it is the <a href=\"\">NSF-HiFiGAN</a> from DiffSinger. Instead of using mel-spectrogram as input like HiFi-GAN, <a href=\"https://arxiv.org/pdf/1810.11946\">NSF</a> (neural source-filter) takes in the learnt “spectral” features \\(z\\) and F0. The F0 is used to generate the excitation signal using a harmonic + noise <em>source module</em>. After that, the excitation signal and \\(z\\) are passed through the <em>neural filter module</em>, which consists of a series of convolutional layers and residual blocks across several upsampling resolutions, to “filter” the excitation signal in order to obtain the output audio. For the detailed implementation, kindly refer to the source code <a href=\"https://github.com/openvpi/DiffSinger/blob/refactor/modules/nsf_hifigan/models.py#L253\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px; margin-top:30px; margin-top:30px;\" src=\"/img/nsf.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Neural source filter model, diagram from ths <a href=\"https://arxiv.org/pdf/1810.11946\" target=\"_blank\" rel=\"noopener\">NSF paper.</a> (exact details might differ from DiffSinger's implementation)</figcaption>\n</figure>\n<br/>\n\n<p>One more point to add is that, RVC chooses to use the <em>coarse</em> version of F0 (which is basically being <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/train/extract/extract_f0_print.py#L95\" target=\"_blank\" rel=\"noopener\">discretized into a fixed range of integers</a>) in the prior encoder, instead of the continuous-value fine-grain F0. I suppose the reason might be that the discretized “F0 tokens” are much easier to be used together with the HuBERT features, in this case, the discretized F0s are passed through an embedding layer and added to a projection of the HuBERT features, hence obtaining the condition signal for the prior encoder (see the source code <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L64\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<h2 id=\"3-4-Retrieval-Module\"><a href=\"#3-4-Retrieval-Module\" class=\"headerlink\" title=\"3.4 - Retrieval Module\"></a>3.4 - Retrieval Module</h2><p>The above sections should cover most of the high-level details in So-VITS. So, what’s new in RVC? The key idea is to store the ContentVec features for each speaker during training, hoping that they can be reused during inference, to add more target-speaker-related information in the generation pipeline. The aim is to generate output audio that could capture more detailed timbre and nuances of the target speaker, and reduce timbre leakage from the source speaker.</p>\n<p>To do this, we need to store a <strong>vector index</strong> for each speaker. During training, all HuBERT feature vectors corresponding to the same speaker are saved. As later during inference, we want to be able to quickly search for the nearest vectors, given the HuBERT feature vectors from the source audio, so that we can either (i) use these nearest vectors as substitutes, or (ii) fuse them linearly with the source feature vectors. Therefore, we need to store a vector index to facilitate approximate nearest neighbour (ANN) search. Here RVC chooses to store an <a href=\"https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVF.html#structfaiss_1_1IndexIVF\" target=\"_blank\" rel=\"noopener\">inverted index file</a> (IVF), which first requires partitioning the vectors (e.g. through clustering), and then create an index that maps each cluster (centroid) to the data points (vectors) that belong to that cluster. During ANN search, we first identify the cluster centroid, and only searches the vectors in the cluster - this normally gives us a good enough approximate candidate. ANN is a super interesting topic - kindly refer to <a href=\"https://github.com/facebookresearch/faiss/wiki\" target=\"_blank\" rel=\"noopener\">faiss</a>‘s wiki if you are interested in other advanced indexing and search techniques, such as the popular <a href=\"https://www.pinecone.io/learn/series/faiss/product-quantization/\" target=\"_blank\" rel=\"noopener\">IVF-PQ</a> and <a href=\"https://www.pinecone.io/learn/series/faiss/hnsw/\" target=\"_blank\" rel=\"noopener\">HNSW</a> for large-scale vector search.</p>\n<h2 id=\"4-Inference\"><a href=\"#4-Inference\" class=\"headerlink\" title=\"4 - Inference\"></a>4 - Inference</h2><figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-infer.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: RVC for inference.</figcaption>\n</figure>\n<br/>\n\n<p>For inference, let’s first discuss the retrieval module. Given the source HuBERT feature vectors and a selected target speaker, we search for top-\\(K\\) vectors that are most similar to the source vectors (RVC chooses \\(K = 8\\)) from the vector index. RVC also introduces an <code>index_rate</code> parameter, \\(\\alpha\\), which decides how much of the target speaker feature vectors should be linearly fused with the source vectors (refer to Figure 2). The intuition here is that, although ContentVec is supposed to output speaker-invariant source feature vectors, in practice it might still have some source speaker-related information, so swapping it out with the target speaker features should better reduce the “timbre leak” from the source speaker. You can refer to the authors’ notes on <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/docs/en/faq_en.md#q11what-is-the-index-rate-for-and-how-to-adjust-it\" target=\"_blank\" rel=\"noopener\">how to tune the index rate</a> - in most of the cases in RVC \\(\\alpha = 0.3\\).</p>\n<p>The rest of the inference part in the VITS module is straightforward - a latent variable \\(z_p\\) is sampled from the prior encoder, which is conditioned on F0 and the (swapped) HuBERT features. As discussed, \\(z_p\\) is passed through an inverse flow \\(f^{-1}\\) to increase its distribution complexity, and it is fed into the NSF-HiFiGAN vocoder, together with the F0, to generate the output.</p>\n<h2 id=\"5-Related-Work\"><a href=\"#5-Related-Work\" class=\"headerlink\" title=\"5 - Related Work\"></a>5 - Related Work</h2><p>It’s truly an exciting time for singing voice conversion. First, there is a <a href=\"https://www.vc-challenge.org/\" target=\"_blank\" rel=\"noopener\">singing voice conversion challenge</a> last year, and you can observe various new systems proposed with innovations on different content feature extractor, pitch extractor, and vocoder. Diffusion models, such as <a href=\"https://arxiv.org/pdf/2105.13871\" target=\"_blank\" rel=\"noopener\">DiffSVC</a>, are a popular choice recently, which formulates the conversion task as a denoising task using denoising diffusion probabilitic models (DDPM), conditioned by content features, F0s and loudness. To further speed up the inference speed, which is a commonly known issue for DDPMs, there is already a recent work that uses <a href=\"https://arxiv.org/pdf/2401.01792\" target=\"_blank\" rel=\"noopener\">consistency models</a>. There are also various attempts to make SVC fast and resource-efficient: <a href=\"https://www.dafx.de/paper-archive/2023/DAFx23_paper_21.pdf\" target=\"_blank\" rel=\"noopener\">Nercessian et al. 2023</a> runs SVC as a plugin in real-time, <a href=\"https://arxiv.org/pdf/2011.05731\" target=\"_blank\" rel=\"noopener\">FastSVC</a> achieves for a real-time factor of ~0.25 (1min to convert 4min of singing) on CPUs. </p>\n<p>For a quick summary of the recent singing voice conversion methods, I recommend you to check out <a href=\"https://medium.com/qosmo-lab/state-of-the-art-singing-voice-conversion-methods-12f01b35405b\" target=\"_blank\" rel=\"noopener\">this awesome article</a> by Naotake Masuda from Neutone. Also, check out a <a href=\"https://arxiv.org/pdf/2310.05203\" target=\"_blank\" rel=\"noopener\">comparative study paper</a> on the systems submitted to the recent singing voice conversion challenge.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>Sharing a few code portals that point to the important modules in RVC:</p>\n<ul>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract/extract_f0_rmvpe.py#L33\" target=\"_blank\" rel=\"noopener\">Pitch extraction</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract_feature_print.py#L80\" target=\"_blank\" rel=\"noopener\">Content feature extraction</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L621\" target=\"_blank\" rel=\"noopener\">VITS module</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L467\" target=\"_blank\" rel=\"noopener\">NSF-HiFiGAN vocoder</a></li>\n<li>Retrieval module <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer-web.py#L616\" target=\"_blank\" rel=\"noopener\">indexing</a> and <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/vc/pipeline.py#L235\" target=\"_blank\" rel=\"noopener\">ANN retrieval</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Technical concepts in the RVC project<br>2 - Individual modules, such as VITS, RMVPE, HuBERT<br>3 - The <code>top-k</code> retrieval module, and how does it improve generation quality</p>\n<div class=\"post-table-of-contents\">\n    <h3>Table of Contents</h3>\n    <a href=\"#\">1 - Introduction</a><br/>\n    <a href=\"#\">2 - Architecture</a><br/>\n    <a href=\"#\">3 - Deep Dive</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.1 - Content Feature Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.2 - Pitch Extraction</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.3 - Acoustic Model (VITS)</a><br/>\n    <a href=\"#\">&emsp;&emsp;&emsp;3.4 - Retrieval Module</a><br/>\n    <a href=\"#\">4 - Inference</a><br/>\n    <a href=\"#\">5 - Related Work</a><br/>\n</div>\n\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p><strong>AI cover songs</strong> have taken the internet by storm alongside the recent generative AI boom ignited by ChatGPT. If you still haven’t had any experience on this piece of heavenly magic, have a listen to <a href=\"https://www.nytimes.com/2023/04/19/arts/music/ai-drake-the-weeknd-fake.html\" target=\"_blank\" rel=\"noopener\">Fake Drake</a>, <a href=\"https://www.youtube.com/watch?v=HWsb7zTKplc&ab_channel=AiCovers\" target=\"_blank\" rel=\"noopener\">AI Frank Sinatra</a>, or <a href=\"https://www.youtube.com/watch?v=uPMXn7IbdXw&ab_channel=%E5%8D%8E%E8%AF%ADAI%E7%BF%BB%E5%94%B1\" target=\"_blank\" rel=\"noopener\">Stephanie Sun</a> (who later posted a <a href=\"https://www.straitstimes.com/life/entertainment/how-do-i-fight-with-that-stefanie-sun-issues-gloomy-response-to-popularity-of-ai-stefanie-sun\" target=\"_blank\" rel=\"noopener\">gloomy response</a> on the matter). To be honest, it’s weird to feel exciting and scary at the same time after listening to them. </p>\n<p>In this blog post, I would like to provide a breakdown on (arguably) one of the most popular voice conversion project, which is the <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI\" target=\"_blank\" rel=\"noopener\"><strong>RVC project</strong></a>. I personally think that RVC is one of the reasons why AI covers have gained huge momentum, given that RVC provides <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE\" target=\"_blank\" rel=\"noopener\">a rather permissive license</a> to use its source code and pretrained models, and an easy-to-use, beginner-friendly Web UI for fine-tuning. </p>\n<p>I have come across many non-technical tutorials / videos about how to fine-tune RVC, but have yet to read an in-depth breakdown on the technical side of things, hence the motivation of writing this blog post. I will re-draw some of the diagrams based on my understanding, and (try to) justify the important steps in the RVC model. </p>\n<p>Another thing to note is that, RVC shares a lot of common concepts with its “predecessor”, the <a href=\"https://github.com/svc-develop-team/so-vits-svc/tree/4.1-Stable\" target=\"_blank\" rel=\"noopener\"><strong>So-VITS project</strong></a>, so I hope that this post provides enough details to help readers understand both projects. My crude understanding is that the main difference between So-VITS and RVC is the <code>top-k</code> retrieval module, so although the choices of some parts of the modules might be different, but the overall framework should stay similar.</p>\n<h2 id=\"2-Architecture\"><a href=\"#2-Architecture\" class=\"headerlink\" title=\"2 - Architecture\"></a>2 - Architecture</h2><p>Voice conversion is essentially a disentanglement task which aims to separate the content and the speaker information. Generally, a voice conversion model consists of a <strong>content encoder</strong> that extracts speaker-invariant content information (such as phonemes, text, intonation), and a <strong>acoustic model</strong> that reconstructs the target voice based on the given content. </p>\n<p>We can break-down RVC into the following modules:</p>\n<ul>\n<li>A <strong>content feature extractor</strong> to extract information such as phonemes, intonation, etc. from the source audio. Here, RVC chooses <a href=\"\"><strong>HuBERT</strong></a>, or more precisely, a variant of <a href=\"\"><strong>ContentVec</strong></a> - this choice is similar to the early versions of So-VITS. </li>\n<li>A <strong>pitch extractor</strong> to get the coarse-level and fine-level F0 estimation. Pitch is an important part of the content information, especially in the context of singing voice. Here, RVC chooses <a href=\"\"><strong>RMVPE</strong></a>.</li>\n<li>A <strong>conditional acoustic model</strong> to generate the target audio based on given conditions (i.e. speaker ID &amp; content information). Here, RVC chooses <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\"><strong>VITS</strong></a> as its generation framework, with some noticeble influence by <a href=\"https://arxiv.org/abs/2010.05646\" target=\"_blank\" rel=\"noopener\">HiFi-GAN</a> on the <a href=\"https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan\" target=\"_blank\" rel=\"noopener\">vocoder</a> - this choice is also largely inherited from So-VITS.</li>\n<li>A <strong>retrieval module</strong> newly introduced by RVC. Here, RVC stores all content features of the same speaker into a vector index, which can be used later for similarity search during inference. With this, since the content features are from the training set instead of being extracted solely from the source audio, it could introduce more information about the target speaker, further helping the reconstruction to sound more like the target speaker.</li>\n</ul>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-train.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: The architecture of RVC for training.</figcaption>\n</figure>\n<br/>\n\n<h2 id=\"3-1-Content-Feature-Extraction\"><a href=\"#3-1-Content-Feature-Extraction\" class=\"headerlink\" title=\"3.1 - Content Feature Extraction\"></a>3.1 - Content Feature Extraction</h2><p><strong>HuBERT</strong> is one of the most popular self-supervised speech representation, commonly used in speech and audio related tasks. First, audio chunks are converted into a series of tokens via an offline clustering method (e.g. a K-Means clustering on the MFCCs over a large dataset). Then, some tokens within a sequence are masked, and a Transformer is trained to predict the masked tokens, i.e. the <strong>masked language modelling</strong> method (there is a reason why BERT appears in the name). The features learnt seems to help reduce word-error rate in speech recognition tasks, showing its superiority in encoding content-related information. The clustering / masked-language-modelling parts are sometimes called the <em>teacher</em> / <em>student</em> modules.</p>\n<p>Both RVC and So-VITS uses the same <a href=\"https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt\" target=\"_blank\" rel=\"noopener\">hubert-base</a> model, but despite its name, I don’t think it’s a purely pretrained HuBERT model, because from the <a href=\"https://proceedings.mlr.press/v162/qian22b/qian22b.pdf\" target=\"_blank\" rel=\"noopener\"><strong>ContentVec</strong></a> paper, it claims that HuBERT features could still achieve fairly good results on speaker identification, contradicting with the aim to disentangle speaker information. From the clues provided in So-VITS, I am more convinced that this “HuBERT” model has a higher resemblance with <strong>ContentVec</strong> (check out their <a href=\"https://www.youtube.com/watch?v=aiGp1g-dCY4&ab_channel=YangZhang\" target=\"_blank\" rel=\"noopener\">video</a>), which requires more steps to reduce the source speaker information. ContentVec is basically HuBERT, with a few speaker-invariant tweaks:</p>\n<ul>\n<li>During offline-clustering, before converting the audio into HuBERT features, the audio is randomly converted into other speaker’s voices;</li>\n<li>Add speaker-related transformation (e.g. formant transform) as augmentation when training the speech representation network, and impose a contrastive loss to enforce invariance in timbre changes;</li>\n<li>During masked label prediction, feed speaker info as condition to the student network to remove any need for further encoding speaker info. </li>\n</ul>\n<figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/contentvec.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: ContentVec training, diagram from the <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\">original paper.</a></figcaption>\n</figure>\n<br/>\n\n<h2 id=\"3-2-Pitch-Extraction\"><a href=\"#3-2-Pitch-Extraction\" class=\"headerlink\" title=\"3.2 - Pitch Extraction\"></a>3.2 - Pitch Extraction</h2><p>Pitch (or more precisely, fundamental frequency) extraction is crucial for singing voice transfer. A popular pitch extractor used previously is <a href=\"https://github.com/marl/crepe\" target=\"_blank\" rel=\"noopener\">crepe</a>, which is a convolutional neural network trained in a supervised manner. RVC chooses a recent work called <a href=\"https://arxiv.org/pdf/2306.15412\" target=\"_blank\" rel=\"noopener\">RMVPE</a>, which is based on a U-Net like architecture. The key improvement is that RMVPE performs well even when the input audio is not a clean vocal track, so users can extract vocal pitch directly for polyphonic music, removing the need of running through a source separation model (probably the reason why the authors choose a U-Net). They also observe accuracy improvements over other pitch extraction models. </p>\n<h2 id=\"3-3-Acoustic-Model-VITS\"><a href=\"#3-3-Acoustic-Model-VITS\" class=\"headerlink\" title=\"3.3 - Acoustic Model - VITS\"></a>3.3 - Acoustic Model - VITS</h2><p>The gist of RVC and So-VITS is their acoustic model, which is based on <a href=\"https://arxiv.org/pdf/2106.06103\" target=\"_blank\" rel=\"noopener\">VITS</a>. VITS is essentially a <strong>conditional VAE</strong>, augmented with <strong>normalizing flows</strong> and an <strong>adversarial training</strong> process. You can observe these 3 parts under the VITS module in Figure 1.</p>\n<p>First, let’s frame the voice conversion problem as a <em>conditional generation</em> problem. Let \\(p(y | c)\\) denote the likelihood function that represents the voice conversion process, where \\(y\\) is the output audio and \\(c\\) is the content information condition (i.e. the HuBERT features and extracted pitch). We can approximate this intractable likelihood function by maximizing the following ELBO function, following the conditional VAE formulation:</p>\n<p>$$E_{z\\sim q(z|y)}[\\log p(y|z, c)] - \\mathcal{D}_{KL}(q(z|y) || p(z|c))$$</p>\n<p>From here we need 3 neural networks to parameterize the posterior encoder \\(q(z|y)\\), the prior encoder \\(p(z|c)\\), and a decoder \\(p(y|z, c)\\). The first term in the ELBO is the reconstruction loss, and the second term is the KL loss between the posterior and prior distribution. This pretty much sums up the conditional VAE formulation.</p>\n<p>To further improve on this, the authors find that increasing the expressiveness of the prior distribution is important for generating realistic samples. So, a <strong>normalizing (invertible) flow</strong> \\(f_\\theta\\) is added to allow a transformation on the prior into a more complex distribution. Since the flow is invertible, during training the posterior is passed into the flow and compute the KL loss with the prior - this additional transform will help to bridge the “gap” between the posterior and the prior. Later during inference, the prior is passed into the inverse flow \\(f^{-1}_{\\theta}\\) to map it to a more complex distribution for generation. This highly resembles the technique proposed in the <a href=\"https://arxiv.org/pdf/1505.05770\" target=\"_blank\" rel=\"noopener\">variational inference with normalizing flows</a> paper.</p>\n<p>Lastly, as seen in most of the literature on high-fidelity singing voice synthesis or audio synthesis, a <strong><a href=\"\">HiFi-GAN</a>-style adversarial training</strong> is used on the decoder to improve generation quality. You can observe this technique used in e.g. <a href=\"\">DiffSinger</a>, <a href=\"\">RAVE</a>, <a href=\"\">Encodec</a>, <a href=\"\">Stable Audio</a>, etc. The adversarial training introduces a (multi-period) discriminator that tells if the generation is ground truth or generated output. A few losses are introduced:</p>\n<ul>\n<li>For the discriminator, to better distinguish ground truth / generated output: \\(L_{\\textrm{adv}}(D) = E_{y,z}((D(y) - 1)^2 + (D(G(z)))^2)\\) </li>\n<li>For the generator (VITS decoder), to better “confuse” the discriminator: \\(L_{\\textrm{adv}}(G) = E_{z}(D(G(z) - 1)^2)\\) </li>\n<li>Feature matching loss from <a href=\"https://arxiv.org/pdf/1512.09300\" target=\"_blank\" rel=\"noopener\">Larsen et al., 2016</a>, <a href=\"https://arxiv.org/pdf/1910.06711\" target=\"_blank\" rel=\"noopener\">Kumar et al., 2019</a>, which is essentially the L1 loss of the discriminator’s intermediate states, when taking \\(y\\) and \\(G(z)\\) as input respectively: \\(L_{\\textrm{FM}}(G) = E_{y,z}(\\sum_{l} ||D_l(y) - D_l(G(z))||_1)\\)</li>\n</ul>\n<p>A little more about the decoder used (or more commonly known as <strong>vocoder</strong> here), it is the <a href=\"\">NSF-HiFiGAN</a> from DiffSinger. Instead of using mel-spectrogram as input like HiFi-GAN, <a href=\"https://arxiv.org/pdf/1810.11946\">NSF</a> (neural source-filter) takes in the learnt “spectral” features \\(z\\) and F0. The F0 is used to generate the excitation signal using a harmonic + noise <em>source module</em>. After that, the excitation signal and \\(z\\) are passed through the <em>neural filter module</em>, which consists of a series of convolutional layers and residual blocks across several upsampling resolutions, to “filter” the excitation signal in order to obtain the output audio. For the detailed implementation, kindly refer to the source code <a href=\"https://github.com/openvpi/DiffSinger/blob/refactor/modules/nsf_hifigan/models.py#L253\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<figure>\n  <img style=\"width:100%; margin-top:30px; margin-top:30px; margin-top:30px;\" src=\"/img/nsf.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Neural source filter model, diagram from ths <a href=\"https://arxiv.org/pdf/1810.11946\" target=\"_blank\" rel=\"noopener\">NSF paper.</a> (exact details might differ from DiffSinger's implementation)</figcaption>\n</figure>\n<br/>\n\n<p>One more point to add is that, RVC chooses to use the <em>coarse</em> version of F0 (which is basically being <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/train/extract/extract_f0_print.py#L95\" target=\"_blank\" rel=\"noopener\">discretized into a fixed range of integers</a>) in the prior encoder, instead of the continuous-value fine-grain F0. I suppose the reason might be that the discretized “F0 tokens” are much easier to be used together with the HuBERT features, in this case, the discretized F0s are passed through an embedding layer and added to a projection of the HuBERT features, hence obtaining the condition signal for the prior encoder (see the source code <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L64\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<h2 id=\"3-4-Retrieval-Module\"><a href=\"#3-4-Retrieval-Module\" class=\"headerlink\" title=\"3.4 - Retrieval Module\"></a>3.4 - Retrieval Module</h2><p>The above sections should cover most of the high-level details in So-VITS. So, what’s new in RVC? The key idea is to store the ContentVec features for each speaker during training, hoping that they can be reused during inference, to add more target-speaker-related information in the generation pipeline. The aim is to generate output audio that could capture more detailed timbre and nuances of the target speaker, and reduce timbre leakage from the source speaker.</p>\n<p>To do this, we need to store a <strong>vector index</strong> for each speaker. During training, all HuBERT feature vectors corresponding to the same speaker are saved. As later during inference, we want to be able to quickly search for the nearest vectors, given the HuBERT feature vectors from the source audio, so that we can either (i) use these nearest vectors as substitutes, or (ii) fuse them linearly with the source feature vectors. Therefore, we need to store a vector index to facilitate approximate nearest neighbour (ANN) search. Here RVC chooses to store an <a href=\"https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVF.html#structfaiss_1_1IndexIVF\" target=\"_blank\" rel=\"noopener\">inverted index file</a> (IVF), which first requires partitioning the vectors (e.g. through clustering), and then create an index that maps each cluster (centroid) to the data points (vectors) that belong to that cluster. During ANN search, we first identify the cluster centroid, and only searches the vectors in the cluster - this normally gives us a good enough approximate candidate. ANN is a super interesting topic - kindly refer to <a href=\"https://github.com/facebookresearch/faiss/wiki\" target=\"_blank\" rel=\"noopener\">faiss</a>‘s wiki if you are interested in other advanced indexing and search techniques, such as the popular <a href=\"https://www.pinecone.io/learn/series/faiss/product-quantization/\" target=\"_blank\" rel=\"noopener\">IVF-PQ</a> and <a href=\"https://www.pinecone.io/learn/series/faiss/hnsw/\" target=\"_blank\" rel=\"noopener\">HNSW</a> for large-scale vector search.</p>\n<h2 id=\"4-Inference\"><a href=\"#4-Inference\" class=\"headerlink\" title=\"4 - Inference\"></a>4 - Inference</h2><figure>\n  <img style=\"width:100%; margin-top:30px;\" src=\"/img/rvc-infer.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: RVC for inference.</figcaption>\n</figure>\n<br/>\n\n<p>For inference, let’s first discuss the retrieval module. Given the source HuBERT feature vectors and a selected target speaker, we search for top-\\(K\\) vectors that are most similar to the source vectors (RVC chooses \\(K = 8\\)) from the vector index. RVC also introduces an <code>index_rate</code> parameter, \\(\\alpha\\), which decides how much of the target speaker feature vectors should be linearly fused with the source vectors (refer to Figure 2). The intuition here is that, although ContentVec is supposed to output speaker-invariant source feature vectors, in practice it might still have some source speaker-related information, so swapping it out with the target speaker features should better reduce the “timbre leak” from the source speaker. You can refer to the authors’ notes on <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/docs/en/faq_en.md#q11what-is-the-index-rate-for-and-how-to-adjust-it\" target=\"_blank\" rel=\"noopener\">how to tune the index rate</a> - in most of the cases in RVC \\(\\alpha = 0.3\\).</p>\n<p>The rest of the inference part in the VITS module is straightforward - a latent variable \\(z_p\\) is sampled from the prior encoder, which is conditioned on F0 and the (swapped) HuBERT features. As discussed, \\(z_p\\) is passed through an inverse flow \\(f^{-1}\\) to increase its distribution complexity, and it is fed into the NSF-HiFiGAN vocoder, together with the F0, to generate the output.</p>\n<h2 id=\"5-Related-Work\"><a href=\"#5-Related-Work\" class=\"headerlink\" title=\"5 - Related Work\"></a>5 - Related Work</h2><p>It’s truly an exciting time for singing voice conversion. First, there is a <a href=\"https://www.vc-challenge.org/\" target=\"_blank\" rel=\"noopener\">singing voice conversion challenge</a> last year, and you can observe various new systems proposed with innovations on different content feature extractor, pitch extractor, and vocoder. Diffusion models, such as <a href=\"https://arxiv.org/pdf/2105.13871\" target=\"_blank\" rel=\"noopener\">DiffSVC</a>, are a popular choice recently, which formulates the conversion task as a denoising task using denoising diffusion probabilitic models (DDPM), conditioned by content features, F0s and loudness. To further speed up the inference speed, which is a commonly known issue for DDPMs, there is already a recent work that uses <a href=\"https://arxiv.org/pdf/2401.01792\" target=\"_blank\" rel=\"noopener\">consistency models</a>. There are also various attempts to make SVC fast and resource-efficient: <a href=\"https://www.dafx.de/paper-archive/2023/DAFx23_paper_21.pdf\" target=\"_blank\" rel=\"noopener\">Nercessian et al. 2023</a> runs SVC as a plugin in real-time, <a href=\"https://arxiv.org/pdf/2011.05731\" target=\"_blank\" rel=\"noopener\">FastSVC</a> achieves for a real-time factor of ~0.25 (1min to convert 4min of singing) on CPUs. </p>\n<p>For a quick summary of the recent singing voice conversion methods, I recommend you to check out <a href=\"https://medium.com/qosmo-lab/state-of-the-art-singing-voice-conversion-methods-12f01b35405b\" target=\"_blank\" rel=\"noopener\">this awesome article</a> by Naotake Masuda from Neutone. Also, check out a <a href=\"https://arxiv.org/pdf/2310.05203\" target=\"_blank\" rel=\"noopener\">comparative study paper</a> on the systems submitted to the recent singing voice conversion challenge.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>Sharing a few code portals that point to the important modules in RVC:</p>\n<ul>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract/extract_f0_rmvpe.py#L33\" target=\"_blank\" rel=\"noopener\">Pitch extraction</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/infer/modules/train/extract_feature_print.py#L80\" target=\"_blank\" rel=\"noopener\">Content feature extraction</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L621\" target=\"_blank\" rel=\"noopener\">VITS module</a></li>\n<li><a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/lib/infer_pack/models.py#L467\" target=\"_blank\" rel=\"noopener\">NSF-HiFiGAN vocoder</a></li>\n<li>Retrieval module <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer-web.py#L616\" target=\"_blank\" rel=\"noopener\">indexing</a> and <a href=\"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/3548b4f1a55336629955c0d51deeb24b6de9c46e/infer/modules/vc/pipeline.py#L235\" target=\"_blank\" rel=\"noopener\">ANN retrieval</a></li>\n</ul>\n"},{"title":"Approximation of The Power Function","date":"2024-01-02T06:33:21.000Z","estimatedReadTime":"~10 minutes","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nThe power function \\\\(x^y\\\\) is integral to many DSP applications, such as dB to linear gain conversion (\\\\(y = 10^\\frac{x}{20}\\\\)), and semitone to Hz conversion (\\\\(f_t = f_0 \\cdot 2^{\\frac{t}{12}}\\\\)). When studying the code in [Dexed](https://github.com/asb2m10/dexed), an FM synth modelled over DX7, I find many use cases of the `exp2` function (\\\\(2^x\\\\)), especially in the amplitude envelope calculation. \n\nIn this post, we will look at how \\\\(2^x\\\\), or the `exp2` function, can be approximated for speed-intensive, precision-tolerant use cases. Note that we only discuss the case of `exp2`, because it is a convenient base in floating point representation (more on this later), and it is easily extendable to the generic power function \\\\(x^y\\\\). Given \\\\(f(k) = 2^k\\\\), we can transform the power function by multiplying a constant \\\\(\\log_{2}{x}\\\\) on the input to make use of \\\\(f(\\cdot)\\\\):\n\n$$x^y = 2^{y \\cdot \\log_{2}{x}} = f(y \\cdot \\log_{2}{x})$$\n\n## Initial ideas\n\nA straightforward approach is to truncate the **Taylor series** of \\\\(2^x\\\\) up to the \\\\(n\\\\)-th term. One can get the Taylor series of \\\\(2^x\\\\) as:\n\n$$2^x = e^{x \\ln 2} = 1 + \\frac{x \\ln 2}{1!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{(x \\ln 2)^3}{3!} + ... $$\n\nHowever, to get a good approximation across a wide input range, it requires higher order of polynomials, which is computationally intensive. \n\nAnother idea from Dexed is to [use a finite-range lookup table and fixed-point arithmetic](https://github.com/asb2m10/dexed/blob/master/Source/msfa/exp2.h), however this method is usable only for fixed-point systems.\n\nTo get a more precise and efficient implementation in floating point, we need to first understand the floating point representation.\n\n## Separating the integer and decimal part\n\nLet's say we want to implement an `exp-2` approximation for a single-precision (32-bit) floating point system. According to [IEEE-754 floating point representation](https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/), it consists of 1 sign bit, 8 exponent bits, and 32 mantissa (or fractional) bits, as depicted in the diagram:\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ieee_fp.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: IEEE-754 single-precision floating point format.</figcaption>\n</figure>\n\nThe corresponding formula of single-precision floating point is \\\\((−1)^{S} × 1.M × 2^{(E − 127)}\\\\). From this formula, we can observe that: **given an integer input, calculating `exp2` is essentially bit-shifting to get the exponent bits \\\\(E\\\\)**. We also need to add the bias value in the exponent bits before bit-shifting. For single-precision, the bias value is 127 or 0x7f, as shown in the formula above.\n\nThis gives us an idea of how we can tackle the approximation separately, given an input \\\\(x\\\\):\n- for the integer part \\\\(\\lfloor x \\rfloor \\\\), bit-shift to the exponent bits;\n- for the decimal part  \\\\(x - \\lfloor x \\rfloor \\\\), use a rational approximation;\n- multiply the output of both parts \\\\(2^{x} = 2^{\\lfloor x \\rfloor} \\cdot 2^{x - \\lfloor x \\rfloor}\\\\) (in C++, we can use `ldexp`)\n\n## Rational approximation of `exp2f`\n\nDepending on the [rounding mode](https://en.wikipedia.org/wiki/Rounding) used to extract the integer part, the range of the decimal part would either be within \\\\([-0.5, 0.5]\\\\) or \\\\([0, 1)\\\\). With this, we only need an approximation precise enough within this range, which is more achievable.\n\nThere are a myriad of ideas on how this approximation could be achieved. We can start from an n-th order polynomial approximation. For example, with the help of `np.polyfit` we can get a 3rd-order polynomial approximation:\n\n$$ 2^{x} \\approx 0.05700169x^{3}\\ + 0.24858144x^{2} + 0.69282515x + 0.9991608, \\quad x \\in [-1, 1]$$\n\nThis is actually quite close to the Taylor's expansion at order 3:\n\n$$ 2^{x} \\approx \\frac{(x \\ln 2)^3}{3!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{x \\ln 2}{1!} + 1 $$\n\n$$ \\quad \\quad \\quad \\quad \\quad = 0.0555041x^{3}\\ + 0.2402265x^{2} + 0.693147x + 1 $$\n\nThe [Cephes library](https://github.com/nearform/node-cephes/blob/master/cephes/exp2.c) uses a [Padé approximant](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant) in the form of:\n\n$$ 2^{x} \\approx 1 +  2x \\frac{P(x^2)}{Q(x^2) - xP(x^2)}, \\quad x \\in [-0.5, 0.5]$$\n\n$$ P(x) = 0.002309x^{2}+20.202x+1513.906 $$\n\n$$ Q(x) = x^{2}+233.184x+4368.211 $$\n\nFrom [a blog post by Paul Mineiro](http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html), it seems like the author also uses something similar to Padé approximant, but with a lower polynomial order:\n\n$$ 2^{x} \\approx 1 + \\frac{27.7280233}{4.84252568 - x} − 0.49012907x − 5.7259425, \\quad x \\in [0, 1)$$\n\n## Timing and Accuracy\n\nWe report the absolute error of each approximation method within a given input range. [Test script here](https://gist.github.com/gudgud96/ec369cd017b10fb1376300fa325f9321).\n\nWithin input range of \\\\([0, 1)\\\\), 10000 sample points:\n\n|                      |                 max                    |                   min                   |                   avg                  |\n|----------------------|----------------------------------------|-----------------------------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 2.423 \\times 10^{-3} \\quad\\\\) | \\\\(\\quad 1.192 \\times 10^{-7} \\quad\\\\)  | \\\\(\\quad 6.736 \\times 10^{-4} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 5.829 \\times 10^{-5} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 2.267 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 2.384 \\times 10^{-7} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 2.501 \\times 10^{-8} \\quad\\\\) |\n\nWithin input range of \\\\([-0.5, 0.5]\\\\), 10000 sample points:\n\n|                      |                 max                    |                   min                   |                   avg                  |\n|----------------------|----------------------------------------|-----------------------------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 8.423 \\times 10^{-4} \\quad\\\\) | \\\\(\\quad 5.960 \\times 10^{-8} \\quad\\\\)  | \\\\(\\quad 4.764 \\times 10^{-4} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 4.995 \\times 10^{-5} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 1.623 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 1.192 \\times 10^{-7} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 1.798 \\times 10^{-8} \\quad\\\\) |\n\n\nWe also measure the total time taken to run on 10000 sample points, averaged across 5 runs:\n\n|                      |                 in secs                |\n|----------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 4.747 \\times 10^{-5} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 8.229 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 4.854 \\times 10^{-4} \\quad\\\\) |\n\nWe can see Cephes provides the best accuracy, while 3rd-order polynomial approximation provides the best speed. Mineiro's method keeps the absolute error within the order of magnitude \\\\(10^{-5}\\\\), while using only ~20% of the time needed by Cephes.\n\n\n## Code example in SIMD\n\nSIMD is commonly used to provide further computation speedup on CPU. The aim of of this post is also to find an efficient SIMD implementation for `exp2`, which is still lacking in common SIMD operation sets. Below we will look at an example of `exp2` approximation implemented using SSE3. We use the 3rd-order polynomial approximation below:\n\n```c++\n__m128 fast_exp_sse (__m128 x)  {\n    __m128 x_int_f, x_frac, xx;\n    __m128i x_int;\n\n    __m128 c0  = _mm_set1_ps (0.05700169f);\n    __m128 c1  = _mm_set1_ps (0.24858144f);\n    __m128 c2  = _mm_set1_ps (0.69282515f);\n    __m128 c3  = _mm_set1_ps (0.99916080f);\n\n    // obtain the integer and fractional part\n    x_int = _mm_cvtps_epi32(x);\n    x_int_f = _mm_cvtepi32_ps(x_int);\n    x_frac = _mm_sub_ps(x, x_int_f);\n\n    // perform 3rd-order polynomial approximation on fractional part\n    xx = _mm_mul_ps(x_frac, c0);\n    xx = _mm_add_ps(xx, c1);\n    xx = _mm_mul_ps(x_frac, xx);\n    xx = _mm_add_ps(xx, c2);\n    xx = _mm_mul_ps(x_frac, xx);\n    xx = _mm_add_ps(xx, c3);\n\n    // compute 2^n for integer part through bit-shifting and adding to exponent field\n    x_int = _mm_add_epi32(x_int, _mm_set1_epi32(0x7f));\n    x_int = _mm_slli_epi32(x_int, 23);\n    x_int_f = _mm_castsi128_ps(x_int);\n\n    // compute final result, 2^n = (2^i)(2^f)\n    xx = _mm_mul_ps(xx, x_int_f);\n\n    return xx\n}\n```\n\nSome notes to discuss:\n\n- For the integer rounding part, `_mm_cvtps_epi32` is used, which is a float-to-int casting. To use round-to-nearest mode, we can use `_mm_round_ps`, but it is only supported in SSE4.1.\n\n- There is a difference between **type conversion** `_mm_cvtps_epi32` and **reinterpret casting** `_mm_castsi128_ps`. Type conversion converts a fixed point integer representation to a floating point representation, and retain its value. Reinterpret casting takes the byte pattern of the fixed-point input, and reinterprets it based on the floating point representation.\n\n- Padé approximant can be used by replacing lines 16-21, and would require the division operator `_mm_div_ps`.\n\n## References\n\n1. [Creating a Compiler Optimized Inlineable Implementation of Intel Svml Simd Intrinsics](http://ijeais.org/wp-content/uploads/2018/07/IJAER180702.pdf)\n\n2. [Added vectorized implementation of the exponential function for ARM/NEON](http://dalab.se.sjtu.edu.cn/gitlab/xiaoyuwei/eigen/-/commit/cc5d7ff5238da45ef7416ec94f18227486ed9643)\n\n3. [Fastest Implementation of the Natural Exponential Function Using SSE](https://stackoverflow.com/questions/47025373/fastest-implementation-of-the-natural-exponential-function-using-sse)\n\n4. [exp-2 in torch-cephes library](https://github.com/google-deepmind/torch-cephes/blob/master/cephes/cmath/exp2.c)\n\n5. [Fast Approximate Logarithm, Exponential, Power, and Inverse Root](http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html)\n\n6. [fastapprox](https://github.com/etheory/fastapprox)\n\n7. [Where does this approximation for 2^{x} − 1 come from?](https://math.stackexchange.com/questions/4581468/where-does-this-approximation-for-2x-1-come-from)","source":"_posts/exp-2.md","raw":"---\ntitle: Approximation of The Power Function\ndate: 2024-01-02 14:33:21\ntags:\n    - Music Signal Processing\nestimatedReadTime: ~10 minutes\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nThe power function \\\\(x^y\\\\) is integral to many DSP applications, such as dB to linear gain conversion (\\\\(y = 10^\\frac{x}{20}\\\\)), and semitone to Hz conversion (\\\\(f_t = f_0 \\cdot 2^{\\frac{t}{12}}\\\\)). When studying the code in [Dexed](https://github.com/asb2m10/dexed), an FM synth modelled over DX7, I find many use cases of the `exp2` function (\\\\(2^x\\\\)), especially in the amplitude envelope calculation. \n\nIn this post, we will look at how \\\\(2^x\\\\), or the `exp2` function, can be approximated for speed-intensive, precision-tolerant use cases. Note that we only discuss the case of `exp2`, because it is a convenient base in floating point representation (more on this later), and it is easily extendable to the generic power function \\\\(x^y\\\\). Given \\\\(f(k) = 2^k\\\\), we can transform the power function by multiplying a constant \\\\(\\log_{2}{x}\\\\) on the input to make use of \\\\(f(\\cdot)\\\\):\n\n$$x^y = 2^{y \\cdot \\log_{2}{x}} = f(y \\cdot \\log_{2}{x})$$\n\n## Initial ideas\n\nA straightforward approach is to truncate the **Taylor series** of \\\\(2^x\\\\) up to the \\\\(n\\\\)-th term. One can get the Taylor series of \\\\(2^x\\\\) as:\n\n$$2^x = e^{x \\ln 2} = 1 + \\frac{x \\ln 2}{1!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{(x \\ln 2)^3}{3!} + ... $$\n\nHowever, to get a good approximation across a wide input range, it requires higher order of polynomials, which is computationally intensive. \n\nAnother idea from Dexed is to [use a finite-range lookup table and fixed-point arithmetic](https://github.com/asb2m10/dexed/blob/master/Source/msfa/exp2.h), however this method is usable only for fixed-point systems.\n\nTo get a more precise and efficient implementation in floating point, we need to first understand the floating point representation.\n\n## Separating the integer and decimal part\n\nLet's say we want to implement an `exp-2` approximation for a single-precision (32-bit) floating point system. According to [IEEE-754 floating point representation](https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/), it consists of 1 sign bit, 8 exponent bits, and 32 mantissa (or fractional) bits, as depicted in the diagram:\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ieee_fp.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: IEEE-754 single-precision floating point format.</figcaption>\n</figure>\n\nThe corresponding formula of single-precision floating point is \\\\((−1)^{S} × 1.M × 2^{(E − 127)}\\\\). From this formula, we can observe that: **given an integer input, calculating `exp2` is essentially bit-shifting to get the exponent bits \\\\(E\\\\)**. We also need to add the bias value in the exponent bits before bit-shifting. For single-precision, the bias value is 127 or 0x7f, as shown in the formula above.\n\nThis gives us an idea of how we can tackle the approximation separately, given an input \\\\(x\\\\):\n- for the integer part \\\\(\\lfloor x \\rfloor \\\\), bit-shift to the exponent bits;\n- for the decimal part  \\\\(x - \\lfloor x \\rfloor \\\\), use a rational approximation;\n- multiply the output of both parts \\\\(2^{x} = 2^{\\lfloor x \\rfloor} \\cdot 2^{x - \\lfloor x \\rfloor}\\\\) (in C++, we can use `ldexp`)\n\n## Rational approximation of `exp2f`\n\nDepending on the [rounding mode](https://en.wikipedia.org/wiki/Rounding) used to extract the integer part, the range of the decimal part would either be within \\\\([-0.5, 0.5]\\\\) or \\\\([0, 1)\\\\). With this, we only need an approximation precise enough within this range, which is more achievable.\n\nThere are a myriad of ideas on how this approximation could be achieved. We can start from an n-th order polynomial approximation. For example, with the help of `np.polyfit` we can get a 3rd-order polynomial approximation:\n\n$$ 2^{x} \\approx 0.05700169x^{3}\\ + 0.24858144x^{2} + 0.69282515x + 0.9991608, \\quad x \\in [-1, 1]$$\n\nThis is actually quite close to the Taylor's expansion at order 3:\n\n$$ 2^{x} \\approx \\frac{(x \\ln 2)^3}{3!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{x \\ln 2}{1!} + 1 $$\n\n$$ \\quad \\quad \\quad \\quad \\quad = 0.0555041x^{3}\\ + 0.2402265x^{2} + 0.693147x + 1 $$\n\nThe [Cephes library](https://github.com/nearform/node-cephes/blob/master/cephes/exp2.c) uses a [Padé approximant](https://en.wikipedia.org/wiki/Pad%C3%A9_approximant) in the form of:\n\n$$ 2^{x} \\approx 1 +  2x \\frac{P(x^2)}{Q(x^2) - xP(x^2)}, \\quad x \\in [-0.5, 0.5]$$\n\n$$ P(x) = 0.002309x^{2}+20.202x+1513.906 $$\n\n$$ Q(x) = x^{2}+233.184x+4368.211 $$\n\nFrom [a blog post by Paul Mineiro](http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html), it seems like the author also uses something similar to Padé approximant, but with a lower polynomial order:\n\n$$ 2^{x} \\approx 1 + \\frac{27.7280233}{4.84252568 - x} − 0.49012907x − 5.7259425, \\quad x \\in [0, 1)$$\n\n## Timing and Accuracy\n\nWe report the absolute error of each approximation method within a given input range. [Test script here](https://gist.github.com/gudgud96/ec369cd017b10fb1376300fa325f9321).\n\nWithin input range of \\\\([0, 1)\\\\), 10000 sample points:\n\n|                      |                 max                    |                   min                   |                   avg                  |\n|----------------------|----------------------------------------|-----------------------------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 2.423 \\times 10^{-3} \\quad\\\\) | \\\\(\\quad 1.192 \\times 10^{-7} \\quad\\\\)  | \\\\(\\quad 6.736 \\times 10^{-4} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 5.829 \\times 10^{-5} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 2.267 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 2.384 \\times 10^{-7} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 2.501 \\times 10^{-8} \\quad\\\\) |\n\nWithin input range of \\\\([-0.5, 0.5]\\\\), 10000 sample points:\n\n|                      |                 max                    |                   min                   |                   avg                  |\n|----------------------|----------------------------------------|-----------------------------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 8.423 \\times 10^{-4} \\quad\\\\) | \\\\(\\quad 5.960 \\times 10^{-8} \\quad\\\\)  | \\\\(\\quad 4.764 \\times 10^{-4} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 4.995 \\times 10^{-5} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 1.623 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 1.192 \\times 10^{-7} \\quad\\\\) | \\\\(\\quad 0 \\quad\\\\)                     | \\\\(\\quad 1.798 \\times 10^{-8} \\quad\\\\) |\n\n\nWe also measure the total time taken to run on 10000 sample points, averaged across 5 runs:\n\n|                      |                 in secs                |\n|----------------------|----------------------------------------|\n| 3rd-order polynomial | \\\\(\\quad 4.747 \\times 10^{-5} \\quad\\\\) |\n| Mineiro's method     | \\\\(\\quad 8.229 \\times 10^{-5} \\quad\\\\) |\n| Cephes' method       | \\\\(\\quad 4.854 \\times 10^{-4} \\quad\\\\) |\n\nWe can see Cephes provides the best accuracy, while 3rd-order polynomial approximation provides the best speed. Mineiro's method keeps the absolute error within the order of magnitude \\\\(10^{-5}\\\\), while using only ~20% of the time needed by Cephes.\n\n\n## Code example in SIMD\n\nSIMD is commonly used to provide further computation speedup on CPU. The aim of of this post is also to find an efficient SIMD implementation for `exp2`, which is still lacking in common SIMD operation sets. Below we will look at an example of `exp2` approximation implemented using SSE3. We use the 3rd-order polynomial approximation below:\n\n```c++\n__m128 fast_exp_sse (__m128 x)  {\n    __m128 x_int_f, x_frac, xx;\n    __m128i x_int;\n\n    __m128 c0  = _mm_set1_ps (0.05700169f);\n    __m128 c1  = _mm_set1_ps (0.24858144f);\n    __m128 c2  = _mm_set1_ps (0.69282515f);\n    __m128 c3  = _mm_set1_ps (0.99916080f);\n\n    // obtain the integer and fractional part\n    x_int = _mm_cvtps_epi32(x);\n    x_int_f = _mm_cvtepi32_ps(x_int);\n    x_frac = _mm_sub_ps(x, x_int_f);\n\n    // perform 3rd-order polynomial approximation on fractional part\n    xx = _mm_mul_ps(x_frac, c0);\n    xx = _mm_add_ps(xx, c1);\n    xx = _mm_mul_ps(x_frac, xx);\n    xx = _mm_add_ps(xx, c2);\n    xx = _mm_mul_ps(x_frac, xx);\n    xx = _mm_add_ps(xx, c3);\n\n    // compute 2^n for integer part through bit-shifting and adding to exponent field\n    x_int = _mm_add_epi32(x_int, _mm_set1_epi32(0x7f));\n    x_int = _mm_slli_epi32(x_int, 23);\n    x_int_f = _mm_castsi128_ps(x_int);\n\n    // compute final result, 2^n = (2^i)(2^f)\n    xx = _mm_mul_ps(xx, x_int_f);\n\n    return xx\n}\n```\n\nSome notes to discuss:\n\n- For the integer rounding part, `_mm_cvtps_epi32` is used, which is a float-to-int casting. To use round-to-nearest mode, we can use `_mm_round_ps`, but it is only supported in SSE4.1.\n\n- There is a difference between **type conversion** `_mm_cvtps_epi32` and **reinterpret casting** `_mm_castsi128_ps`. Type conversion converts a fixed point integer representation to a floating point representation, and retain its value. Reinterpret casting takes the byte pattern of the fixed-point input, and reinterprets it based on the floating point representation.\n\n- Padé approximant can be used by replacing lines 16-21, and would require the division operator `_mm_div_ps`.\n\n## References\n\n1. [Creating a Compiler Optimized Inlineable Implementation of Intel Svml Simd Intrinsics](http://ijeais.org/wp-content/uploads/2018/07/IJAER180702.pdf)\n\n2. [Added vectorized implementation of the exponential function for ARM/NEON](http://dalab.se.sjtu.edu.cn/gitlab/xiaoyuwei/eigen/-/commit/cc5d7ff5238da45ef7416ec94f18227486ed9643)\n\n3. [Fastest Implementation of the Natural Exponential Function Using SSE](https://stackoverflow.com/questions/47025373/fastest-implementation-of-the-natural-exponential-function-using-sse)\n\n4. [exp-2 in torch-cephes library](https://github.com/google-deepmind/torch-cephes/blob/master/cephes/cmath/exp2.c)\n\n5. [Fast Approximate Logarithm, Exponential, Power, and Inverse Root](http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html)\n\n6. [fastapprox](https://github.com/etheory/fastapprox)\n\n7. [Where does this approximation for 2^{x} − 1 come from?](https://math.stackexchange.com/questions/4581468/where-does-this-approximation-for-2x-1-come-from)","slug":"exp-2","published":1,"updated":"2025-06-27T10:09:01.552Z","_id":"cmbae7c0w0001809kc8qrbb98","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>The power function \\(x^y\\) is integral to many DSP applications, such as dB to linear gain conversion (\\(y = 10^\\frac{x}{20}\\)), and semitone to Hz conversion (\\(f_t = f_0 \\cdot 2^{\\frac{t}{12}}\\)). When studying the code in <a href=\"https://github.com/asb2m10/dexed\" target=\"_blank\" rel=\"noopener\">Dexed</a>, an FM synth modelled over DX7, I find many use cases of the <code>exp2</code> function (\\(2^x\\)), especially in the amplitude envelope calculation. </p>\n<p>In this post, we will look at how \\(2^x\\), or the <code>exp2</code> function, can be approximated for speed-intensive, precision-tolerant use cases. Note that we only discuss the case of <code>exp2</code>, because it is a convenient base in floating point representation (more on this later), and it is easily extendable to the generic power function \\(x^y\\). Given \\(f(k) = 2^k\\), we can transform the power function by multiplying a constant \\(\\log_{2}{x}\\) on the input to make use of \\(f(\\cdot)\\):</p>\n<p>$$x^y = 2^{y \\cdot \\log_{2}{x}} = f(y \\cdot \\log_{2}{x})$$</p>\n<h2 id=\"Initial-ideas\"><a href=\"#Initial-ideas\" class=\"headerlink\" title=\"Initial ideas\"></a>Initial ideas</h2><p>A straightforward approach is to truncate the <strong>Taylor series</strong> of \\(2^x\\) up to the \\(n\\)-th term. One can get the Taylor series of \\(2^x\\) as:</p>\n<p>$$2^x = e^{x \\ln 2} = 1 + \\frac{x \\ln 2}{1!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{(x \\ln 2)^3}{3!} + … $$</p>\n<p>However, to get a good approximation across a wide input range, it requires higher order of polynomials, which is computationally intensive. </p>\n<p>Another idea from Dexed is to <a href=\"https://github.com/asb2m10/dexed/blob/master/Source/msfa/exp2.h\" target=\"_blank\" rel=\"noopener\">use a finite-range lookup table and fixed-point arithmetic</a>, however this method is usable only for fixed-point systems.</p>\n<p>To get a more precise and efficient implementation in floating point, we need to first understand the floating point representation.</p>\n<h2 id=\"Separating-the-integer-and-decimal-part\"><a href=\"#Separating-the-integer-and-decimal-part\" class=\"headerlink\" title=\"Separating the integer and decimal part\"></a>Separating the integer and decimal part</h2><p>Let’s say we want to implement an <code>exp-2</code> approximation for a single-precision (32-bit) floating point system. According to <a href=\"https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/\" target=\"_blank\" rel=\"noopener\">IEEE-754 floating point representation</a>, it consists of 1 sign bit, 8 exponent bits, and 32 mantissa (or fractional) bits, as depicted in the diagram:</p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ieee_fp.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: IEEE-754 single-precision floating point format.</figcaption>\n</figure>\n\n<p>The corresponding formula of single-precision floating point is \\((−1)^{S} × 1.M × 2^{(E − 127)}\\). From this formula, we can observe that: <strong>given an integer input, calculating <code>exp2</code> is essentially bit-shifting to get the exponent bits \\(E\\)</strong>. We also need to add the bias value in the exponent bits before bit-shifting. For single-precision, the bias value is 127 or 0x7f, as shown in the formula above.</p>\n<p>This gives us an idea of how we can tackle the approximation separately, given an input \\(x\\):</p>\n<ul>\n<li>for the integer part \\(\\lfloor x \\rfloor \\), bit-shift to the exponent bits;</li>\n<li>for the decimal part  \\(x - \\lfloor x \\rfloor \\), use a rational approximation;</li>\n<li>multiply the output of both parts \\(2^{x} = 2^{\\lfloor x \\rfloor} \\cdot 2^{x - \\lfloor x \\rfloor}\\) (in C++, we can use <code>ldexp</code>)</li>\n</ul>\n<h2 id=\"Rational-approximation-of-exp2f\"><a href=\"#Rational-approximation-of-exp2f\" class=\"headerlink\" title=\"Rational approximation of exp2f\"></a>Rational approximation of <code>exp2f</code></h2><p>Depending on the <a href=\"https://en.wikipedia.org/wiki/Rounding\" target=\"_blank\" rel=\"noopener\">rounding mode</a> used to extract the integer part, the range of the decimal part would either be within \\([-0.5, 0.5]\\) or \\([0, 1)\\). With this, we only need an approximation precise enough within this range, which is more achievable.</p>\n<p>There are a myriad of ideas on how this approximation could be achieved. We can start from an n-th order polynomial approximation. For example, with the help of <code>np.polyfit</code> we can get a 3rd-order polynomial approximation:</p>\n<p>$$ 2^{x} \\approx 0.05700169x^{3}\\ + 0.24858144x^{2} + 0.69282515x + 0.9991608, \\quad x \\in [-1, 1]$$</p>\n<p>This is actually quite close to the Taylor’s expansion at order 3:</p>\n<p>$$ 2^{x} \\approx \\frac{(x \\ln 2)^3}{3!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{x \\ln 2}{1!} + 1 $$</p>\n<p>$$ \\quad \\quad \\quad \\quad \\quad = 0.0555041x^{3}\\ + 0.2402265x^{2} + 0.693147x + 1 $$</p>\n<p>The <a href=\"https://github.com/nearform/node-cephes/blob/master/cephes/exp2.c\" target=\"_blank\" rel=\"noopener\">Cephes library</a> uses a <a href=\"https://en.wikipedia.org/wiki/Pad%C3%A9_approximant\" target=\"_blank\" rel=\"noopener\">Padé approximant</a> in the form of:</p>\n<p>$$ 2^{x} \\approx 1 +  2x \\frac{P(x^2)}{Q(x^2) - xP(x^2)}, \\quad x \\in [-0.5, 0.5]$$</p>\n<p>$$ P(x) = 0.002309x^{2}+20.202x+1513.906 $$</p>\n<p>$$ Q(x) = x^{2}+233.184x+4368.211 $$</p>\n<p>From <a href=\"http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html\" target=\"_blank\" rel=\"noopener\">a blog post by Paul Mineiro</a>, it seems like the author also uses something similar to Padé approximant, but with a lower polynomial order:</p>\n<p>$$ 2^{x} \\approx 1 + \\frac{27.7280233}{4.84252568 - x} − 0.49012907x − 5.7259425, \\quad x \\in [0, 1)$$</p>\n<h2 id=\"Timing-and-Accuracy\"><a href=\"#Timing-and-Accuracy\" class=\"headerlink\" title=\"Timing and Accuracy\"></a>Timing and Accuracy</h2><p>We report the absolute error of each approximation method within a given input range. <a href=\"https://gist.github.com/gudgud96/ec369cd017b10fb1376300fa325f9321\" target=\"_blank\" rel=\"noopener\">Test script here</a>.</p>\n<p>Within input range of \\([0, 1)\\), 10000 sample points:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>max</th>\n<th>min</th>\n<th>avg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 2.423 \\times 10^{-3} \\quad\\)</td>\n<td>\\(\\quad 1.192 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 6.736 \\times 10^{-4} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 5.829 \\times 10^{-5} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 2.267 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 2.384 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 2.501 \\times 10^{-8} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>Within input range of \\([-0.5, 0.5]\\), 10000 sample points:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>max</th>\n<th>min</th>\n<th>avg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 8.423 \\times 10^{-4} \\quad\\)</td>\n<td>\\(\\quad 5.960 \\times 10^{-8} \\quad\\)</td>\n<td>\\(\\quad 4.764 \\times 10^{-4} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 4.995 \\times 10^{-5} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 1.623 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 1.192 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 1.798 \\times 10^{-8} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>We also measure the total time taken to run on 10000 sample points, averaged across 5 runs:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>in secs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 4.747 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 8.229 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 4.854 \\times 10^{-4} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>We can see Cephes provides the best accuracy, while 3rd-order polynomial approximation provides the best speed. Mineiro’s method keeps the absolute error within the order of magnitude \\(10^{-5}\\), while using only ~20% of the time needed by Cephes.</p>\n<h2 id=\"Code-example-in-SIMD\"><a href=\"#Code-example-in-SIMD\" class=\"headerlink\" title=\"Code example in SIMD\"></a>Code example in SIMD</h2><p>SIMD is commonly used to provide further computation speedup on CPU. The aim of of this post is also to find an efficient SIMD implementation for <code>exp2</code>, which is still lacking in common SIMD operation sets. Below we will look at an example of <code>exp2</code> approximation implemented using SSE3. We use the 3rd-order polynomial approximation below:</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">__m128 <span class=\"title\">fast_exp_sse</span> <span class=\"params\">(__m128 x)</span>  </span>&#123;</span><br><span class=\"line\">    __m128 x_int_f, x_frac, xx;</span><br><span class=\"line\">    __m128i x_int;</span><br><span class=\"line\"></span><br><span class=\"line\">    __m128 c0  = _mm_set1_ps (<span class=\"number\">0.05700169f</span>);</span><br><span class=\"line\">    __m128 c1  = _mm_set1_ps (<span class=\"number\">0.24858144f</span>);</span><br><span class=\"line\">    __m128 c2  = _mm_set1_ps (<span class=\"number\">0.69282515f</span>);</span><br><span class=\"line\">    __m128 c3  = _mm_set1_ps (<span class=\"number\">0.99916080f</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// obtain the integer and fractional part</span></span><br><span class=\"line\">    x_int = _mm_cvtps_epi32(x);</span><br><span class=\"line\">    x_int_f = _mm_cvtepi32_ps(x_int);</span><br><span class=\"line\">    x_frac = _mm_sub_ps(x, x_int_f);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// perform 3rd-order polynomial approximation on fractional part</span></span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, c0);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c1);</span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, xx);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c2);</span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, xx);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c3);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// compute 2^n for integer part through bit-shifting and adding to exponent field</span></span><br><span class=\"line\">    x_int = _mm_add_epi32(x_int, _mm_set1_epi32(<span class=\"number\">0x7f</span>));</span><br><span class=\"line\">    x_int = _mm_slli_epi32(x_int, <span class=\"number\">23</span>);</span><br><span class=\"line\">    x_int_f = _mm_castsi128_ps(x_int);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// compute final result, 2^n = (2^i)(2^f)</span></span><br><span class=\"line\">    xx = _mm_mul_ps(xx, x_int_f);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> xx</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Some notes to discuss:</p>\n<ul>\n<li><p>For the integer rounding part, <code>_mm_cvtps_epi32</code> is used, which is a float-to-int casting. To use round-to-nearest mode, we can use <code>_mm_round_ps</code>, but it is only supported in SSE4.1.</p>\n</li>\n<li><p>There is a difference between <strong>type conversion</strong> <code>_mm_cvtps_epi32</code> and <strong>reinterpret casting</strong> <code>_mm_castsi128_ps</code>. Type conversion converts a fixed point integer representation to a floating point representation, and retain its value. Reinterpret casting takes the byte pattern of the fixed-point input, and reinterprets it based on the floating point representation.</p>\n</li>\n<li><p>Padé approximant can be used by replacing lines 16-21, and would require the division operator <code>_mm_div_ps</code>.</p>\n</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ol>\n<li><p><a href=\"http://ijeais.org/wp-content/uploads/2018/07/IJAER180702.pdf\" target=\"_blank\" rel=\"noopener\">Creating a Compiler Optimized Inlineable Implementation of Intel Svml Simd Intrinsics</a></p>\n</li>\n<li><p><a href=\"http://dalab.se.sjtu.edu.cn/gitlab/xiaoyuwei/eigen/-/commit/cc5d7ff5238da45ef7416ec94f18227486ed9643\" target=\"_blank\" rel=\"noopener\">Added vectorized implementation of the exponential function for ARM/NEON</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/questions/47025373/fastest-implementation-of-the-natural-exponential-function-using-sse\" target=\"_blank\" rel=\"noopener\">Fastest Implementation of the Natural Exponential Function Using SSE</a></p>\n</li>\n<li><p><a href=\"https://github.com/google-deepmind/torch-cephes/blob/master/cephes/cmath/exp2.c\" target=\"_blank\" rel=\"noopener\">exp-2 in torch-cephes library</a></p>\n</li>\n<li><p><a href=\"http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html\" target=\"_blank\" rel=\"noopener\">Fast Approximate Logarithm, Exponential, Power, and Inverse Root</a></p>\n</li>\n<li><p><a href=\"https://github.com/etheory/fastapprox\" target=\"_blank\" rel=\"noopener\">fastapprox</a></p>\n</li>\n<li><p><a href=\"https://math.stackexchange.com/questions/4581468/where-does-this-approximation-for-2x-1-come-from\" target=\"_blank\" rel=\"noopener\">Where does this approximation for 2^{x} − 1 come from?</a></p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>The power function \\(x^y\\) is integral to many DSP applications, such as dB to linear gain conversion (\\(y = 10^\\frac{x}{20}\\)), and semitone to Hz conversion (\\(f_t = f_0 \\cdot 2^{\\frac{t}{12}}\\)). When studying the code in <a href=\"https://github.com/asb2m10/dexed\" target=\"_blank\" rel=\"noopener\">Dexed</a>, an FM synth modelled over DX7, I find many use cases of the <code>exp2</code> function (\\(2^x\\)), especially in the amplitude envelope calculation. </p>\n<p>In this post, we will look at how \\(2^x\\), or the <code>exp2</code> function, can be approximated for speed-intensive, precision-tolerant use cases. Note that we only discuss the case of <code>exp2</code>, because it is a convenient base in floating point representation (more on this later), and it is easily extendable to the generic power function \\(x^y\\). Given \\(f(k) = 2^k\\), we can transform the power function by multiplying a constant \\(\\log_{2}{x}\\) on the input to make use of \\(f(\\cdot)\\):</p>\n<p>$$x^y = 2^{y \\cdot \\log_{2}{x}} = f(y \\cdot \\log_{2}{x})$$</p>\n<h2 id=\"Initial-ideas\"><a href=\"#Initial-ideas\" class=\"headerlink\" title=\"Initial ideas\"></a>Initial ideas</h2><p>A straightforward approach is to truncate the <strong>Taylor series</strong> of \\(2^x\\) up to the \\(n\\)-th term. One can get the Taylor series of \\(2^x\\) as:</p>\n<p>$$2^x = e^{x \\ln 2} = 1 + \\frac{x \\ln 2}{1!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{(x \\ln 2)^3}{3!} + … $$</p>\n<p>However, to get a good approximation across a wide input range, it requires higher order of polynomials, which is computationally intensive. </p>\n<p>Another idea from Dexed is to <a href=\"https://github.com/asb2m10/dexed/blob/master/Source/msfa/exp2.h\" target=\"_blank\" rel=\"noopener\">use a finite-range lookup table and fixed-point arithmetic</a>, however this method is usable only for fixed-point systems.</p>\n<p>To get a more precise and efficient implementation in floating point, we need to first understand the floating point representation.</p>\n<h2 id=\"Separating-the-integer-and-decimal-part\"><a href=\"#Separating-the-integer-and-decimal-part\" class=\"headerlink\" title=\"Separating the integer and decimal part\"></a>Separating the integer and decimal part</h2><p>Let’s say we want to implement an <code>exp-2</code> approximation for a single-precision (32-bit) floating point system. According to <a href=\"https://www.geeksforgeeks.org/ieee-standard-754-floating-point-numbers/\" target=\"_blank\" rel=\"noopener\">IEEE-754 floating point representation</a>, it consists of 1 sign bit, 8 exponent bits, and 32 mantissa (or fractional) bits, as depicted in the diagram:</p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ieee_fp.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: IEEE-754 single-precision floating point format.</figcaption>\n</figure>\n\n<p>The corresponding formula of single-precision floating point is \\((−1)^{S} × 1.M × 2^{(E − 127)}\\). From this formula, we can observe that: <strong>given an integer input, calculating <code>exp2</code> is essentially bit-shifting to get the exponent bits \\(E\\)</strong>. We also need to add the bias value in the exponent bits before bit-shifting. For single-precision, the bias value is 127 or 0x7f, as shown in the formula above.</p>\n<p>This gives us an idea of how we can tackle the approximation separately, given an input \\(x\\):</p>\n<ul>\n<li>for the integer part \\(\\lfloor x \\rfloor \\), bit-shift to the exponent bits;</li>\n<li>for the decimal part  \\(x - \\lfloor x \\rfloor \\), use a rational approximation;</li>\n<li>multiply the output of both parts \\(2^{x} = 2^{\\lfloor x \\rfloor} \\cdot 2^{x - \\lfloor x \\rfloor}\\) (in C++, we can use <code>ldexp</code>)</li>\n</ul>\n<h2 id=\"Rational-approximation-of-exp2f\"><a href=\"#Rational-approximation-of-exp2f\" class=\"headerlink\" title=\"Rational approximation of exp2f\"></a>Rational approximation of <code>exp2f</code></h2><p>Depending on the <a href=\"https://en.wikipedia.org/wiki/Rounding\" target=\"_blank\" rel=\"noopener\">rounding mode</a> used to extract the integer part, the range of the decimal part would either be within \\([-0.5, 0.5]\\) or \\([0, 1)\\). With this, we only need an approximation precise enough within this range, which is more achievable.</p>\n<p>There are a myriad of ideas on how this approximation could be achieved. We can start from an n-th order polynomial approximation. For example, with the help of <code>np.polyfit</code> we can get a 3rd-order polynomial approximation:</p>\n<p>$$ 2^{x} \\approx 0.05700169x^{3}\\ + 0.24858144x^{2} + 0.69282515x + 0.9991608, \\quad x \\in [-1, 1]$$</p>\n<p>This is actually quite close to the Taylor’s expansion at order 3:</p>\n<p>$$ 2^{x} \\approx \\frac{(x \\ln 2)^3}{3!} + \\frac{(x \\ln 2)^2}{2!} + \\frac{x \\ln 2}{1!} + 1 $$</p>\n<p>$$ \\quad \\quad \\quad \\quad \\quad = 0.0555041x^{3}\\ + 0.2402265x^{2} + 0.693147x + 1 $$</p>\n<p>The <a href=\"https://github.com/nearform/node-cephes/blob/master/cephes/exp2.c\" target=\"_blank\" rel=\"noopener\">Cephes library</a> uses a <a href=\"https://en.wikipedia.org/wiki/Pad%C3%A9_approximant\" target=\"_blank\" rel=\"noopener\">Padé approximant</a> in the form of:</p>\n<p>$$ 2^{x} \\approx 1 +  2x \\frac{P(x^2)}{Q(x^2) - xP(x^2)}, \\quad x \\in [-0.5, 0.5]$$</p>\n<p>$$ P(x) = 0.002309x^{2}+20.202x+1513.906 $$</p>\n<p>$$ Q(x) = x^{2}+233.184x+4368.211 $$</p>\n<p>From <a href=\"http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html\" target=\"_blank\" rel=\"noopener\">a blog post by Paul Mineiro</a>, it seems like the author also uses something similar to Padé approximant, but with a lower polynomial order:</p>\n<p>$$ 2^{x} \\approx 1 + \\frac{27.7280233}{4.84252568 - x} − 0.49012907x − 5.7259425, \\quad x \\in [0, 1)$$</p>\n<h2 id=\"Timing-and-Accuracy\"><a href=\"#Timing-and-Accuracy\" class=\"headerlink\" title=\"Timing and Accuracy\"></a>Timing and Accuracy</h2><p>We report the absolute error of each approximation method within a given input range. <a href=\"https://gist.github.com/gudgud96/ec369cd017b10fb1376300fa325f9321\" target=\"_blank\" rel=\"noopener\">Test script here</a>.</p>\n<p>Within input range of \\([0, 1)\\), 10000 sample points:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>max</th>\n<th>min</th>\n<th>avg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 2.423 \\times 10^{-3} \\quad\\)</td>\n<td>\\(\\quad 1.192 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 6.736 \\times 10^{-4} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 5.829 \\times 10^{-5} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 2.267 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 2.384 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 2.501 \\times 10^{-8} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>Within input range of \\([-0.5, 0.5]\\), 10000 sample points:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>max</th>\n<th>min</th>\n<th>avg</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 8.423 \\times 10^{-4} \\quad\\)</td>\n<td>\\(\\quad 5.960 \\times 10^{-8} \\quad\\)</td>\n<td>\\(\\quad 4.764 \\times 10^{-4} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 4.995 \\times 10^{-5} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 1.623 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 1.192 \\times 10^{-7} \\quad\\)</td>\n<td>\\(\\quad 0 \\quad\\)</td>\n<td>\\(\\quad 1.798 \\times 10^{-8} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>We also measure the total time taken to run on 10000 sample points, averaged across 5 runs:</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>in secs</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>3rd-order polynomial</td>\n<td>\\(\\quad 4.747 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Mineiro’s method</td>\n<td>\\(\\quad 8.229 \\times 10^{-5} \\quad\\)</td>\n</tr>\n<tr>\n<td>Cephes’ method</td>\n<td>\\(\\quad 4.854 \\times 10^{-4} \\quad\\)</td>\n</tr>\n</tbody></table>\n<p>We can see Cephes provides the best accuracy, while 3rd-order polynomial approximation provides the best speed. Mineiro’s method keeps the absolute error within the order of magnitude \\(10^{-5}\\), while using only ~20% of the time needed by Cephes.</p>\n<h2 id=\"Code-example-in-SIMD\"><a href=\"#Code-example-in-SIMD\" class=\"headerlink\" title=\"Code example in SIMD\"></a>Code example in SIMD</h2><p>SIMD is commonly used to provide further computation speedup on CPU. The aim of of this post is also to find an efficient SIMD implementation for <code>exp2</code>, which is still lacking in common SIMD operation sets. Below we will look at an example of <code>exp2</code> approximation implemented using SSE3. We use the 3rd-order polynomial approximation below:</p>\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">__m128 <span class=\"title\">fast_exp_sse</span> <span class=\"params\">(__m128 x)</span>  </span>&#123;</span><br><span class=\"line\">    __m128 x_int_f, x_frac, xx;</span><br><span class=\"line\">    __m128i x_int;</span><br><span class=\"line\"></span><br><span class=\"line\">    __m128 c0  = _mm_set1_ps (<span class=\"number\">0.05700169f</span>);</span><br><span class=\"line\">    __m128 c1  = _mm_set1_ps (<span class=\"number\">0.24858144f</span>);</span><br><span class=\"line\">    __m128 c2  = _mm_set1_ps (<span class=\"number\">0.69282515f</span>);</span><br><span class=\"line\">    __m128 c3  = _mm_set1_ps (<span class=\"number\">0.99916080f</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// obtain the integer and fractional part</span></span><br><span class=\"line\">    x_int = _mm_cvtps_epi32(x);</span><br><span class=\"line\">    x_int_f = _mm_cvtepi32_ps(x_int);</span><br><span class=\"line\">    x_frac = _mm_sub_ps(x, x_int_f);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// perform 3rd-order polynomial approximation on fractional part</span></span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, c0);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c1);</span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, xx);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c2);</span><br><span class=\"line\">    xx = _mm_mul_ps(x_frac, xx);</span><br><span class=\"line\">    xx = _mm_add_ps(xx, c3);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// compute 2^n for integer part through bit-shifting and adding to exponent field</span></span><br><span class=\"line\">    x_int = _mm_add_epi32(x_int, _mm_set1_epi32(<span class=\"number\">0x7f</span>));</span><br><span class=\"line\">    x_int = _mm_slli_epi32(x_int, <span class=\"number\">23</span>);</span><br><span class=\"line\">    x_int_f = _mm_castsi128_ps(x_int);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// compute final result, 2^n = (2^i)(2^f)</span></span><br><span class=\"line\">    xx = _mm_mul_ps(xx, x_int_f);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> xx</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Some notes to discuss:</p>\n<ul>\n<li><p>For the integer rounding part, <code>_mm_cvtps_epi32</code> is used, which is a float-to-int casting. To use round-to-nearest mode, we can use <code>_mm_round_ps</code>, but it is only supported in SSE4.1.</p>\n</li>\n<li><p>There is a difference between <strong>type conversion</strong> <code>_mm_cvtps_epi32</code> and <strong>reinterpret casting</strong> <code>_mm_castsi128_ps</code>. Type conversion converts a fixed point integer representation to a floating point representation, and retain its value. Reinterpret casting takes the byte pattern of the fixed-point input, and reinterprets it based on the floating point representation.</p>\n</li>\n<li><p>Padé approximant can be used by replacing lines 16-21, and would require the division operator <code>_mm_div_ps</code>.</p>\n</li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ol>\n<li><p><a href=\"http://ijeais.org/wp-content/uploads/2018/07/IJAER180702.pdf\" target=\"_blank\" rel=\"noopener\">Creating a Compiler Optimized Inlineable Implementation of Intel Svml Simd Intrinsics</a></p>\n</li>\n<li><p><a href=\"http://dalab.se.sjtu.edu.cn/gitlab/xiaoyuwei/eigen/-/commit/cc5d7ff5238da45ef7416ec94f18227486ed9643\" target=\"_blank\" rel=\"noopener\">Added vectorized implementation of the exponential function for ARM/NEON</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/questions/47025373/fastest-implementation-of-the-natural-exponential-function-using-sse\" target=\"_blank\" rel=\"noopener\">Fastest Implementation of the Natural Exponential Function Using SSE</a></p>\n</li>\n<li><p><a href=\"https://github.com/google-deepmind/torch-cephes/blob/master/cephes/cmath/exp2.c\" target=\"_blank\" rel=\"noopener\">exp-2 in torch-cephes library</a></p>\n</li>\n<li><p><a href=\"http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html\" target=\"_blank\" rel=\"noopener\">Fast Approximate Logarithm, Exponential, Power, and Inverse Root</a></p>\n</li>\n<li><p><a href=\"https://github.com/etheory/fastapprox\" target=\"_blank\" rel=\"noopener\">fastapprox</a></p>\n</li>\n<li><p><a href=\"https://math.stackexchange.com/questions/4581468/where-does-this-approximation-for-2x-1-come-from\" target=\"_blank\" rel=\"noopener\">Where does this approximation for 2^{x} − 1 come from?</a></p>\n</li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ck5s3jzfj00004qv5g9865rpz","tag_id":"ck5s3yyg300064qv5evr99y2v","_id":"ck5s3yyg400074qv5e8ls7ygf"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0iy0001tbm86k7r8jw0","_id":"ck89oi0j10003tbm8gm9e58j0"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0j00002tbm8b5htefr6","_id":"ck89oi0j10004tbm8ajru04wh"},{"post_id":"ck8jt5xfe0000jbm81um92vdu","tag_id":"ck8jt5xfj0001jbm8dllq10kz","_id":"ck8jt5xfl0002jbm8e18p3kpx"},{"post_id":"ck8jt5xfe0000jbm81um92vdu","tag_id":"ck89oi0j00002tbm8b5htefr6","_id":"ck8jt5xfl0003jbm87hvxh4b4"},{"post_id":"ckbpvdoks0000qlm88ndz3z7a","tag_id":"ck89oi0iy0001tbm86k7r8jw0","_id":"ckbpvdol00001qlm8ek7a0zws"},{"post_id":"ckgcjd4cq0000w19khjzs6q1z","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"ckgcjd4d10003w19kc0mv76d4"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ckgcjd4dd0004w19k4yhyhcc8","_id":"ckgcjd4df0006w19kgrnrbwl8"},{"post_id":"ckbpvdoks0000qlm88ndz3z7a","tag_id":"ckgcjd4dd0004w19k4yhyhcc8","_id":"ckgcjd4df0007w19khky7a6je"},{"post_id":"ckgcjeans0008w19k9uev444u","tag_id":"ckgcjf4oj000aw19k1ebe5cjw","_id":"ckgcjf4oj000bw19k784r7fzv"},{"post_id":"ckhwutnte00002f9kgkfwhopl","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"ckhwuu4ct00012f9kdfgc58kn"},{"post_id":"ckhwutnte00002f9kgkfwhopl","tag_id":"ckhwuuf6000022f9kaxjk187i","_id":"ckhwuuf6100032f9k2blm007o"},{"post_id":"cklqf68yn0000y59k0kv6fs2z","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"cklsa7ayi0004y59k512sdj7j"},{"post_id":"cklqf68yn0000y59k0kv6fs2z","tag_id":"cklqf68yw0001y59k3pu4dzx4","_id":"cklsa7ayq0005y59kfkdndop0"},{"post_id":"ckzu0m6n80000qg9khdg3busn","tag_id":"ckgcjf4oj000aw19k1ebe5cjw","_id":"ckzu0rz2k0000v49k1uxn0uqd"},{"post_id":"clgs5b6k30000qyc91zdc0k6b","tag_id":"ck5s3yyg300064qv5evr99y2v","_id":"clgs5b6ka0001qyc97nrzdmnj"},{"post_id":"cmbae7c0w0001809kc8qrbb98","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"cmbae7c0y0004809kak2zdwcn"},{"post_id":"cmbae7c0s0000809kgkv13omt","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"cmcelk24r00004h9ked4ke3un"},{"post_id":"cmbae7c0s0000809kgkv13omt","tag_id":"ckhwuuf6000022f9kaxjk187i","_id":"cmcelk24u00014h9k7zvk7d75"}],"Tag":[{"name":"centOS","_id":"ck5rzh79g00018dv5hvt3fro0"},{"name":"redis","_id":"ck5rzh79h00028dv5bcno9ru7"},{"name":"test1","_id":"ck5rzhckr00058dv5e1cj86cx"},{"name":"test2","_id":"ck5rzhcks00068dv55wag4oae"},{"name":"General Thoughts","_id":"ck5s3yyg300064qv5evr99y2v"},{"name":"AI Music","_id":"ck5s3z7pb00084qv54k2z6635"},{"name":"VAE","_id":"ck89oi0iy0001tbm86k7r8jw0"},{"name":"Symbolic Music","_id":"ck89oi0j00002tbm8b5htefr6"},{"name":"Transformer","_id":"ck8jt5xfj0001jbm8dllq10kz"},{"name":"Music Signal Processing","_id":"ckgcjd4cy0002w19k82ta7lzp"},{"name":"Music Representation Learning","_id":"ckgcjd4dd0004w19k4yhyhcc8"},{"name":"Music Information Retrieval","_id":"ckgcjf4oj000aw19k1ebe5cjw"},{"name":"Deep Learning","_id":"ckhwuuf6000022f9kaxjk187i"},{"name":"ML in Production","_id":"cklqf68yw0001y59k3pu4dzx4"},{"name":"Music Business","_id":"clefbw7a70001mlc9d04shb6c"}]}}