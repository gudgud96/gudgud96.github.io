{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"source/img/cook.jpg","path":"img/cook.jpg","modified":0,"renderable":0},{"_id":"source/img/dao.png","path":"img/dao.png","modified":0,"renderable":0},{"_id":"source/img/fifa-deep-nn.png","path":"img/fifa-deep-nn.png","modified":0,"renderable":0},{"_id":"source/img/ge14.jpg","path":"img/ge14.jpg","modified":0,"renderable":0},{"_id":"source/img/german.jpg","path":"img/german.jpg","modified":0,"renderable":0},{"_id":"source/img/happiness.jpg","path":"img/happiness.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_1.jpg","path":"img/img_small_1.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_2.jpg","path":"img/img_small_2.jpg","modified":0,"renderable":0},{"_id":"source/img/linear-regressor-knockout.PNG","path":"img/linear-regressor-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/loc.png","path":"img/loc.png","modified":0,"renderable":0},{"_id":"source/img/logo.png","path":"img/logo.png","modified":0,"renderable":0},{"_id":"source/img/model_performance.png","path":"img/model_performance.png","modified":0,"renderable":0},{"_id":"source/img/music-supply.jpg","path":"img/music-supply.jpg","modified":0,"renderable":0},{"_id":"source/img/nn-knockout.PNG","path":"img/nn-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/religion.png","path":"img/religion.png","modified":0,"renderable":0},{"_id":"source/img/slide_1.jpg","path":"img/slide_1.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_2.jpg","path":"img/slide_2.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_3.jpg","path":"img/slide_3.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_4.jpg","path":"img/slide_4.jpg","modified":0,"renderable":0},{"_id":"source/img/trust.jpg","path":"img/trust.jpg","modified":0,"renderable":0},{"_id":"source/img/work-for-you.jpg","path":"img/work-for-you.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-life.jpg","path":"img/ai-life.jpg","modified":0,"renderable":0},{"_id":"source/img/belgium.jpg","path":"img/belgium.jpg","modified":0,"renderable":0},{"_id":"source/img/blockchain.png","path":"img/blockchain.png","modified":0,"renderable":0},{"_id":"source/img/money.jpg","path":"img/money.jpg","modified":0,"renderable":0},{"_id":"source/img/passing-mark.jpg","path":"img/passing-mark.jpg","modified":0,"renderable":0},{"_id":"source/img/prof.png","path":"img/prof.png","modified":0,"renderable":0},{"_id":"source/img/results.png","path":"img/results.png","modified":0,"renderable":0},{"_id":"source/img/science-empire.jpg","path":"img/science-empire.jpg","modified":0,"renderable":0},{"_id":"source/img/ubi.jpeg","path":"img/ubi.jpeg","modified":0,"renderable":0},{"_id":"source/img/unstoppable.jpg","path":"img/unstoppable.jpg","modified":0,"renderable":0},{"_id":"source/img/win-ai.jpg","path":"img/win-ai.jpg","modified":0,"renderable":0},{"_id":"source/img/brazil.png","path":"img/brazil.png","modified":0,"renderable":0},{"_id":"source/img/consumerism.gif","path":"img/consumerism.gif","modified":0,"renderable":0},{"_id":"source/img/licc.jpg","path":"img/licc.jpg","modified":0,"renderable":0},{"_id":"source/img/plants.jpg","path":"img/plants.jpg","modified":0,"renderable":0},{"_id":"source/img/science-dark.jpg","path":"img/science-dark.jpg","modified":0,"renderable":0},{"_id":"source/img/udacity-nb.jpg","path":"img/udacity-nb.jpg","modified":0,"renderable":0},{"_id":"source/img/storytelling.png","path":"img/storytelling.png","modified":0,"renderable":0},{"_id":"source/img/imperialism.jpg","path":"img/imperialism.jpg","modified":0,"renderable":0},{"_id":"source/img/money2.jpg","path":"img/money2.jpg","modified":0,"renderable":0},{"_id":"source/img/humanism.jpeg","path":"img/humanism.jpeg","modified":0,"renderable":0},{"_id":"source/img/too-big-to-fail.jpg","path":"img/too-big-to-fail.jpg","modified":0,"renderable":0},{"_id":"source/img/gossip.jpg","path":"img/gossip.jpg","modified":0,"renderable":0},{"_id":"source/img/profile.jpeg","path":"img/profile.jpeg","modified":0,"renderable":0},{"_id":"source/img/najib-tun-m.jpg","path":"img/najib-tun-m.jpg","modified":0,"renderable":0},{"_id":"source/img/sapiens.jpg","path":"img/sapiens.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-electric.png","path":"img/ai-electric.png","modified":0,"renderable":0},{"_id":"source/img/taryn.png","path":"img/taryn.png","modified":0,"renderable":0},{"_id":"source/about/profile.jpeg","path":"about/profile.jpeg","modified":0,"renderable":0},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css.map","path":"css/aircloud.css.map","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.less","path":"css/aircloud.less","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/js/index.js","path":"js/index.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css","path":"css/aircloud.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.css","path":"css/aircloud.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.css.map","path":"css/aircloud.css.map","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.less","path":"css/aircloud.less","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/index.js","path":"js/index.js","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"source/resume_new.pdf","path":"resume_new.pdf","modified":0,"renderable":0},{"_id":"source/img/extres.png","path":"img/extres.png","modified":0,"renderable":0},{"_id":"source/img/ashis.png","path":"img/ashis.png","modified":0,"renderable":0},{"_id":"source/img/virtuoso.png","path":"img/virtuoso.png","modified":0,"renderable":0},{"_id":"source/img/midivae.png","path":"img/midivae.png","modified":0,"renderable":0},{"_id":"source/img/musicvae.png","path":"img/musicvae.png","modified":0,"renderable":0},{"_id":"source/img/deep-analogy.png","path":"img/deep-analogy.png","modified":0,"renderable":0},{"_id":"source/img/ashis2.png","path":"img/ashis2.png","modified":0,"renderable":0},{"_id":"source/img/relative-attention.png","path":"img/relative-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-attention-2.png","path":"img/relative-attention-2.png","modified":0,"renderable":0},{"_id":"source/img/new-relative-attention.png","path":"img/new-relative-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention.png","path":"img/relative-local-attention.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-2.png","path":"img/relative-local-attention-2.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-srel.png","path":"img/relative-local-attention-srel.png","modified":0,"renderable":0},{"_id":"source/img/relative-local-attention-unmasked.png","path":"img/relative-local-attention-unmasked.png","modified":0,"renderable":0},{"_id":"source/img/music-transformer-results.png","path":"img/music-transformer-results.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl.png","path":"img/kingma-ssl.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl-2.png","path":"img/kingma-ssl-2.png","modified":0,"renderable":0},{"_id":"source/img/kingma-ssl-3.png","path":"img/kingma-ssl-3.png","modified":0,"renderable":0},{"_id":"source/img/vade-ssl.png","path":"img/vade-ssl.png","modified":0,"renderable":0},{"_id":"source/img/radford-sentiment.png","path":"img/radford-sentiment.png","modified":0,"renderable":0},{"_id":"source/Resume_2020.pdf","path":"Resume_2020.pdf","modified":0,"renderable":0},{"_id":"source/img/stft.png","path":"img/stft.png","modified":0,"renderable":0},{"_id":"source/img/istft.png","path":"img/istft.png","modified":0,"renderable":0},{"_id":"source/img/ismir_attr.png","path":"img/ismir_attr.png","modified":0,"renderable":0},{"_id":"source/img/ismir_conn.png","path":"img/ismir_conn.png","modified":0,"renderable":0},{"_id":"source/img/ismir_dmel.png","path":"img/ismir_dmel.png","modified":0,"renderable":0},{"_id":"source/img/ismir_edit.png","path":"img/ismir_edit.png","modified":0,"renderable":0},{"_id":"source/img/ismir_bebop.png","path":"img/ismir_bebop.png","modified":0,"renderable":0},{"_id":"source/img/ismir_pianotree.png","path":"img/ismir_pianotree.png","modified":0,"renderable":0},{"_id":"source/img/ismir_singing.png","path":"img/ismir_singing.png","modified":0,"renderable":0},{"_id":"source/img/ismir_sketchnet.png","path":"img/ismir_sketchnet.png","modified":0,"renderable":0},{"_id":"source/img/ismir_drumgan.png","path":"img/ismir_drumgan.png","modified":0,"renderable":0},{"_id":"source/img/ismir_fadernets.png","path":"img/ismir_fadernets.png","modified":0,"renderable":0},{"_id":"source/img/ismir_interpretable.png","path":"img/ismir_interpretable.png","modified":0,"renderable":0},{"_id":"source/img/ismir_jazz.png","path":"img/ismir_jazz.png","modified":0,"renderable":0},{"_id":"source/img/ismir_jyun.png","path":"img/ismir_jyun.png","modified":0,"renderable":0},{"_id":"source/img/ismir_metric.png","path":"img/ismir_metric.png","modified":0,"renderable":0},{"_id":"source/img/ismir_unets.png","path":"img/ismir_unets.png","modified":0,"renderable":0},{"_id":"source/img/ismir_phoneme1.png","path":"img/ismir_phoneme1.png","modified":0,"renderable":0},{"_id":"source/img/ismir_phoneme2.png","path":"img/ismir_phoneme2.png","modified":0,"renderable":0},{"_id":"source/img/ismir_multitask.png","path":"img/ismir_multitask.png","modified":0,"renderable":0},{"_id":"source/img/ismir_vocal.png","path":"img/ismir_vocal.png","modified":0,"renderable":0},{"_id":"source/img/ismir_furkan.png","path":"img/ismir_furkan.png","modified":0,"renderable":0},{"_id":"source/img/ismir_doras.png","path":"img/ismir_doras.png","modified":0,"renderable":0},{"_id":"source/img/ismir_lottery.png","path":"img/ismir_lottery.png","modified":0,"renderable":0},{"_id":"source/img/ismir_spherical.png","path":"img/ismir_spherical.png","modified":0,"renderable":0},{"_id":"source/img/ismir_transcription.png","path":"img/ismir_transcription.png","modified":0,"renderable":0},{"_id":"source/img/mean-vs-max-pool.png","path":"img/mean-vs-max-pool.png","modified":0,"renderable":0},{"_id":"source/img/csd-ml-ops.png","path":"img/csd-ml-ops.png","modified":1,"renderable":0},{"_id":"source/img/csd-hpcp.png","path":"img/csd-hpcp.png","modified":1,"renderable":0},{"_id":"source/img/csd-hybrid.png","path":"img/csd-hybrid.png","modified":1,"renderable":0},{"_id":"source/img/csd-bytecover.png","path":"img/csd-bytecover.png","modified":1,"renderable":0},{"_id":"source/img/csd-dominant.png","path":"img/csd-dominant.png","modified":1,"renderable":0},{"_id":"source/img/csd-shingles.png","path":"img/csd-shingles.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1579847575470},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1579847575470},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1579847575470},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1579847575470},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1579847575470},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1579847575476},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1579847568417},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1579847575470},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1579847575470},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1579847575470},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1579847575471},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1579847575471},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1579847575471},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1579847575471},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1579847575471},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1579847575471},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1579847575471},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1579847575471},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1579847575472},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1579847575475},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1579847575475},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1579847575475},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1579847575475},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1579847575476},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1579847575476},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1579847575474},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1579847575476},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1579847575479},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1579847575483},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1579847575487},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1579847575487},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1579847575474},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1579847575474},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1579847575480},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1579847575482},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1579847575486},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1579847575482},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1579847575481},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1579847575483},{"_id":"themes/hexo-theme-aircloud/.gitignore","hash":"5a4a925cfd624633dafaacaced416c8d7272dcef","modified":1579853649534},{"_id":"themes/hexo-theme-aircloud/LICENSE","hash":"218b4bf797149a2751a015812a9adefe368185c1","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/_config.yml","hash":"0ad3a6ab2c9bb07fb1e030052622fdcde5c6f28a","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/readme-en.md","hash":"2903b1e9db12cd72ed6f8c10be14cd7f6afd82cf","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/readme.md","hash":"4be1fc64bd1dc335a986a39594564e89bd7eba43","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/languages/en.yml","hash":"93d77f44c0386df2defce0ac465b19e9a85f4d2f","modified":1579887859365},{"_id":"themes/hexo-theme-aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/index.ejs","hash":"09e2407d615be7fe7ac41d11df3b7026e7393080","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/layout.ejs","hash":"7efd113aee90e698e187d0ea1f0b42a1c00d210e","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/post.ejs","hash":"2eb5fc0c2bb801528c3db3b09e6cb4d073e3ad99","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/footer.ejs","hash":"2ab1dc9da5183fc5e74a4bddbf0c29f992057ec9","modified":1579887884627},{"_id":"themes/hexo-theme-aircloud/layout/_partial/head.ejs","hash":"3f18d5d4951a205bab25b08d6bf85b054c84a21b","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/nav.ejs","hash":"f7bb88510ece895a48490c63d33323dc4eff4136","modified":1579887766729},{"_id":"themes/hexo-theme-aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.css","hash":"64914aa6ecd5b948676870e0809e0f220b162e3b","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.less","hash":"8aef4d8cfdefbcd2e28d4985a4f79a5005ca0b6c","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.css","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.less","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.css","hash":"40d7cadf42b130ea1b40de1ae73b2b00e27f476f","modified":1579883019352},{"_id":"themes/hexo-theme-aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.less","hash":"194ac7db2eeee7307fcb7470302f8172100181fb","modified":1579860381468},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.css","hash":"cfe668f5e11de4d20ec6538d480b74a86380de02","modified":1579887938712},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888254394},{"_id":"themes/hexo-theme-aircloud/source/_less/post.css","hash":"4adf531589cb55413264c188b29ae47ab703beb8","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/post.less","hash":"bbbd81c03e7581950d82bf971eda49e8bed7bee1","modified":1579883019353},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css","hash":"e6082557a5f0e546169ab1aa0ba29bda4ef5c182","modified":1579883019355},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1579853649546},{"_id":"themes/hexo-theme-aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579853649547},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579853649547},{"_id":"source/_posts/test-post.md","hash":"623b1b89f04a61ba2905a155459a59ed1dddd8de","modified":1579859460776},{"_id":"source/test-page/index.md","hash":"a80f212d7be56d71b665227a1dcc10ea67d246ef","modified":1579856642376},{"_id":"source/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1579857100564},{"_id":"source/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1579857100564},{"_id":"source/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1579857100565},{"_id":"source/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1579857100565},{"_id":"source/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1579857100566},{"_id":"source/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1579857100566},{"_id":"source/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1579857100567},{"_id":"source/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1579857100567},{"_id":"source/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1579857100569},{"_id":"source/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1579857100569},{"_id":"source/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1579857100569},{"_id":"source/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1579857100569},{"_id":"source/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1579857100571},{"_id":"source/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1579857100572},{"_id":"source/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1579857100574},{"_id":"source/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1579857100577},{"_id":"source/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1579857100577},{"_id":"source/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1579857100577},{"_id":"source/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1579857100578},{"_id":"source/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1579857100582},{"_id":"source/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1579857100584},{"_id":"source/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1579857100562},{"_id":"source/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1579857100562},{"_id":"source/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1579857100563},{"_id":"source/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1579857100570},{"_id":"source/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1579857100572},{"_id":"source/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1579857100573},{"_id":"source/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1579857100575},{"_id":"source/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1579857100576},{"_id":"source/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1579857100582},{"_id":"source/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1579857100583},{"_id":"source/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1579857100584},{"_id":"source/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1579857100563},{"_id":"source/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1579857100563},{"_id":"source/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1579857100568},{"_id":"source/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1579857100572},{"_id":"source/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1579857100576},{"_id":"source/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1579857100583},{"_id":"source/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1579857100578},{"_id":"source/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1579857100568},{"_id":"source/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1579857100570},{"_id":"source/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1579857100567},{"_id":"source/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1579857100582},{"_id":"source/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1579857100566},{"_id":"source/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579857100574},{"_id":"source/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1579857100571},{"_id":"source/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1579857100575},{"_id":"source/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1579857100561},{"_id":"source/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1579857100581},{"_id":"source/about/index.md","hash":"141865e7564e804f533be719ae6dc010ba27177b","modified":1614698770496},{"_id":"source/tags/index.md","hash":"22dd3308e0a3db852e008fa8c8d526142e790dcb","modified":1579859110818},{"_id":"source/bloglist/index.md","hash":"bd54774723080ac879678dd97b1a30c9064a3402","modified":1606275210264},{"_id":"source/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579863788426},{"_id":"source/_posts/ai-music-direction.md","hash":"17b249dc54092c7984a9a9869b8f05d9e1f4fe46","modified":1592548634917},{"_id":"source/_posts/nature-v2.md","hash":"66f2a37a1d885848cb3c97ef9d86529466e96b60","modified":1579866217542},{"_id":"source/_posts/sapiens-1.md","hash":"613604e163770883841e61a4a47fb3fed184d72c","modified":1579866217543},{"_id":"source/_posts/sapiens-2.md","hash":"ed008d144dcaa5a419b30df28d2bca8fd78741b5","modified":1579866217543},{"_id":"themes/hexo-theme-aircloud/source/css/fonts.css","hash":"c5e7b1d0ada40787eb87fdeef7e64d00588046c3","modified":1579867807637},{"_id":"themes/aircloud/_config.yml","hash":"fce8b918a9ee52e05bd95fdc83bd53e8fe8478ac","modified":1579888968113},{"_id":"themes/aircloud/test.md","hash":"501c404781cfbfe856c8d55dd5f1cd612dd83e41","modified":1579888897249},{"_id":"themes/aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968117},{"_id":"themes/aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968119},{"_id":"themes/aircloud/languages/en.yml","hash":"bc9617a0051b78acbc6f8bd0169d2c51632c6a8f","modified":1602869899555},{"_id":"themes/aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579888968114},{"_id":"themes/aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579888968115},{"_id":"themes/aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579888968117},{"_id":"themes/aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579888968116},{"_id":"themes/aircloud/layout/index.ejs","hash":"09e2407d615be7fe7ac41d11df3b7026e7393080","modified":1579888968115},{"_id":"themes/aircloud/layout/layout.ejs","hash":"7efd113aee90e698e187d0ea1f0b42a1c00d210e","modified":1579888968116},{"_id":"themes/aircloud/layout/post.ejs","hash":"2eb5fc0c2bb801528c3db3b09e6cb4d073e3ad99","modified":1579888968115},{"_id":"themes/aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579888968116},{"_id":"themes/aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968123},{"_id":"themes/aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968124},{"_id":"themes/aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968130},{"_id":"themes/aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968127},{"_id":"themes/aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968122},{"_id":"themes/aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968121},{"_id":"themes/aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579888968119},{"_id":"themes/aircloud/layout/_partial/footer.ejs","hash":"2ab1dc9da5183fc5e74a4bddbf0c29f992057ec9","modified":1579888968119},{"_id":"themes/aircloud/layout/_partial/head.ejs","hash":"3f18d5d4951a205bab25b08d6bf85b054c84a21b","modified":1579888968118},{"_id":"themes/aircloud/layout/_partial/nav.ejs","hash":"b4ec0d5cf15f1ea5f584a795cb744be593a64f66","modified":1602869899561},{"_id":"themes/aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579888968118},{"_id":"themes/aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579888968125},{"_id":"themes/aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579888968128},{"_id":"themes/aircloud/source/_less/common.css","hash":"64914aa6ecd5b948676870e0809e0f220b162e3b","modified":1579888968127},{"_id":"themes/aircloud/source/_less/common.less","hash":"8aef4d8cfdefbcd2e28d4985a4f79a5005ca0b6c","modified":1579888968133},{"_id":"themes/aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579888968129},{"_id":"themes/aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579888968129},{"_id":"themes/aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579888968131},{"_id":"themes/aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579888968121},{"_id":"themes/aircloud/source/_less/hightlight.css","hash":"c8102d3ba9920a2f874be203b3f6925a7d90bdfb","modified":1585907918636},{"_id":"themes/aircloud/source/_less/hightlight.less","hash":"fa11cfd5496659841b3833a9377dccac573f4567","modified":1585907921900},{"_id":"themes/aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579888968125},{"_id":"themes/aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579888968126},{"_id":"themes/aircloud/source/_less/layout.less","hash":"194ac7db2eeee7307fcb7470302f8172100181fb","modified":1579888968120},{"_id":"themes/aircloud/source/_less/layout.css","hash":"40d7cadf42b130ea1b40de1ae73b2b00e27f476f","modified":1579888968130},{"_id":"themes/aircloud/source/_less/nav.css","hash":"32d0640c30a3c921e1f19f74cff2c5095f6ae02c","modified":1579889523601},{"_id":"themes/aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888968130},{"_id":"themes/aircloud/source/_less/post.css","hash":"4adf531589cb55413264c188b29ae47ab703beb8","modified":1579888968127},{"_id":"themes/aircloud/source/_less/post.less","hash":"bbbd81c03e7581950d82bf971eda49e8bed7bee1","modified":1579888968123},{"_id":"themes/aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579888968121},{"_id":"themes/aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579888968120},{"_id":"themes/aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579888968134},{"_id":"themes/aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579888968125},{"_id":"themes/aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579888968122},{"_id":"themes/aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579888968124},{"_id":"themes/aircloud/source/css/aircloud.css","hash":"6bd4c7f2d7f257176e1c540fc21a0b6d59012e9d","modified":1602869899591},{"_id":"themes/aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579888968136},{"_id":"themes/aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579888968135},{"_id":"themes/aircloud/source/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1579888968137},{"_id":"themes/aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579888968137},{"_id":"themes/aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579888968133},{"_id":"themes/aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579888968132},{"_id":"themes/aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579888968132},{"_id":"themes/aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579888968138},{"_id":"public/about/index.html","hash":"a84a43e2388afa0aa8f530a340e8d25707aaedc9","modified":1614707332497},{"_id":"public/tags/index.html","hash":"58203aefd186d183ad69a0143e57dbde19bc945c","modified":1614707332497},{"_id":"public/bloglist/index.html","hash":"cab14db7b8296e792330f67e1630b407e1f141df","modified":1606293946801},{"_id":"public/2018/09/24/ai-music-direction/index.html","hash":"62cb6fca41669375ab2c2ef1d851edc237b7f414","modified":1602952475010},{"_id":"public/archives/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/archives/2018/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/archives/2018/09/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/tags/General-Thoughts/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/tags/AI-Music/index.html","hash":"1faa6f215707357b13469eb504df0f0965ca6253","modified":1592546827459},{"_id":"public/index.html","hash":"ad64b31d1a08ee9ec8552370deacc188be5dedf8","modified":1614707332497},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1580035814789},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1580035814789},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1580035814789},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1580035814789},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1580035814789},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1580035814789},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1580035814789},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1580035814789},{"_id":"public/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1580035814789},{"_id":"public/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1580035814789},{"_id":"public/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1580035814789},{"_id":"public/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1580035814789},{"_id":"public/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1580035814789},{"_id":"public/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1580035814789},{"_id":"public/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1580035814789},{"_id":"public/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1580035814789},{"_id":"public/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1580035814789},{"_id":"public/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1580035814789},{"_id":"public/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1580035814789},{"_id":"public/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1580035814789},{"_id":"public/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1580035814789},{"_id":"public/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1580035814789},{"_id":"public/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1580035814789},{"_id":"public/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1580035814789},{"_id":"public/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1580035814789},{"_id":"public/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1580035814789},{"_id":"public/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1580035814789},{"_id":"public/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1580035814789},{"_id":"public/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1580035814789},{"_id":"public/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1580035814789},{"_id":"public/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1580035814789},{"_id":"public/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1580035814789},{"_id":"public/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1580035814789},{"_id":"public/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1580035814789},{"_id":"public/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1580035814789},{"_id":"public/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1580035814789},{"_id":"public/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1580035814789},{"_id":"public/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1580035814789},{"_id":"public/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1580035814789},{"_id":"public/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1580035814789},{"_id":"public/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1580035814789},{"_id":"public/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1580035814789},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1580035814789},{"_id":"public/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1580035814789},{"_id":"public/css/style.css","hash":"d4cfa90089c78a8b791252afae9fafa3b5658900","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1580035814789},{"_id":"public/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1580035814789},{"_id":"public/css/aircloud.css","hash":"c38c125a9e466cad54b4f28d0fe02cbfa863978c","modified":1580035814789},{"_id":"public/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1580035814789},{"_id":"public/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1580035814789},{"_id":"public/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1580035814789},{"_id":"public/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1580035814789},{"_id":"public/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1580035814789},{"_id":"public/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1580035814789},{"_id":"public/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1580035814789},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1580035814789},{"_id":"public/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1580035814789},{"_id":"public/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1580035814789},{"_id":"public/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1580035814789},{"_id":"public/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1580035814789},{"_id":"public/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1580035814789},{"_id":"public/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1580035814789},{"_id":"public/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1580035814789},{"_id":"public/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1580035814789},{"_id":"public/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1580035814789},{"_id":"public/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1580035814789},{"_id":"public/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1580035814789},{"_id":"public/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1580035814789},{"_id":"source/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1580035544388},{"_id":"public/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1580035814789},{"_id":"source/.DS_Store","hash":"d9cc413eb6c55e0a9690d4ee1880efb015d0a301","modified":1614705002293},{"_id":"source/_posts/vae-symbolic-music.md","hash":"61ac179ef1dd4cfdf5dd612ef3d051dfa8007f39","modified":1602869899471},{"_id":"source/img/extres.png","hash":"69d1d086fa0c0a8cc449a57ac523a6a46345da43","modified":1585282883093},{"_id":"source/img/ashis.png","hash":"e6ddb88bf9d29a480ded59086f70f6880c6fd3cf","modified":1585282883085},{"_id":"source/img/virtuoso.png","hash":"a00ef13f5cfa7775af0c131c62c9c447044ef4b0","modified":1585282883134},{"_id":"source/img/midivae.png","hash":"c060f51c9eb72566f9ec5adcb7581167c6429948","modified":1585282883099},{"_id":"source/img/musicvae.png","hash":"e2d3f7d11481c160bc640d506ab0d8c86ca585d9","modified":1585282883104},{"_id":"source/img/deep-analogy.png","hash":"cbe306dc2c8e08b1254ec41d80b2c277e5363e57","modified":1585282883092},{"_id":"source/img/ashis2.png","hash":"a89e324fb31fe350465a840c1c01d617ac91ec0c","modified":1585280695619},{"_id":"public/archives/2020/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/archives/2020/01/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/tags/VAE/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/tags/Symbolic-Music/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/2020/01/26/vae-symbolic-music/index.html","hash":"0d5984740322f2aacd94c788d51488838ecd00b4","modified":1602952475010},{"_id":"public/img/extres.png","hash":"69d1d086fa0c0a8cc449a57ac523a6a46345da43","modified":1585283543298},{"_id":"public/img/ashis.png","hash":"e6ddb88bf9d29a480ded59086f70f6880c6fd3cf","modified":1585283543298},{"_id":"public/img/virtuoso.png","hash":"a00ef13f5cfa7775af0c131c62c9c447044ef4b0","modified":1585283543298},{"_id":"public/img/midivae.png","hash":"c060f51c9eb72566f9ec5adcb7581167c6429948","modified":1585283543298},{"_id":"public/img/musicvae.png","hash":"e2d3f7d11481c160bc640d506ab0d8c86ca585d9","modified":1585283543298},{"_id":"public/img/ashis2.png","hash":"a89e324fb31fe350465a840c1c01d617ac91ec0c","modified":1585283543298},{"_id":"public/img/deep-analogy.png","hash":"cbe306dc2c8e08b1254ec41d80b2c277e5363e57","modified":1585283543298},{"_id":"source/_posts/annotated-music-transformer.md","hash":"c659ea98fdccac010a62e8c75b0a109127bdc1e6","modified":1592546801360},{"_id":"source/img/new-relative-attention.png","hash":"a2c5013e332df7dc8086be591883d58bc4850b9c","modified":1592469569146},{"_id":"source/img/relative-local-attention-srel.png","hash":"96569fd5291975329f23f950ba1ed831b624c48e","modified":1592497269083},{"_id":"source/img/relative-attention.png","hash":"266bba2f1a135de1f30732985b8fe225cae0749f","modified":1592464556862},{"_id":"source/img/relative-local-attention.png","hash":"65fff103a7ae5bf506c4c0eeb8382a4bb4f5d8ef","modified":1592495938980},{"_id":"source/img/relative-attention-2.png","hash":"7d1540d295547a638177d1ba8ed78e900565db9f","modified":1592465454831},{"_id":"source/img/relative-local-attention-2.png","hash":"c75d4d366e4249f72b396d159365c0101316a998","modified":1592496645623},{"_id":"source/img/relative-local-attention-unmasked.png","hash":"ea943cfb1924f6987c94fadd954448dc8f004ce6","modified":1592534677329},{"_id":"source/img/music-transformer-results.png","hash":"7a9710bf6c7e31b528d8cd0269e29d1cad59e31d","modified":1592536234833},{"_id":"public/archives/2020/04/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/tags/Transformer/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/2020/04/01/annotated-music-transformer/index.html","hash":"df8499cb1f3cbd9b1297123efbec22a0dd538892","modified":1602952475010},{"_id":"public/img/new-relative-attention.png","hash":"a2c5013e332df7dc8086be591883d58bc4850b9c","modified":1592546827459},{"_id":"public/img/relative-local-attention.png","hash":"65fff103a7ae5bf506c4c0eeb8382a4bb4f5d8ef","modified":1592546827459},{"_id":"public/img/relative-local-attention-srel.png","hash":"96569fd5291975329f23f950ba1ed831b624c48e","modified":1592546827459},{"_id":"public/img/relative-attention-2.png","hash":"7d1540d295547a638177d1ba8ed78e900565db9f","modified":1592546827459},{"_id":"public/img/relative-attention.png","hash":"266bba2f1a135de1f30732985b8fe225cae0749f","modified":1592546827459},{"_id":"public/img/relative-local-attention-2.png","hash":"c75d4d366e4249f72b396d159365c0101316a998","modified":1592546827459},{"_id":"public/img/relative-local-attention-unmasked.png","hash":"ea943cfb1924f6987c94fadd954448dc8f004ce6","modified":1592546827459},{"_id":"public/img/music-transformer-results.png","hash":"7a9710bf6c7e31b528d8cd0269e29d1cad59e31d","modified":1592546827459},{"_id":"source/_posts/semi-supervised-music.md","hash":"2eb308aae0276c34379fbce2d6746d4432b10153","modified":1602869899470},{"_id":"source/img/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1592559269022},{"_id":"source/img/kingma-ssl.png","hash":"cad81409c536059666ba9a280ee5ba4aee411782","modified":1592559255521},{"_id":"source/img/radford-sentiment.png","hash":"42f1e71fe640e5fb42150ccd93d5b459da5e7696","modified":1592820108195},{"_id":"source/img/vade-ssl.png","hash":"8614f67849668b3deec6b661003cfe83da1b253a","modified":1592811491270},{"_id":"source/img/kingma-ssl-2.png","hash":"6ee03f31df4da02be259cf629a35d2c383fc449f","modified":1592797175323},{"_id":"source/img/kingma-ssl-3.png","hash":"1c5a92be6ec67ac7d001332271aa1d27c5c344ab","modified":1592806605473},{"_id":"public/archives/2020/05/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/2020/05/13/semi-supervised-music/index.html","hash":"7c5c751bcbbeac8d8b43106551f250251d76b271","modified":1602952475010},{"_id":"public/img/radford-sentiment.png","hash":"42f1e71fe640e5fb42150ccd93d5b459da5e7696","modified":1592821459928},{"_id":"public/img/vade-ssl.png","hash":"8614f67849668b3deec6b661003cfe83da1b253a","modified":1592821459928},{"_id":"public/img/kingma-ssl-3.png","hash":"1c5a92be6ec67ac7d001332271aa1d27c5c344ab","modified":1592821459928},{"_id":"public/img/kingma-ssl.png","hash":"cad81409c536059666ba9a280ee5ba4aee411782","modified":1592821459928},{"_id":"public/img/kingma-ssl-2.png","hash":"6ee03f31df4da02be259cf629a35d2c383fc449f","modified":1592821459928},{"_id":"source/Resume_2020.pdf","hash":"f275e524fd8ee71d1b7f58ce6b48f60bad44e001","modified":1602869899464},{"_id":"source/_posts/conv_fourier.md","hash":"1094dcdeda19bb225bb6e136c03a92e8e83b8d1d","modified":1602869899469},{"_id":"source/publications/index.md","hash":"d548d21f125d033ec092ceaf422c7d6c6f9e85ef","modified":1602869899549},{"_id":"themes/aircloud/layout/publications.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1602869899567},{"_id":"source/img/stft.png","hash":"86df8714b98730bb24c18b42223dd06cbe6f11df","modified":1602869899528},{"_id":"source/img/istft.png","hash":"ffa95973d1878a0a14b3d213f56b9eecd15c18cd","modified":1602869899496},{"_id":"source/_posts/ismir_2020_pt2.md","hash":"eb30d3f34b5b4f454daa3c661f6e7a8c5da06eaa","modified":1602952400759},{"_id":"source/_posts/ismir_2020.md","hash":"f0ce25411914e16fcad7fb895c2d6d42ab7a8653","modified":1602952398986},{"_id":"source/img/ismir_edit.png","hash":"364d2ece9b5525213a1e700c0ef685784380e807","modified":1602912544063},{"_id":"source/img/ismir_jyun.png","hash":"dd056567e51500155e16ab11bc063fa7f3733017","modified":1602921833892},{"_id":"source/img/ismir_unets.png","hash":"ec6312fdeddaa5b3067d4a02efbd6f2afa84c5ee","modified":1602923135999},{"_id":"source/img/ismir_bebop.png","hash":"fd61564838e19bfb90b84c45299d7f70fe1b30d0","modified":1602921195272},{"_id":"source/img/ismir_transcription.png","hash":"dd055291d7f0b63d152ce8c6d6489bf6e9c883b1","modified":1602928208488},{"_id":"source/img/ismir_conn.png","hash":"3b42a3ed59f54cc8621531b7017cd8be56221392","modified":1602908710514},{"_id":"source/img/ismir_fadernets.png","hash":"1450922df5f3f5468e927f8d67b0214aad6db988","modified":1602921546811},{"_id":"source/img/ismir_doras.png","hash":"bc535670967f6b695c7e9eb2dde6275fa748d019","modified":1602928299007},{"_id":"source/img/ismir_dmel.png","hash":"6a3d3596a10da114493778877545a7213ea41fd5","modified":1602910694673},{"_id":"source/img/ismir_attr.png","hash":"79d0586a896e7bea1b05da1b1a917e371933f2a3","modified":1602907612266},{"_id":"source/img/ismir_jazz.png","hash":"65479baab8b04356c7e76609bb6937097c08ef6e","modified":1602921718171},{"_id":"source/img/ismir_singing.png","hash":"5ef1ae3a6e037cfb2a7aa9b55d3925c53bc7b3a6","modified":1602921937706},{"_id":"source/img/ismir_sketchnet.png","hash":"78223b97711bbbd786b7a57f550b97bf06912816","modified":1602921628844},{"_id":"source/img/ismir_furkan.png","hash":"5e074dd55d3458e09413408cf3472a5cbfdaea47","modified":1602928333389},{"_id":"source/img/ismir_phoneme2.png","hash":"e8909e3db86a9cd5e4f66b1e74f544dd517a5bda","modified":1602923649191},{"_id":"source/img/ismir_drumgan.png","hash":"e041aaaa9341ca7a028ed9a5d3de12625711e992","modified":1602922003887},{"_id":"source/img/ismir_interpretable.png","hash":"66256379a003a5d9d1eec436b6e3199605e1315a","modified":1602921375540},{"_id":"source/img/ismir_metric.png","hash":"dfc2618bd8481693be2441a2b4f6e2da90d6f280","modified":1602921885028},{"_id":"source/img/ismir_multitask.png","hash":"a0d498ac948f9571f5b1b24f822c9289e46eb00f","modified":1602924249584},{"_id":"source/img/ismir_spherical.png","hash":"29160a167b5deba7894a4adebf8efb2546028d18","modified":1602928179258},{"_id":"source/img/ismir_lottery.png","hash":"1587e48a6f7b0220bad0d30b7743fe39f867a913","modified":1602928264016},{"_id":"source/img/ismir_vocal.png","hash":"9a7ef49c936c53c9db2a1a47fa027ae2f98ebc68","modified":1602925018253},{"_id":"source/img/ismir_phoneme1.png","hash":"c79177db6ee7cdfefc54fcd2f70d017fe7470c68","modified":1602923566710},{"_id":"source/img/ismir_pianotree.png","hash":"292f049232ec4a4fbb9a692aae82c883ac4cd7d8","modified":1602921304799},{"_id":"public/publications/index.html","hash":"2aebc5f17dafd5b613b53fb29bccf786b615271f","modified":1602952475010},{"_id":"public/2020/07/24/conv_fourier/index.html","hash":"11d51203051bfe555290d0522287161cba31b2ff","modified":1602952475010},{"_id":"public/tags/Music-Signal-Processing/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/tags/Music-Representation-Learning/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/archives/2020/07/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/tags/Music-Information-Retrieval/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/archives/2020/10/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/2020/10/17/ismir_2020/index.html","hash":"ce2a0e2599bd0d2b0edb19e70e5beb045a6c64d8","modified":1602952475010},{"_id":"public/2020/10/17/ismir_2020_pt2/index.html","hash":"35a4ee710e9cbd393c10302ea6ce6b2571da7779","modified":1602952475010},{"_id":"public/img/ismir_edit.png","hash":"364d2ece9b5525213a1e700c0ef685784380e807","modified":1602952475010},{"_id":"public/img/ismir_jyun.png","hash":"dd056567e51500155e16ab11bc063fa7f3733017","modified":1602952475010},{"_id":"public/img/ismir_unets.png","hash":"ec6312fdeddaa5b3067d4a02efbd6f2afa84c5ee","modified":1602952475010},{"_id":"public/img/ismir_bebop.png","hash":"fd61564838e19bfb90b84c45299d7f70fe1b30d0","modified":1602952475010},{"_id":"public/img/ismir_transcription.png","hash":"dd055291d7f0b63d152ce8c6d6489bf6e9c883b1","modified":1602952475010},{"_id":"public/img/ismir_conn.png","hash":"3b42a3ed59f54cc8621531b7017cd8be56221392","modified":1602952475010},{"_id":"public/img/ismir_fadernets.png","hash":"1450922df5f3f5468e927f8d67b0214aad6db988","modified":1602952475010},{"_id":"public/img/ismir_doras.png","hash":"bc535670967f6b695c7e9eb2dde6275fa748d019","modified":1602952475010},{"_id":"public/img/ismir_sketchnet.png","hash":"78223b97711bbbd786b7a57f550b97bf06912816","modified":1602952475010},{"_id":"public/img/ismir_jazz.png","hash":"65479baab8b04356c7e76609bb6937097c08ef6e","modified":1602952475010},{"_id":"public/img/ismir_furkan.png","hash":"5e074dd55d3458e09413408cf3472a5cbfdaea47","modified":1602952475010},{"_id":"public/img/ismir_dmel.png","hash":"6a3d3596a10da114493778877545a7213ea41fd5","modified":1602952475010},{"_id":"public/img/ismir_attr.png","hash":"79d0586a896e7bea1b05da1b1a917e371933f2a3","modified":1602952475010},{"_id":"public/img/ismir_singing.png","hash":"5ef1ae3a6e037cfb2a7aa9b55d3925c53bc7b3a6","modified":1602952475010},{"_id":"public/img/ismir_drumgan.png","hash":"e041aaaa9341ca7a028ed9a5d3de12625711e992","modified":1602952475010},{"_id":"public/img/ismir_phoneme2.png","hash":"e8909e3db86a9cd5e4f66b1e74f544dd517a5bda","modified":1602952475010},{"_id":"public/img/ismir_lottery.png","hash":"1587e48a6f7b0220bad0d30b7743fe39f867a913","modified":1602952475010},{"_id":"public/img/ismir_vocal.png","hash":"9a7ef49c936c53c9db2a1a47fa027ae2f98ebc68","modified":1602952475010},{"_id":"public/img/ismir_spherical.png","hash":"29160a167b5deba7894a4adebf8efb2546028d18","modified":1602952475010},{"_id":"public/img/ismir_metric.png","hash":"dfc2618bd8481693be2441a2b4f6e2da90d6f280","modified":1602952475010},{"_id":"public/img/ismir_multitask.png","hash":"a0d498ac948f9571f5b1b24f822c9289e46eb00f","modified":1602952475010},{"_id":"public/img/ismir_interpretable.png","hash":"66256379a003a5d9d1eec436b6e3199605e1315a","modified":1602952475010},{"_id":"public/img/ismir_phoneme1.png","hash":"c79177db6ee7cdfefc54fcd2f70d017fe7470c68","modified":1602952475010},{"_id":"public/img/ismir_pianotree.png","hash":"292f049232ec4a4fbb9a692aae82c883ac4cd7d8","modified":1602952475010},{"_id":"source/_posts/param-pooling.md","hash":"3e86a1a1b796fddc7dc3f80b8dc9f2d9a0805403","modified":1606293922962},{"_id":"source/img/mean-vs-max-pool.png","hash":"79ed22fdd7924a2375389830ab99f6eb08645f2b","modified":1606278632425},{"_id":"public/archives/2020/11/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/tags/Deep-Learning/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/2020/11/25/param-pooling/index.html","hash":"e93f3b7df9efb692b8154b92361741e17894d799","modified":1606293946801},{"_id":"public/img/mean-vs-max-pool.png","hash":"79ed22fdd7924a2375389830ab99f6eb08645f2b","modified":1606293946801},{"_id":"source/_posts/challenge-csd.md","hash":"77ff51b05e2dc885ce70bfe7b5a281b372ff7432","modified":1614707303485},{"_id":"source/img/csd-shingles.png","hash":"64ca2d1021c74502805c01ac7dc28d9029087b84","modified":1614705608141},{"_id":"source/img/csd-ml-ops.png","hash":"25a5597798234d7d6226e6d0b7e3ac0e4bfffde9","modified":1614703676074},{"_id":"source/img/csd-dominant.png","hash":"10c2b448c90f0c2f609952b7fc62783efd0345ec","modified":1614703939313},{"_id":"source/img/csd-hybrid.png","hash":"13521a1a300fcf5dd343d4aa3f82ac7d7c975520","modified":1614704370900},{"_id":"source/img/csd-bytecover.png","hash":"89fe60f67e666e26da7ad386fe84ebe5beeacc80","modified":1614704968059},{"_id":"source/img/csd-hpcp.png","hash":"853f4bb3e5da58bd5fde8d71c38516a4e2452e58","modified":1614704302666},{"_id":"public/tags/ML-in-Production/index.html","hash":"40e4005831e43ca820adb883eac55f7d99721c0e","modified":1614707332497},{"_id":"public/archives/page/2/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/archives/2021/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/archives/2021/02/index.html","hash":"29b4f530b3cff02ba2dfd3a3b08212d3e9a30d90","modified":1614707332497},{"_id":"public/2021/02/25/challenge-csd/index.html","hash":"ac2b479e29ae9fbf33874b0a031af7cf1c8f1086","modified":1614707332497},{"_id":"public/img/csd-shingles.png","hash":"64ca2d1021c74502805c01ac7dc28d9029087b84","modified":1614707332497},{"_id":"public/img/csd-dominant.png","hash":"10c2b448c90f0c2f609952b7fc62783efd0345ec","modified":1614707332497},{"_id":"public/img/csd-ml-ops.png","hash":"25a5597798234d7d6226e6d0b7e3ac0e4bfffde9","modified":1614707332497},{"_id":"public/img/csd-hybrid.png","hash":"13521a1a300fcf5dd343d4aa3f82ac7d7c975520","modified":1614707332497},{"_id":"public/img/csd-bytecover.png","hash":"89fe60f67e666e26da7ad386fe84ebe5beeacc80","modified":1614707332497},{"_id":"public/img/csd-hpcp.png","hash":"853f4bb3e5da58bd5fde8d71c38516a4e2452e58","modified":1614707332497}],"Category":[],"Data":[],"Page":[{"layout":"about","title":"About","date":"2016-04-20T20:48:33.000Z","comments":1,"_content":"## About Me\n\nHi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.\n\nI worked as a research assistant under the [Audio, Music, Affective Computing and AI team](http://dorienherremans.com/team) (AMAAI) at Singapore University of Technology and Design. \n\nMy research interests include cover song detection, controllable deep generative models on music, and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.\n\nI earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, [Visenze](https://www.visenze.com/), and [DSAIR @ NTU](https://dsair.ntu.edu.sg/Pages/Home.aspx).\n\nMusic wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About\"\ndate: 2016-04-21 04:48:33\ncomments: true\n---\n## About Me\n\nHi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.\n\nI worked as a research assistant under the [Audio, Music, Affective Computing and AI team](http://dorienherremans.com/team) (AMAAI) at Singapore University of Technology and Design. \n\nMy research interests include cover song detection, controllable deep generative models on music, and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.\n\nI earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, [Visenze](https://www.visenze.com/), and [DSAIR @ NTU](https://dsair.ntu.edu.sg/Pages/Home.aspx).\n\nMusic wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!","updated":"2021-03-02T15:26:10.496Z","path":"about/index.html","_id":"ck5ryykx7000011v5f6uj62kx","content":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>Hi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.</p>\n<p>I worked as a research assistant under the <a href=\"http://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Audio, Music, Affective Computing and AI team</a> (AMAAI) at Singapore University of Technology and Design. </p>\n<p>My research interests include cover song detection, controllable deep generative models on music, and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.</p>\n<p>I earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, <a href=\"https://www.visenze.com/\" target=\"_blank\" rel=\"noopener\">Visenze</a>, and <a href=\"https://dsair.ntu.edu.sg/Pages/Home.aspx\" target=\"_blank\" rel=\"noopener\">DSAIR @ NTU</a>.</p>\n<p>Music wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>Hi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.</p>\n<p>I worked as a research assistant under the <a href=\"http://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Audio, Music, Affective Computing and AI team</a> (AMAAI) at Singapore University of Technology and Design. </p>\n<p>My research interests include cover song detection, controllable deep generative models on music, and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.</p>\n<p>I earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, <a href=\"https://www.visenze.com/\" target=\"_blank\" rel=\"noopener\">Visenze</a>, and <a href=\"https://dsair.ntu.edu.sg/Pages/Home.aspx\" target=\"_blank\" rel=\"noopener\">DSAIR @ NTU</a>.</p>\n<p>Music wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!</p>\n"},{"layout":"tags","title":"Tags","_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\n---","date":"2020-01-24T09:45:10.818Z","updated":"2020-01-24T09:45:10.818Z","path":"tags/index.html","comments":1,"_id":"ck5rzbz8j00008dv5cddl8ema","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"Blog List","date":"2020-01-22T20:48:33.000Z","comments":1,"_content":"## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\nSander Dieleman: https://benanne.github.io/\nJustin Salamon: https://www.justinsalamon.com/news\nJordi Pons: http://www.jordipons.me/\nIlaria Manco: https://ilariamanco.com/\n\n### ML in Production\nChip Huyen: https://huyenchip.com/\nEugene Yan: https://eugeneyan.com/\nShreya Shankar: https://www.shreya-shankar.com/\nNetflix Blog: https://netflixtechblog.com/\nAssaf Pinhasi: https://medium.com/@assaf.pinhasi\n\n### Other Wonderful Folks\nJiang Yu Ngyuwi: https://nguwijy.wordpress.com\nThomas Kipf: http://tkipf.github.io/\nChaitanya Joshi: https://chaitjo.github.io/\n","source":"bloglist/index.md","raw":"---\nlayout: \"about\"\ntitle: \"Blog List\"\ndate: 2020-01-23 04:48:33\ncomments: true\n---\n## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\nSander Dieleman: https://benanne.github.io/\nJustin Salamon: https://www.justinsalamon.com/news\nJordi Pons: http://www.jordipons.me/\nIlaria Manco: https://ilariamanco.com/\n\n### ML in Production\nChip Huyen: https://huyenchip.com/\nEugene Yan: https://eugeneyan.com/\nShreya Shankar: https://www.shreya-shankar.com/\nNetflix Blog: https://netflixtechblog.com/\nAssaf Pinhasi: https://medium.com/@assaf.pinhasi\n\n### Other Wonderful Folks\nJiang Yu Ngyuwi: https://nguwijy.wordpress.com\nThomas Kipf: http://tkipf.github.io/\nChaitanya Joshi: https://chaitjo.github.io/\n","updated":"2020-11-25T03:33:30.264Z","path":"bloglist/index.html","_id":"ck5s0x47j0000w5v57s73193s","content":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a><br>Sander Dieleman: <a href=\"https://benanne.github.io/\" target=\"_blank\" rel=\"noopener\">https://benanne.github.io/</a><br>Justin Salamon: <a href=\"https://www.justinsalamon.com/news\" target=\"_blank\" rel=\"noopener\">https://www.justinsalamon.com/news</a><br>Jordi Pons: <a href=\"http://www.jordipons.me/\" target=\"_blank\" rel=\"noopener\">http://www.jordipons.me/</a><br>Ilaria Manco: <a href=\"https://ilariamanco.com/\" target=\"_blank\" rel=\"noopener\">https://ilariamanco.com/</a></p>\n<h3 id=\"ML-in-Production\"><a href=\"#ML-in-Production\" class=\"headerlink\" title=\"ML in Production\"></a>ML in Production</h3><p>Chip Huyen: <a href=\"https://huyenchip.com/\" target=\"_blank\" rel=\"noopener\">https://huyenchip.com/</a><br>Eugene Yan: <a href=\"https://eugeneyan.com/\" target=\"_blank\" rel=\"noopener\">https://eugeneyan.com/</a><br>Shreya Shankar: <a href=\"https://www.shreya-shankar.com/\" target=\"_blank\" rel=\"noopener\">https://www.shreya-shankar.com/</a><br>Netflix Blog: <a href=\"https://netflixtechblog.com/\" target=\"_blank\" rel=\"noopener\">https://netflixtechblog.com/</a><br>Assaf Pinhasi: <a href=\"https://medium.com/@assaf.pinhasi\" target=\"_blank\" rel=\"noopener\">https://medium.com/@assaf.pinhasi</a></p>\n<h3 id=\"Other-Wonderful-Folks\"><a href=\"#Other-Wonderful-Folks\" class=\"headerlink\" title=\"Other Wonderful Folks\"></a>Other Wonderful Folks</h3><p>Jiang Yu Ngyuwi: <a href=\"https://nguwijy.wordpress.com\" target=\"_blank\" rel=\"noopener\">https://nguwijy.wordpress.com</a><br>Thomas Kipf: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a><br>Sander Dieleman: <a href=\"https://benanne.github.io/\" target=\"_blank\" rel=\"noopener\">https://benanne.github.io/</a><br>Justin Salamon: <a href=\"https://www.justinsalamon.com/news\" target=\"_blank\" rel=\"noopener\">https://www.justinsalamon.com/news</a><br>Jordi Pons: <a href=\"http://www.jordipons.me/\" target=\"_blank\" rel=\"noopener\">http://www.jordipons.me/</a><br>Ilaria Manco: <a href=\"https://ilariamanco.com/\" target=\"_blank\" rel=\"noopener\">https://ilariamanco.com/</a></p>\n<h3 id=\"ML-in-Production\"><a href=\"#ML-in-Production\" class=\"headerlink\" title=\"ML in Production\"></a>ML in Production</h3><p>Chip Huyen: <a href=\"https://huyenchip.com/\" target=\"_blank\" rel=\"noopener\">https://huyenchip.com/</a><br>Eugene Yan: <a href=\"https://eugeneyan.com/\" target=\"_blank\" rel=\"noopener\">https://eugeneyan.com/</a><br>Shreya Shankar: <a href=\"https://www.shreya-shankar.com/\" target=\"_blank\" rel=\"noopener\">https://www.shreya-shankar.com/</a><br>Netflix Blog: <a href=\"https://netflixtechblog.com/\" target=\"_blank\" rel=\"noopener\">https://netflixtechblog.com/</a><br>Assaf Pinhasi: <a href=\"https://medium.com/@assaf.pinhasi\" target=\"_blank\" rel=\"noopener\">https://medium.com/@assaf.pinhasi</a></p>\n<h3 id=\"Other-Wonderful-Folks\"><a href=\"#Other-Wonderful-Folks\" class=\"headerlink\" title=\"Other Wonderful Folks\"></a>Other Wonderful Folks</h3><p>Jiang Yu Ngyuwi: <a href=\"https://nguwijy.wordpress.com\" target=\"_blank\" rel=\"noopener\">https://nguwijy.wordpress.com</a><br>Thomas Kipf: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a></p>\n"},{"layout":"publications","title":"Publications","date":"2020-01-22T20:48:33.000Z","comments":1,"_content":"## Publications\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n\n> Using **\"faders\"** (latent regularization & disentanglement) to control low-level musical features and **\"presets\"** (GM-VAE) to capture the relationship between the \"faders\" and the abstract, high-level feature for controllable music generation.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n\n> Introducing **GM-VAEs** on temporal piano performance modelling, hence allowing **fine-grained controllability** on performance style features (articulation & dynamics) for audio synthesis.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n\n> A two-stage music generation pipeline with a **chord generator** and a **chord-to-note generator** as a *seq2seq* task. Chord embeddings unveil the pattern of the **Circle-of-Fifths**.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\">DEMO</a>\n</div>","source":"publications/index.md","raw":"---\nlayout: \"publications\"\ntitle: \"Publications\"\ndate: 2020-01-23 04:48:33\ncomments: true\n---\n## Publications\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n\n> Using **\"faders\"** (latent regularization & disentanglement) to control low-level musical features and **\"presets\"** (GM-VAE) to capture the relationship between the \"faders\" and the abstract, high-level feature for controllable music generation.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n\n> Introducing **GM-VAEs** on temporal piano performance modelling, hence allowing **fine-grained controllability** on performance style features (articulation & dynamics) for audio synthesis.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n\n> A two-stage music generation pipeline with a **chord generator** and a **chord-to-note generator** as a *seq2seq* task. Chord embeddings unveil the pattern of the **Circle-of-Fifths**.\n\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\">DEMO</a>\n</div>","updated":"2020-10-16T17:38:19.549Z","path":"publications/index.html","_id":"ckgcjd4cv0001w19k2zkg3ytc","content":"<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n\n<blockquote>\n<p>Using <strong>“faders”</strong> (latent regularization &amp; disentanglement) to control low-level musical features and <strong>“presets”</strong> (GM-VAE) to capture the relationship between the “faders” and the abstract, high-level feature for controllable music generation.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n\n<blockquote>\n<p>Introducing <strong>GM-VAEs</strong> on temporal piano performance modelling, hence allowing <strong>fine-grained controllability</strong> on performance style features (articulation &amp; dynamics) for audio synthesis.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n\n<blockquote>\n<p>A two-stage music generation pipeline with a <strong>chord generator</strong> and a <strong>chord-to-note generator</strong> as a <em>seq2seq</em> task. Chord embeddings unveil the pattern of the <strong>Circle-of-Fifths</strong>.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Publications\"><a href=\"#Publications\" class=\"headerlink\" title=\"Publications\"></a>Publications</h2><h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>\n<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>\n<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>\n\n<blockquote>\n<p>Using <strong>“faders”</strong> (latent regularization &amp; disentanglement) to control low-level musical features and <strong>“presets”</strong> (GM-VAE) to capture the relationship between the “faders” and the abstract, high-level feature for controllable music generation.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2007.15474.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/music-fader-nets\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://music-fadernets.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>\n<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>\n<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>\n\n<blockquote>\n<p>Introducing <strong>GM-VAEs</strong> on temporal piano performance modelling, hence allowing <strong>fine-grained controllability</strong> on performance style features (articulation &amp; dynamics) for audio synthesis.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"https://arxiv.org/pdf/2006.09833.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/piano-synthesis\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://piano-performance-synthesis.github.io/\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>\n\n<br/>\n\n<h3><mark style=\"background-color: rgba(39,243,106,0.15);\">ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>\n<ins>Hao Hao Tan</ins><br/>\n<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>\n\n<blockquote>\n<p>A two-stage music generation pipeline with a <strong>chord generator</strong> and a <strong>chord-to-note generator</strong> as a <em>seq2seq</em> task. Chord embeddings unveil the pattern of the <strong>Circle-of-Fifths</strong>.</p>\n</blockquote>\n<div class=\"page-tags\">\n    <a class=\"item\" href=\"http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf\" target=\"_blank\" rel=\"noopener\">PDF</a>\n    <a class=\"item\" href=\"https://github.com/gudgud96/ChordAL\" target=\"_blank\" rel=\"noopener\">CODE</a>\n    <a class=\"item\" href=\"https://soundcloud.com/hord-hord-basedomposer\" target=\"_blank\" rel=\"noopener\">DEMO</a>\n</div>"}],"Post":[{"title":"Where Could AI Music Possibly Head Towards?","date":"2018-09-24T08:52:50.000Z","_content":"\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","source":"_posts/ai-music-direction.md","raw":"---\ntitle: Where Could AI Music Possibly Head Towards?\ndate: 2018-09-24 16:52:50\ntags:\n    - General Thoughts\n---\n\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","slug":"ai-music-direction","published":1,"updated":"2020-06-19T06:37:14.917Z","_id":"ck5s3jzfj00004qv5g9865rpz","comments":1,"layout":"post","photos":[],"link":"","content":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n"},{"title":"VAE In Symbolic Music Modelling","date":"2020-01-26T09:54:50.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","source":"_posts/vae-symbolic-music.md","raw":"---\ntitle: VAE In Symbolic Music Modelling\ndate: 2020-01-26 17:54:50\ntags:\n    - VAE\n    - Symbolic Music\n    - Music Representation Learning\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","slug":"vae-symbolic-music","published":1,"updated":"2020-10-16T17:38:19.471Z","_id":"ck89oi0it0000tbm8hg9jhjt3","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n"},{"title":"Understanding Music Transformer","date":"2020-04-01T09:54:50.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Concepts discussed in the Music Transformer paper\n2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention\n3 - Ideas in the paper to efficiently implement the above attention mechanisms\n4 - Results on music generation\n\n## 1 - Introduction \nI personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the \"skewing\" procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.\n\n## 2 - Background of the Music Transformer\n\nThe [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf) paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. \n\nThe idea is as follows: by modelling each **performance event** as a token (as proposed in [Oore et al.](https://arxiv.org/pdf/1808.03715)), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the [demo samples](https://magenta.tensorflow.org/music-transformer) provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.\n\nSome related work of using Transformer architecture on generating music include [MuseNet](https://openai.com/blog/musenet/) (from OpenAI), and also [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf). It is evident that the Transformer architecture would be the backbone of music generation models in future research.\n\nFor the basics of Transformers, we refer the readers who are interested to these links: \n- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP;\n- [Understanding and Applying Self-Attention for NLP Video](https://www.youtube.com/watch?v=OYygPG4d9H0);\n- [Transformer Video by Hung-yi Lee](https://www.youtube.com/watch?v=ugWDIIOHtPA), Mandarin version;\n- [Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/), a great blog post by Chaitanya Joshi.\n\n## 3 - Transformer with Relative Attention\n\nIn my opinion, the Music Transformer paper is not only an application work, but its crux also includes *optimization* work on implementing Transformer with **relative attention**. We will delve into this part below.\n\nThe essence of Transformer is on **self-attention**: for an output sequence of the Transformer, the elements on each position is a result of \"attending to\" (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$\n\nwhere \\\\(Q, K, V\\\\) represents the query, key and value tensors, each having tensor shape \\\\((l, d)\\\\), with \\\\(l\\\\) and \\\\(d\\\\) representing the sequence length and the number of dimensions used in the model respectively.\n\nIn [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), the concept of **relative attention** is proposed. The idea is to represent **relative position representations** more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$\n\nThe difference is to learn an additional tensor \\\\(S^{rel}\\\\) of shape \\\\((l, l)\\\\). From the shape itself, we can see that the values \\\\(v_{ij}\\\\) in \\\\(S^{rel}\\\\) must be related to the relative distance of position \\\\(i\\\\) and \\\\(j\\\\) in length \\\\(l\\\\). \nIn fact, we can also view a sequence as a **fully connected graph** of its elements, hence \\\\(S^{rel}\\\\) can be seen as a tensor representing information about the **edges** in the graph.  \n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\nHow do we learn the values in \\\\(S^{rel}\\\\)? In Shaw et al., this is done by learning an extra weight tensor \\\\(R\\\\) of shape \\\\((l, l, d)\\\\) -- only 1 dimension extra as compared to \\\\(S^{rel}\\\\). We can see it as having an **embedding** of dimension \\\\(d\\\\), at each position in a distance matrix of shape \\\\((l, l)\\\\). The values \\\\(R_{i, j}\\\\) means the embeddings which encode the relative position between position \\\\(i\\\\) from query tensor,  and position \\\\(j\\\\) from key tensor. Here, we only discuss cases of **masked self-attention** without looking forward -- only positions where \\\\(i \\ge j\\\\) is concerned because the model only attends to nodes that occur at the current time step or before.\n\nAfter that, we reshape \\\\(Q\\\\) into a \\\\((l, 1, d)\\\\) tensor, and multiply by $$S^{rel} = QR^T$$\n\nHowever, this incurs a total space complexity of \\\\(O(L^2 D)\\\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to \"perform some algorithmic tricks\" in order to obtain \\\\(S^{rel}\\\\) in space complexity of \\\\(O(LD)\\\\).\n\n## 4 - Relative Global Attention\n\nIn Music Transformer, the number of unique embeddings in \\\\(E^r\\\\) is assumed to be fixed. They used \\\\(l\\\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\\\(l - 1\\\\), from first position to the \\\\(l^{th}\\\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the \"edge\" between 1st and 3rd element, is the same as the embedding for the \"edge\" between 2nd and 4th element. \n\nIf we follow the assumptions made above, we can observe that the desired \\\\(S_{rel}\\\\) can actually be calculated from multiplying \\\\(Q{E^{r}}^T\\\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\\\(S_{rel}\\\\) from \\\\(Q{E^{r}}^T\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\nThe bottom row in Figure 2 is known as the \"skewing\" procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\\\(M_\\textrm{masked}\\\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\\\(S_{rel}\\\\).\n\nHence, without using the gigantic \\\\(R\\\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\\\(O(LD)\\\\) space complexity. Although both implementations result in \\\\(O(L^{2}D)\\\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.\n\n## 4 - Relative Local Attention\n\n**Local attention**, first coined in [Luong et al](https://arxiv.org/pdf/1508.04025) and [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf), means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens *nearby* at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several *non-overlapping* blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a *memory* block which includes the current block and a finite number of tokens before the block).\n\nFor simplicity, let's start with an example of dividing the input sequence of 6 tokens into 2 blocks -- Block 1 \\\\([x_1, x_2, x_3]\\\\), and Block 2 \\\\([x_4, x_5, x_6]\\\\). Hence, the attention components include: Block 1 \\\\(\\leftrightarrow\\\\) Block 1, Block 2 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2, as shown in Figure 2.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\nThe difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\\\(\\leftrightarrow\\\\) Block 1 as in global attention.\n\nSo, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\\\(S_{rel}\\\\) which can capture relative position information, which similarly we can obtain the values from \\\\(Q{E^r}^T\\\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\\\(S_{rel}\\\\) look like for 2 blocks:\n\n<figure>\n  <img style=\"width:40%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\nFrom the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\\\(S_{rel}\\\\) output for Block 1 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and \"skewing\" procedure can be used. The only thing we need to change is the unmasked \\\\(S_{rel}\\\\) output for Block 2 \\\\(\\leftrightarrow\\\\) Block 1, which will be the focus of discussion below.\n\n<br/>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\nWe denote \\\\(N\\\\) to be the block length, hence in our simple example \\\\(N = 6 \\div 2 = 3\\\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\\\(E^r\\\\), only that this time we collect indices from \\\\(1\\\\) to \\\\(2N-1\\\\) (if we notice in the desired \\\\(S_{rel}\\\\), these are the involved embeddings). This index range \\\\([1:2N -1]\\\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\\\(Q{E^r}^T_{1:2N-1}\\\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.\n\nOne thing to note is that the discussion above omits the head indices -- in actual implementation, one can choose to use the same \\\\(S_{rel}\\\\) for each head, or learn separate \\\\(S_{rel}\\\\) for different heads.\n\nNote that this technique can be extended to larger numbers of divided blocks -- generally, self-attending blocks can used the \"skewing\" procedure from relative global attention; non-self-attending blocks should use the latter unmasked \"skewing\" procedure. By this, we can succesfully compute \\\\(QK^T + S_{rel}\\\\), with both shapes \\\\((N, N)\\\\), for each local attention procedure.\n\n## 5 - Results\n\nFinally, we take a quick glance on the results and comparisons made by the paper.\n\n<figure>\n  <img style=\"width:70%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\nIn general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.\n\nThe main takeway from this work is clear: **relative information** does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf)), so we could probably foresee comparisons on different Transformer architectures for music generation in future.\n\n## 6 - Code Implementation\n\nThe Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:\n\n1. [The Main Script](https://github.com/magenta/magenta/tree/master/magenta/models/score2perf)\n2. [Relative Global Attention](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934)\n3. [Relative Local Attention](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964)\n4. [Skewing for Masked Cases (Global)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830)\n5. [Skewing for Unmasked Cases (Local)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813)\n\nThe code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both [Tensorflow](https://github.com/jason9693/MusicTransformer-tensorflow2.0) and [PyTorch](https://github.com/jason9693/MusicTransformer-pytorch/), and I definitely recommend to check them out because it really helped me a lot on understanding the paper.","source":"_posts/annotated-music-transformer.md","raw":"---\ntitle: Understanding Music Transformer\ndate: 2020-04-01 17:54:50\ntags:\n    - Transformer\n    - Symbolic Music\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Concepts discussed in the Music Transformer paper\n2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention\n3 - Ideas in the paper to efficiently implement the above attention mechanisms\n4 - Results on music generation\n\n## 1 - Introduction \nI personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the \"skewing\" procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.\n\n## 2 - Background of the Music Transformer\n\nThe [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf) paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. \n\nThe idea is as follows: by modelling each **performance event** as a token (as proposed in [Oore et al.](https://arxiv.org/pdf/1808.03715)), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the [demo samples](https://magenta.tensorflow.org/music-transformer) provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.\n\nSome related work of using Transformer architecture on generating music include [MuseNet](https://openai.com/blog/musenet/) (from OpenAI), and also [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf). It is evident that the Transformer architecture would be the backbone of music generation models in future research.\n\nFor the basics of Transformers, we refer the readers who are interested to these links: \n- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard NLP;\n- [Understanding and Applying Self-Attention for NLP Video](https://www.youtube.com/watch?v=OYygPG4d9H0);\n- [Transformer Video by Hung-yi Lee](https://www.youtube.com/watch?v=ugWDIIOHtPA), Mandarin version;\n- [Transformers are Graph Neural Networks](https://graphdeeplearning.github.io/post/transformers-are-gnns/), a great blog post by Chaitanya Joshi.\n\n## 3 - Transformer with Relative Attention\n\nIn my opinion, the Music Transformer paper is not only an application work, but its crux also includes *optimization* work on implementing Transformer with **relative attention**. We will delve into this part below.\n\nThe essence of Transformer is on **self-attention**: for an output sequence of the Transformer, the elements on each position is a result of \"attending to\" (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$\n\nwhere \\\\(Q, K, V\\\\) represents the query, key and value tensors, each having tensor shape \\\\((l, d)\\\\), with \\\\(l\\\\) and \\\\(d\\\\) representing the sequence length and the number of dimensions used in the model respectively.\n\nIn [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), the concept of **relative attention** is proposed. The idea is to represent **relative position representations** more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:\n$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$\n\nThe difference is to learn an additional tensor \\\\(S^{rel}\\\\) of shape \\\\((l, l)\\\\). From the shape itself, we can see that the values \\\\(v_{ij}\\\\) in \\\\(S^{rel}\\\\) must be related to the relative distance of position \\\\(i\\\\) and \\\\(j\\\\) in length \\\\(l\\\\). \nIn fact, we can also view a sequence as a **fully connected graph** of its elements, hence \\\\(S^{rel}\\\\) can be seen as a tensor representing information about the **edges** in the graph.  \n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\nHow do we learn the values in \\\\(S^{rel}\\\\)? In Shaw et al., this is done by learning an extra weight tensor \\\\(R\\\\) of shape \\\\((l, l, d)\\\\) -- only 1 dimension extra as compared to \\\\(S^{rel}\\\\). We can see it as having an **embedding** of dimension \\\\(d\\\\), at each position in a distance matrix of shape \\\\((l, l)\\\\). The values \\\\(R_{i, j}\\\\) means the embeddings which encode the relative position between position \\\\(i\\\\) from query tensor,  and position \\\\(j\\\\) from key tensor. Here, we only discuss cases of **masked self-attention** without looking forward -- only positions where \\\\(i \\ge j\\\\) is concerned because the model only attends to nodes that occur at the current time step or before.\n\nAfter that, we reshape \\\\(Q\\\\) into a \\\\((l, 1, d)\\\\) tensor, and multiply by $$S^{rel} = QR^T$$\n\nHowever, this incurs a total space complexity of \\\\(O(L^2 D)\\\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to \"perform some algorithmic tricks\" in order to obtain \\\\(S^{rel}\\\\) in space complexity of \\\\(O(LD)\\\\).\n\n## 4 - Relative Global Attention\n\nIn Music Transformer, the number of unique embeddings in \\\\(E^r\\\\) is assumed to be fixed. They used \\\\(l\\\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\\\(l - 1\\\\), from first position to the \\\\(l^{th}\\\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the \"edge\" between 1st and 3rd element, is the same as the embedding for the \"edge\" between 2nd and 4th element. \n\nIf we follow the assumptions made above, we can observe that the desired \\\\(S_{rel}\\\\) can actually be calculated from multiplying \\\\(Q{E^{r}}^T\\\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\\\(S_{rel}\\\\) from \\\\(Q{E^{r}}^T\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\nThe bottom row in Figure 2 is known as the \"skewing\" procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\\\(M_\\textrm{masked}\\\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\\\(S_{rel}\\\\).\n\nHence, without using the gigantic \\\\(R\\\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\\\(O(LD)\\\\) space complexity. Although both implementations result in \\\\(O(L^{2}D)\\\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.\n\n## 4 - Relative Local Attention\n\n**Local attention**, first coined in [Luong et al](https://arxiv.org/pdf/1508.04025) and [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf), means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens *nearby* at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several *non-overlapping* blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a *memory* block which includes the current block and a finite number of tokens before the block).\n\nFor simplicity, let's start with an example of dividing the input sequence of 6 tokens into 2 blocks -- Block 1 \\\\([x_1, x_2, x_3]\\\\), and Block 2 \\\\([x_4, x_5, x_6]\\\\). Hence, the attention components include: Block 1 \\\\(\\leftrightarrow\\\\) Block 1, Block 2 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2, as shown in Figure 2.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\nThe difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\\\(\\leftrightarrow\\\\) Block 1 as in global attention.\n\nSo, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\\\(S_{rel}\\\\) which can capture relative position information, which similarly we can obtain the values from \\\\(Q{E^r}^T\\\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\\\(S_{rel}\\\\) look like for 2 blocks:\n\n<figure>\n  <img style=\"width:40%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\nFrom the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\\\(S_{rel}\\\\) output for Block 1 \\\\(\\leftrightarrow\\\\) Block 1, and Block 2 \\\\(\\leftrightarrow\\\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and \"skewing\" procedure can be used. The only thing we need to change is the unmasked \\\\(S_{rel}\\\\) output for Block 2 \\\\(\\leftrightarrow\\\\) Block 1, which will be the focus of discussion below.\n\n<br/>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\nWe denote \\\\(N\\\\) to be the block length, hence in our simple example \\\\(N = 6 \\div 2 = 3\\\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\\\(E^r\\\\), only that this time we collect indices from \\\\(1\\\\) to \\\\(2N-1\\\\) (if we notice in the desired \\\\(S_{rel}\\\\), these are the involved embeddings). This index range \\\\([1:2N -1]\\\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\\\(Q{E^r}^T_{1:2N-1}\\\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.\n\nOne thing to note is that the discussion above omits the head indices -- in actual implementation, one can choose to use the same \\\\(S_{rel}\\\\) for each head, or learn separate \\\\(S_{rel}\\\\) for different heads.\n\nNote that this technique can be extended to larger numbers of divided blocks -- generally, self-attending blocks can used the \"skewing\" procedure from relative global attention; non-self-attending blocks should use the latter unmasked \"skewing\" procedure. By this, we can succesfully compute \\\\(QK^T + S_{rel}\\\\), with both shapes \\\\((N, N)\\\\), for each local attention procedure.\n\n## 5 - Results\n\nFinally, we take a quick glance on the results and comparisons made by the paper.\n\n<figure>\n  <img style=\"width:70%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\nIn general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.\n\nThe main takeway from this work is clear: **relative information** does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf)), so we could probably foresee comparisons on different Transformer architectures for music generation in future.\n\n## 6 - Code Implementation\n\nThe Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:\n\n1. [The Main Script](https://github.com/magenta/magenta/tree/master/magenta/models/score2perf)\n2. [Relative Global Attention](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934)\n3. [Relative Local Attention](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964)\n4. [Skewing for Masked Cases (Global)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830)\n5. [Skewing for Unmasked Cases (Local)](https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813)\n\nThe code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both [Tensorflow](https://github.com/jason9693/MusicTransformer-tensorflow2.0) and [PyTorch](https://github.com/jason9693/MusicTransformer-pytorch/), and I definitely recommend to check them out because it really helped me a lot on understanding the paper.","slug":"annotated-music-transformer","published":1,"updated":"2020-06-19T06:06:41.360Z","_id":"ck8jt5xfe0000jbm81um92vdu","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Concepts discussed in the Music Transformer paper<br>2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention<br>3 - Ideas in the paper to efficiently implement the above attention mechanisms<br>4 - Results on music generation</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>I personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the “skewing” procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.</p>\n<h2 id=\"2-Background-of-the-Music-Transformer\"><a href=\"#2-Background-of-the-Music-Transformer\" class=\"headerlink\" title=\"2 - Background of the Music Transformer\"></a>2 - Background of the Music Transformer</h2><p>The <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a> paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. </p>\n<p>The idea is as follows: by modelling each <strong>performance event</strong> as a token (as proposed in <a href=\"https://arxiv.org/pdf/1808.03715\" target=\"_blank\" rel=\"noopener\">Oore et al.</a>), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the <a href=\"https://magenta.tensorflow.org/music-transformer\" target=\"_blank\" rel=\"noopener\">demo samples</a> provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.</p>\n<p>Some related work of using Transformer architecture on generating music include <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a> (from OpenAI), and also <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>. It is evident that the Transformer architecture would be the backbone of music generation models in future research.</p>\n<p>For the basics of Transformers, we refer the readers who are interested to these links: </p>\n<ul>\n<li><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"noopener\">The Annotated Transformer</a> from Harvard NLP;</li>\n<li><a href=\"https://www.youtube.com/watch?v=OYygPG4d9H0\" target=\"_blank\" rel=\"noopener\">Understanding and Applying Self-Attention for NLP Video</a>;</li>\n<li><a href=\"https://www.youtube.com/watch?v=ugWDIIOHtPA\" target=\"_blank\" rel=\"noopener\">Transformer Video by Hung-yi Lee</a>, Mandarin version;</li>\n<li><a href=\"https://graphdeeplearning.github.io/post/transformers-are-gnns/\" target=\"_blank\" rel=\"noopener\">Transformers are Graph Neural Networks</a>, a great blog post by Chaitanya Joshi.</li>\n</ul>\n<h2 id=\"3-Transformer-with-Relative-Attention\"><a href=\"#3-Transformer-with-Relative-Attention\" class=\"headerlink\" title=\"3 - Transformer with Relative Attention\"></a>3 - Transformer with Relative Attention</h2><p>In my opinion, the Music Transformer paper is not only an application work, but its crux also includes <em>optimization</em> work on implementing Transformer with <strong>relative attention</strong>. We will delve into this part below.</p>\n<p>The essence of Transformer is on <strong>self-attention</strong>: for an output sequence of the Transformer, the elements on each position is a result of “attending to” (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$</p>\n<p>where \\(Q, K, V\\) represents the query, key and value tensors, each having tensor shape \\((l, d)\\), with \\(l\\) and \\(d\\) representing the sequence length and the number of dimensions used in the model respectively.</p>\n<p>In <a href=\"https://arxiv.org/pdf/1803.02155.pdf\" target=\"_blank\" rel=\"noopener\">Shaw et al.</a>, the concept of <strong>relative attention</strong> is proposed. The idea is to represent <strong>relative position representations</strong> more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$</p>\n<p>The difference is to learn an additional tensor \\(S^{rel}\\) of shape \\((l, l)\\). From the shape itself, we can see that the values \\(v_{ij}\\) in \\(S^{rel}\\) must be related to the relative distance of position \\(i\\) and \\(j\\) in length \\(l\\).<br>In fact, we can also view a sequence as a <strong>fully connected graph</strong> of its elements, hence \\(S^{rel}\\) can be seen as a tensor representing information about the <strong>edges</strong> in the graph.  </p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\n<p>How do we learn the values in \\(S^{rel}\\)? In Shaw et al., this is done by learning an extra weight tensor \\(R\\) of shape \\((l, l, d)\\) – only 1 dimension extra as compared to \\(S^{rel}\\). We can see it as having an <strong>embedding</strong> of dimension \\(d\\), at each position in a distance matrix of shape \\((l, l)\\). The values \\(R_{i, j}\\) means the embeddings which encode the relative position between position \\(i\\) from query tensor,  and position \\(j\\) from key tensor. Here, we only discuss cases of <strong>masked self-attention</strong> without looking forward – only positions where \\(i \\ge j\\) is concerned because the model only attends to nodes that occur at the current time step or before.</p>\n<p>After that, we reshape \\(Q\\) into a \\((l, 1, d)\\) tensor, and multiply by $$S^{rel} = QR^T$$</p>\n<p>However, this incurs a total space complexity of \\(O(L^2 D)\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to “perform some algorithmic tricks” in order to obtain \\(S^{rel}\\) in space complexity of \\(O(LD)\\).</p>\n<h2 id=\"4-Relative-Global-Attention\"><a href=\"#4-Relative-Global-Attention\" class=\"headerlink\" title=\"4 - Relative Global Attention\"></a>4 - Relative Global Attention</h2><p>In Music Transformer, the number of unique embeddings in \\(E^r\\) is assumed to be fixed. They used \\(l\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\(l - 1\\), from first position to the \\(l^{th}\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the “edge” between 1st and 3rd element, is the same as the embedding for the “edge” between 2nd and 4th element. </p>\n<p>If we follow the assumptions made above, we can observe that the desired \\(S_{rel}\\) can actually be calculated from multiplying \\(Q{E^{r}}^T\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\(S_{rel}\\) from \\(Q{E^{r}}^T\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\n<p>The bottom row in Figure 2 is known as the “skewing” procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\(M_\\textrm{masked}\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\(S_{rel}\\).</p>\n<p>Hence, without using the gigantic \\(R\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\(O(LD)\\) space complexity. Although both implementations result in \\(O(L^{2}D)\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.</p>\n<h2 id=\"4-Relative-Local-Attention\"><a href=\"#4-Relative-Local-Attention\" class=\"headerlink\" title=\"4 - Relative Local Attention\"></a>4 - Relative Local Attention</h2><p><strong>Local attention</strong>, first coined in <a href=\"https://arxiv.org/pdf/1508.04025\" target=\"_blank\" rel=\"noopener\">Luong et al</a> and <a href=\"https://arxiv.org/pdf/1802.05751.pdf\" target=\"_blank\" rel=\"noopener\">Image Transformer</a>, means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens <em>nearby</em> at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several <em>non-overlapping</em> blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a <em>memory</em> block which includes the current block and a finite number of tokens before the block).</p>\n<p>For simplicity, let’s start with an example of dividing the input sequence of 6 tokens into 2 blocks – Block 1 \\([x_1, x_2, x_3]\\), and Block 2 \\([x_4, x_5, x_6]\\). Hence, the attention components include: Block 1 \\(\\leftrightarrow\\) Block 1, Block 2 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2, as shown in Figure 2.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\n<p>The difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\(\\leftrightarrow\\) Block 1 as in global attention.</p>\n<p>So, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\(S_{rel}\\) which can capture relative position information, which similarly we can obtain the values from \\(Q{E^r}^T\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\(S_{rel}\\) look like for 2 blocks:</p>\n<figure>\n  <img style=\"width:40%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\n<p>From the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\(S_{rel}\\) output for Block 1 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and “skewing” procedure can be used. The only thing we need to change is the unmasked \\(S_{rel}\\) output for Block 2 \\(\\leftrightarrow\\) Block 1, which will be the focus of discussion below.</p>\n<br/>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\n<p>We denote \\(N\\) to be the block length, hence in our simple example \\(N = 6 \\div 2 = 3\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\(E^r\\), only that this time we collect indices from \\(1\\) to \\(2N-1\\) (if we notice in the desired \\(S_{rel}\\), these are the involved embeddings). This index range \\([1:2N -1]\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\(Q{E^r}^T_{1:2N-1}\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.</p>\n<p>One thing to note is that the discussion above omits the head indices – in actual implementation, one can choose to use the same \\(S_{rel}\\) for each head, or learn separate \\(S_{rel}\\) for different heads.</p>\n<p>Note that this technique can be extended to larger numbers of divided blocks – generally, self-attending blocks can used the “skewing” procedure from relative global attention; non-self-attending blocks should use the latter unmasked “skewing” procedure. By this, we can succesfully compute \\(QK^T + S_{rel}\\), with both shapes \\((N, N)\\), for each local attention procedure.</p>\n<h2 id=\"5-Results\"><a href=\"#5-Results\" class=\"headerlink\" title=\"5 - Results\"></a>5 - Results</h2><p>Finally, we take a quick glance on the results and comparisons made by the paper.</p>\n<figure>\n  <img style=\"width:70%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\n<p>In general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.</p>\n<p>The main takeway from this work is clear: <strong>relative information</strong> does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>), so we could probably foresee comparisons on different Transformer architectures for music generation in future.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>The Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:</p>\n<ol>\n<li><a href=\"https://github.com/magenta/magenta/tree/master/magenta/models/score2perf\" target=\"_blank\" rel=\"noopener\">The Main Script</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934\" target=\"_blank\" rel=\"noopener\">Relative Global Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964\" target=\"_blank\" rel=\"noopener\">Relative Local Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830\" target=\"_blank\" rel=\"noopener\">Skewing for Masked Cases (Global)</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813\" target=\"_blank\" rel=\"noopener\">Skewing for Unmasked Cases (Local)</a></li>\n</ol>\n<p>The code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both <a href=\"https://github.com/jason9693/MusicTransformer-tensorflow2.0\" target=\"_blank\" rel=\"noopener\">Tensorflow</a> and <a href=\"https://github.com/jason9693/MusicTransformer-pytorch/\" target=\"_blank\" rel=\"noopener\">PyTorch</a>, and I definitely recommend to check them out because it really helped me a lot on understanding the paper.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Concepts discussed in the Music Transformer paper<br>2 - Background of Relative Attention, Relative Global Attention, Relative Local Attention<br>3 - Ideas in the paper to efficiently implement the above attention mechanisms<br>4 - Results on music generation</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>I personally suffer from a lot of pain points when I first try to understand the Music Transformer paper, especially the details in this paper about relative attention, local attention, and also the “skewing” procedure. To facilitate the understanding of Music Transformer, I decide to re-draw some of the diagrams, and explain each step in the model in a more detailed manner. I hope that this post could help more people understand this important work in music generation.</p>\n<h2 id=\"2-Background-of-the-Music-Transformer\"><a href=\"#2-Background-of-the-Music-Transformer\" class=\"headerlink\" title=\"2 - Background of the Music Transformer\"></a>2 - Background of the Music Transformer</h2><p>The <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a> paper, authored by Huang et al. from Google Magenta, proposed a state-of-the-art language-model based music generation architecture. It is one of the first works that introduce Transformers, which gained tremendous success in the NLP field, to the symbolic music generation domain. </p>\n<p>The idea is as follows: by modelling each <strong>performance event</strong> as a token (as proposed in <a href=\"https://arxiv.org/pdf/1808.03715\" target=\"_blank\" rel=\"noopener\">Oore et al.</a>), we can treat each performance event like a word token in a sentence. Hence, we are able to learn the relationships between each performance event through self attention. If we train the model in an autoregressive manner, the model would also be able to learn to generate music , similar to training language models. From the <a href=\"https://magenta.tensorflow.org/music-transformer\" target=\"_blank\" rel=\"noopener\">demo samples</a> provided by Magenta, it has been shown that Music Transformer is capable of generating minute-long piano music with promising quality, moreover exhibiting long-term structure.</p>\n<p>Some related work of using Transformer architecture on generating music include <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a> (from OpenAI), and also <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>. It is evident that the Transformer architecture would be the backbone of music generation models in future research.</p>\n<p>For the basics of Transformers, we refer the readers who are interested to these links: </p>\n<ul>\n<li><a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\" rel=\"noopener\">The Annotated Transformer</a> from Harvard NLP;</li>\n<li><a href=\"https://www.youtube.com/watch?v=OYygPG4d9H0\" target=\"_blank\" rel=\"noopener\">Understanding and Applying Self-Attention for NLP Video</a>;</li>\n<li><a href=\"https://www.youtube.com/watch?v=ugWDIIOHtPA\" target=\"_blank\" rel=\"noopener\">Transformer Video by Hung-yi Lee</a>, Mandarin version;</li>\n<li><a href=\"https://graphdeeplearning.github.io/post/transformers-are-gnns/\" target=\"_blank\" rel=\"noopener\">Transformers are Graph Neural Networks</a>, a great blog post by Chaitanya Joshi.</li>\n</ul>\n<h2 id=\"3-Transformer-with-Relative-Attention\"><a href=\"#3-Transformer-with-Relative-Attention\" class=\"headerlink\" title=\"3 - Transformer with Relative Attention\"></a>3 - Transformer with Relative Attention</h2><p>In my opinion, the Music Transformer paper is not only an application work, but its crux also includes <em>optimization</em> work on implementing Transformer with <strong>relative attention</strong>. We will delve into this part below.</p>\n<p>The essence of Transformer is on <strong>self-attention</strong>: for an output sequence of the Transformer, the elements on each position is a result of “attending to” (weighted sum of, in math terms) the elements on each position in the input sequence. This is done by the following equation:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$$</p>\n<p>where \\(Q, K, V\\) represents the query, key and value tensors, each having tensor shape \\((l, d)\\), with \\(l\\) and \\(d\\) representing the sequence length and the number of dimensions used in the model respectively.</p>\n<p>In <a href=\"https://arxiv.org/pdf/1803.02155.pdf\" target=\"_blank\" rel=\"noopener\">Shaw et al.</a>, the concept of <strong>relative attention</strong> is proposed. The idea is to represent <strong>relative position representations</strong> more efficiently, to allow attention to be informed by how far two positions are apart in a sequence. This is an important factor in music as learning relative position representations help to capture structure information such as repeated motifs, call-and-response, and also scales and arpeggio patterns. Shaw et al. modified the attention equation as below:<br>$$Attention(Q, K, V) = softmax(\\frac{QK^T + S^{rel}}{\\sqrt{d}})V$$</p>\n<p>The difference is to learn an additional tensor \\(S^{rel}\\) of shape \\((l, l)\\). From the shape itself, we can see that the values \\(v_{ij}\\) in \\(S^{rel}\\) must be related to the relative distance of position \\(i\\) and \\(j\\) in length \\(l\\).<br>In fact, we can also view a sequence as a <strong>fully connected graph</strong> of its elements, hence \\(S^{rel}\\) can be seen as a tensor representing information about the <strong>edges</strong> in the graph.  </p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Relative attention in <a src=\"https://arxiv.org/pdf/1803.02155.pdf\">Shaw et al.</a>. A sequence of 3 tokens is represented as a fully connected, backward directed graph, because commonly each node only attends the current steps and before.</figcaption>\n</figure>\n<br/>\n\n<p>How do we learn the values in \\(S^{rel}\\)? In Shaw et al., this is done by learning an extra weight tensor \\(R\\) of shape \\((l, l, d)\\) – only 1 dimension extra as compared to \\(S^{rel}\\). We can see it as having an <strong>embedding</strong> of dimension \\(d\\), at each position in a distance matrix of shape \\((l, l)\\). The values \\(R_{i, j}\\) means the embeddings which encode the relative position between position \\(i\\) from query tensor,  and position \\(j\\) from key tensor. Here, we only discuss cases of <strong>masked self-attention</strong> without looking forward – only positions where \\(i \\ge j\\) is concerned because the model only attends to nodes that occur at the current time step or before.</p>\n<p>After that, we reshape \\(Q\\) into a \\((l, 1, d)\\) tensor, and multiply by $$S^{rel} = QR^T$$</p>\n<p>However, this incurs a total space complexity of \\(O(L^2 D)\\), restricting its application to long sequences. Especially in the context of music, minute-long segments can result in thousands of performance event tokens, due to the granularity of tokens used. Hence, Music Transformer proposes to “perform some algorithmic tricks” in order to obtain \\(S^{rel}\\) in space complexity of \\(O(LD)\\).</p>\n<h2 id=\"4-Relative-Global-Attention\"><a href=\"#4-Relative-Global-Attention\" class=\"headerlink\" title=\"4 - Relative Global Attention\"></a>4 - Relative Global Attention</h2><p>In Music Transformer, the number of unique embeddings in \\(E^r\\) is assumed to be fixed. They used \\(l\\) unique embeddings, ranging from the embedding for the furthest distance (which is \\(l - 1\\), from first position to the \\(l^{th}\\) position), to the embedding for the shortest distance (which is 0). Hence, the embedding for the “edge” between 1st and 3rd element, is the same as the embedding for the “edge” between 2nd and 4th element. </p>\n<p>If we follow the assumptions made above, we can observe that the desired \\(S_{rel}\\) can actually be calculated from multiplying \\(Q{E^{r}}^T\\), only that further steps of processing are needed. From Figure 2 we can see that via some position rearrangement, we can get values of \\(S_{rel}\\) from \\(Q{E^{r}}^T\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/new-relative-attention.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative attention procedure by Music Transformer.</figcaption>\n</figure>\n\n<p>The bottom row in Figure 2 is known as the “skewing” procedure for Music Transformer. Specifically, it involves reshaping a left-padded \\(M_\\textrm{masked}\\) and slice out the padded parts. The reshape function follows row-major ordering, which eventually rearranges each element to the right places which yields \\(S_{rel}\\).</p>\n<p>Hence, without using the gigantic \\(R\\) tensor, and following the abovestated assumptions, a Transformer with relative attention only requires \\(O(LD)\\) space complexity. Although both implementations result in \\(O(L^{2}D)\\) in terms of time complexity, it is reported that Music Transformer implementation can run 6x faster.</p>\n<h2 id=\"4-Relative-Local-Attention\"><a href=\"#4-Relative-Local-Attention\" class=\"headerlink\" title=\"4 - Relative Local Attention\"></a>4 - Relative Local Attention</h2><p><strong>Local attention</strong>, first coined in <a href=\"https://arxiv.org/pdf/1508.04025\" target=\"_blank\" rel=\"noopener\">Luong et al</a> and <a href=\"https://arxiv.org/pdf/1802.05751.pdf\" target=\"_blank\" rel=\"noopener\">Image Transformer</a>, means that instead of attending to every single token (which can be time costly and inefficient for extremely long sequences), the model only attends to tokens <em>nearby</em> at each time step. One way of implementation adopted in Image Transformer, which is also similar to the method discussed in Music Transformer, is the following: firstly, we divide the input sequence into several <em>non-overlapping</em> blocks. Then, each block can only attent to itself, and the block exactly before the current block (in Image Transformer, each block can attent to a <em>memory</em> block which includes the current block and a finite number of tokens before the block).</p>\n<p>For simplicity, let’s start with an example of dividing the input sequence of 6 tokens into 2 blocks – Block 1 \\([x_1, x_2, x_3]\\), and Block 2 \\([x_4, x_5, x_6]\\). Hence, the attention components include: Block 1 \\(\\leftrightarrow\\) Block 1, Block 2 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2, as shown in Figure 2.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-2.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Relative local attention for a 6-token sequence, divided into 2 blocks .</figcaption>\n</figure>\n\n<p>The difference as compared to global attention can be observed when we have 3 blocks (and more). Then, if we set the local window to only include the current block and the previous block, then we no longer attent Block 3 \\(\\leftrightarrow\\) Block 1 as in global attention.</p>\n<p>So, how can relative attention be introduced in local attention? Similarly, we want to include a matrix \\(S_{rel}\\) which can capture relative position information, which similarly we can obtain the values from \\(Q{E^r}^T\\) only with several different operations. To understand how should the operations be changed, we first see how does the desired \\(S_{rel}\\) look like for 2 blocks:</p>\n<figure>\n  <img style=\"width:40%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/relative-local-attention-srel.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Desired relative position matrix .</figcaption>\n</figure>\n\n<p>From the above figure, we can analyze it according to 4 different factions of divided blocks. We can see that \\(S_{rel}\\) output for Block 1 \\(\\leftrightarrow\\) Block 1, and Block 2 \\(\\leftrightarrow\\) Block 2 yields similar matrix as in relative global attention, hence the same reshaping and “skewing” procedure can be used. The only thing we need to change is the unmasked \\(S_{rel}\\) output for Block 2 \\(\\leftrightarrow\\) Block 1, which will be the focus of discussion below.</p>\n<br/>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/relative-local-attention-unmasked.png\" alt=\"\"/>\n  <figcaption><br/>Figure 4: \"Skewing\" procedure for the unmasked part.</figcaption>\n</figure>\n\n<p>We denote \\(N\\) to be the block length, hence in our simple example \\(N = 6 \\div 2 = 3\\). Following Figure 4, we can understand the changed procedure like this: first we gather the unique embeddings from \\(E^r\\), only that this time we collect indices from \\(1\\) to \\(2N-1\\) (if we notice in the desired \\(S_{rel}\\), these are the involved embeddings). This index range \\([1:2N -1]\\) is derived under the setting of attending only to the current and previous block. We similarly multiply \\(Q{E^r}^T_{1:2N-1}\\), and this time we apply top-right and bottom left masks. Then, we pad one rightmost column, flatten to 1D, and reshape via row-major ordering. The desired position will be obtained after slicing.</p>\n<p>One thing to note is that the discussion above omits the head indices – in actual implementation, one can choose to use the same \\(S_{rel}\\) for each head, or learn separate \\(S_{rel}\\) for different heads.</p>\n<p>Note that this technique can be extended to larger numbers of divided blocks – generally, self-attending blocks can used the “skewing” procedure from relative global attention; non-self-attending blocks should use the latter unmasked “skewing” procedure. By this, we can succesfully compute \\(QK^T + S_{rel}\\), with both shapes \\((N, N)\\), for each local attention procedure.</p>\n<h2 id=\"5-Results\"><a href=\"#5-Results\" class=\"headerlink\" title=\"5 - Results\"></a>5 - Results</h2><p>Finally, we take a quick glance on the results and comparisons made by the paper.</p>\n<figure>\n  <img style=\"width:70%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/music-transformer-results.png\" alt=\"\"/>\n  <figcaption><br/>Figure 5: Results in Music Transformer.</figcaption>\n</figure>\n\n<p>In general, relative attention achieves a better NLL loss as compared to vanilla Transformers and other architectures. Moreover, with more timing and relational information, it is evident that the results improved, and the authors also show that Music Transformer generates more coherent music with longer term of temporal structure. It seems like local attention does not really improve the results from global attention.</p>\n<p>The main takeway from this work is clear: <strong>relative information</strong> does matter for music generation, and using relative attention provides more inductive bias for the model to learn these information, which corresponds to more evident musical structure and coherency. There are several attempts of using other Transformer variants (e.g. Transformer-XL in <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a>), so we could probably foresee comparisons on different Transformer architectures for music generation in future.</p>\n<h2 id=\"6-Code-Implementation\"><a href=\"#6-Code-Implementation\" class=\"headerlink\" title=\"6 - Code Implementation\"></a>6 - Code Implementation</h2><p>The Tensorflow code of Music Transformer is open-sourced. Here, I link several portals that resembles with the important parts within the Music Transformer architecture:</p>\n<ol>\n<li><a href=\"https://github.com/magenta/magenta/tree/master/magenta/models/score2perf\" target=\"_blank\" rel=\"noopener\">The Main Script</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1934\" target=\"_blank\" rel=\"noopener\">Relative Global Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py#L2964\" target=\"_blank\" rel=\"noopener\">Relative Local Attention</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L1830\" target=\"_blank\" rel=\"noopener\">Skewing for Masked Cases (Global)</a></li>\n<li><a href=\"https://github.com/tensorflow/tensor2tensor/blob/05f02d8942c1f4a48ad5ee54553e446710658ae7/tensor2tensor/layers/common_attention.py#L2813\" target=\"_blank\" rel=\"noopener\">Skewing for Unmasked Cases (Local)</a></li>\n</ol>\n<p>The code is written using the Tensor2Tensor framework, so it might need some additional effort to set up the environment for execution. Ki-Chang Yang has implemented a very complete unofficial version in both <a href=\"https://github.com/jason9693/MusicTransformer-tensorflow2.0\" target=\"_blank\" rel=\"noopener\">Tensorflow</a> and <a href=\"https://github.com/jason9693/MusicTransformer-pytorch/\" target=\"_blank\" rel=\"noopener\">PyTorch</a>, and I definitely recommend to check them out because it really helped me a lot on understanding the paper.</p>\n"},{"title":"Semi-Supervised Learning for Music Modelling","date":"2020-05-13T10:23:35.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Motivation of using semi-supervised learning in music modelling\n2 - Two SSL frameworks based on latent generative models - **Kingma et al** and **VaDE**\n3 - Applications of these frameworks on music-related tasks\n<br/>\n\n## 1 - Introduction\n\nIn a [previous post](/2020/01/26/vae-symbolic-music/), we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both **inference** and **generation** capabilities. Furthermore, by using extra techniques such as **disentanglement**, **latent regularization**, or using a more complex prior such as **Gaussian mixture model**, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.\n\nIn this post, we introduce the application of **semi-supervised learning (SSL)**, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times **we do not have enough labelled data** for all kinds of reasons -- annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.\n\n## 2 - Why Semi-Supervised Learning?\n\nThe strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal & valence, style, etc.), we can often observe two scenarios: (i) either the **amount of labels is too little**, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes **too noisy and deviated**, due to the subjective nature of these annotations, which hinders the model from learning good representations. \n\nTherefore, one of the solutions is to introduce SSL -- we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of *quality* labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of **representation learning** because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.\n\n## 3 - Applying SSL to Deep Learning Models\n\n### SSL using Deep Generative Models\nWe start from one of the earliest papers that discuss SSL in deep learning models. In [Kingma et al.](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf) the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.\n\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\nThe generation components can be understood as how each model assumes each data point to be generated. \\\\(\\textrm{M1}\\\\) resembles the idea of **latent variable models**, where a data point is generated from a latent prior, and further being projected to the observation space. \\\\(\\textrm{M2}\\\\) is simply two strands of \\\\(\\textrm{M1}\\\\) -- one on the discrete class variable \\\\(\\textbf{y}\\\\), and the other on the continuous latent \\\\(\\textbf{z}\\\\). \\\\(\\textrm{M2}\\\\) can also be viewed as a **disentanglement** model, if we understand it as learning separate spaces for labels in \\\\(\\textbf{y}\\\\), and residual information in \\\\(\\textbf{z}\\\\) (e.g. writing styles in MNIST). \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is generally a hierachical combination of both.\n\nOn the other hand, all exact posterior \\\\(p(\\textbf{z} | \\textbf{X})\\\\) are approximated using variational inference by introducing a new distribution \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\). The posterior can also be called the **inference** component, as we are **inferring** the latent distributions from the observations. \n\nThe posterior for \\\\(\\textrm{M1}\\\\) is evident to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\), and the model employs a separate classifier (e.g. an SVM) to predict \\\\(\\textbf{y}\\\\) from the low-dimension manifold \\\\(\\textbf{z}\\\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\\\(\\textrm{M2}\\\\), the authors parameterized the posterior to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\\\), which the class labels are inferred directly from \\\\(\\textbf{X}\\\\) using a separate Gaussian inference network.\n\nSo, how does the objective function look like if we want to train the model in a semi-supervised manner?\n\nFor \\\\(\\textrm{M1}\\\\), we are basically training a VAE, so the objective function is:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.\n\nFor \\\\(\\textrm{M2}\\\\), we need to consider two cases: if label is present (*supervised*), then the objective function is very similar to the VAE objective function, other than an additional given \\\\(\\textbf{y}\\\\):\n$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (*unsupervised*), then we **marginalize** over all possibilities of class labels as below:\n\n$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ \n\nwhere the additional **entropy** term \\\\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\\\(q_\\phi(y|\\textbf{X})\\\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\\\(\\phi\\\\) and \\\\(\\theta\\\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.\n\n### Variational Deep Embedding (VaDE)\n\n<figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n[VaDE](https://arxiv.org/pdf/1611.05148.pdf) employs the idea of **unsupervised and generative approach on clustering**. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\\\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\\\) for the inference component. One can relate this to discrete representation learning using VAE with a **Gaussian mixture prior** -- after inferring the latent variable \\\\(\\textbf{z}\\\\), the variable is assigned to a particular cluster with index \\\\(y\\\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\\\(z\\\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (*supervised*), the KL term is written as:\n\n$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$\n\nand when labels are not present (*unsupervised*), we similarly **marginalize** over all possibilities of class labels, as we have done for the \\\\(\\textrm{M2}\\\\) model before:\n$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$\n\n### Comparison\n\nHere, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are **latent variable models**, and make use of the **generative** approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to **marginalize** over all classes. In fact, if we look close at the inference component in \\\\(\\textrm{M1} + \\textrm{M2}\\\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\\\(y\\\\) and a standard Gaussian for \\\\(\\textbf{z}\\\\), whereas VaDE integrates both into a single mixture-of-Gaussians.\n\n## 4 - Applications\n\nThe SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both **discriminative** capability for analysis / feature extraction, and **generation** capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn **meaningful musical representations** from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.\n\nAn example discussed for music generation is by [Ferreira et al](http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf) on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from [Radford et al](https://arxiv.org/pdf/1704.01444.pdf) on generating reviews with sentiment. The model used is an \\\\(\\textrm{mLSTM}\\\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\\\(\\textrm{mLSTM}\\\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\\\(\\textrm{M1}\\\\), with the intermediate representation from \\\\(\\textrm{mLSTM}\\\\) as \\\\(\\textbf{z}\\\\), and a separate logistic regressor is used to predict \\\\(y\\\\) from \\\\(\\textbf{z}\\\\).\n\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\nAnother example is by [Luo et al](https://arxiv.org/pdf/1906.08152.pdf) on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional *disentanglement* added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both **discrete** and **continuous** representations for both pitch and timbre -- *discrete* representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the *continuous* representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.\n\nAnother two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the [Tacotron](https://google.github.io/tacotron/) team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve [hierarchical modelling](https://arxiv.org/pdf/1810.07217.pdf) and [semi-supervised learning](https://arxiv.org/pdf/1910.01709.pdf) for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.\n\n## Conclusion\n\nWith the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to \"self-supervisedly\" learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.\n\nFor the fundamental framework papers, please refer to the list below:\n1. [Semi-supervised Learning with Deep Generative Models](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf)\n2. [Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering](https://arxiv.org/pdf/1611.05148.pdf)\n3. [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf)\n\n\n","source":"_posts/semi-supervised-music.md","raw":"---\ntitle: Semi-Supervised Learning for Music Modelling\ndate: 2020-05-13 18:23:35\ntags:\n    - VAE\n    - Music Representation Learning\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Motivation of using semi-supervised learning in music modelling\n2 - Two SSL frameworks based on latent generative models - **Kingma et al** and **VaDE**\n3 - Applications of these frameworks on music-related tasks\n<br/>\n\n## 1 - Introduction\n\nIn a [previous post](/2020/01/26/vae-symbolic-music/), we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both **inference** and **generation** capabilities. Furthermore, by using extra techniques such as **disentanglement**, **latent regularization**, or using a more complex prior such as **Gaussian mixture model**, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.\n\nIn this post, we introduce the application of **semi-supervised learning (SSL)**, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times **we do not have enough labelled data** for all kinds of reasons -- annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.\n\n## 2 - Why Semi-Supervised Learning?\n\nThe strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal & valence, style, etc.), we can often observe two scenarios: (i) either the **amount of labels is too little**, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes **too noisy and deviated**, due to the subjective nature of these annotations, which hinders the model from learning good representations. \n\nTherefore, one of the solutions is to introduce SSL -- we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of *quality* labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of **representation learning** because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.\n\n## 3 - Applying SSL to Deep Learning Models\n\n### SSL using Deep Generative Models\nWe start from one of the earliest papers that discuss SSL in deep learning models. In [Kingma et al.](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf) the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.\n\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\nThe generation components can be understood as how each model assumes each data point to be generated. \\\\(\\textrm{M1}\\\\) resembles the idea of **latent variable models**, where a data point is generated from a latent prior, and further being projected to the observation space. \\\\(\\textrm{M2}\\\\) is simply two strands of \\\\(\\textrm{M1}\\\\) -- one on the discrete class variable \\\\(\\textbf{y}\\\\), and the other on the continuous latent \\\\(\\textbf{z}\\\\). \\\\(\\textrm{M2}\\\\) can also be viewed as a **disentanglement** model, if we understand it as learning separate spaces for labels in \\\\(\\textbf{y}\\\\), and residual information in \\\\(\\textbf{z}\\\\) (e.g. writing styles in MNIST). \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is generally a hierachical combination of both.\n\nOn the other hand, all exact posterior \\\\(p(\\textbf{z} | \\textbf{X})\\\\) are approximated using variational inference by introducing a new distribution \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\). The posterior can also be called the **inference** component, as we are **inferring** the latent distributions from the observations. \n\nThe posterior for \\\\(\\textrm{M1}\\\\) is evident to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\\\), and the model employs a separate classifier (e.g. an SVM) to predict \\\\(\\textbf{y}\\\\) from the low-dimension manifold \\\\(\\textbf{z}\\\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\\\(\\textrm{M2}\\\\), the authors parameterized the posterior to be \\\\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\\\), which the class labels are inferred directly from \\\\(\\textbf{X}\\\\) using a separate Gaussian inference network.\n\nSo, how does the objective function look like if we want to train the model in a semi-supervised manner?\n\nFor \\\\(\\textrm{M1}\\\\), we are basically training a VAE, so the objective function is:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.\n\nFor \\\\(\\textrm{M2}\\\\), we need to consider two cases: if label is present (*supervised*), then the objective function is very similar to the VAE objective function, other than an additional given \\\\(\\textbf{y}\\\\):\n$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (*unsupervised*), then we **marginalize** over all possibilities of class labels as below:\n\n$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ \n\nwhere the additional **entropy** term \\\\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\\\(q_\\phi(y|\\textbf{X})\\\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\\\(\\textrm{M1} + \\textrm{M2}\\\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\\\(\\phi\\\\) and \\\\(\\theta\\\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.\n\n### Variational Deep Embedding (VaDE)\n\n<figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n[VaDE](https://arxiv.org/pdf/1611.05148.pdf) employs the idea of **unsupervised and generative approach on clustering**. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\\\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\\\) for the inference component. One can relate this to discrete representation learning using VAE with a **Gaussian mixture prior** -- after inferring the latent variable \\\\(\\textbf{z}\\\\), the variable is assigned to a particular cluster with index \\\\(y\\\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:\n$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\\\(z\\\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (*supervised*), the KL term is written as:\n\n$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$\n\nand when labels are not present (*unsupervised*), we similarly **marginalize** over all possibilities of class labels, as we have done for the \\\\(\\textrm{M2}\\\\) model before:\n$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$\n\n### Comparison\n\nHere, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are **latent variable models**, and make use of the **generative** approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to **marginalize** over all classes. In fact, if we look close at the inference component in \\\\(\\textrm{M1} + \\textrm{M2}\\\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\\\(y\\\\) and a standard Gaussian for \\\\(\\textbf{z}\\\\), whereas VaDE integrates both into a single mixture-of-Gaussians.\n\n## 4 - Applications\n\nThe SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both **discriminative** capability for analysis / feature extraction, and **generation** capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn **meaningful musical representations** from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.\n\nAn example discussed for music generation is by [Ferreira et al](http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf) on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from [Radford et al](https://arxiv.org/pdf/1704.01444.pdf) on generating reviews with sentiment. The model used is an \\\\(\\textrm{mLSTM}\\\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\\\(\\textrm{mLSTM}\\\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\\\(\\textrm{M1}\\\\), with the intermediate representation from \\\\(\\textrm{mLSTM}\\\\) as \\\\(\\textbf{z}\\\\), and a separate logistic regressor is used to predict \\\\(y\\\\) from \\\\(\\textbf{z}\\\\).\n\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\nAnother example is by [Luo et al](https://arxiv.org/pdf/1906.08152.pdf) on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional *disentanglement* added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both **discrete** and **continuous** representations for both pitch and timbre -- *discrete* representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the *continuous* representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.\n\nAnother two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the [Tacotron](https://google.github.io/tacotron/) team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve [hierarchical modelling](https://arxiv.org/pdf/1810.07217.pdf) and [semi-supervised learning](https://arxiv.org/pdf/1910.01709.pdf) for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.\n\n## Conclusion\n\nWith the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to \"self-supervisedly\" learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.\n\nFor the fundamental framework papers, please refer to the list below:\n1. [Semi-supervised Learning with Deep Generative Models](https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf)\n2. [Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering](https://arxiv.org/pdf/1611.05148.pdf)\n3. [Learning Disentangled Representations with Semi-Supervised Deep Generative Models](https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf)\n\n\n","slug":"semi-supervised-music","published":1,"updated":"2020-10-16T17:38:19.470Z","_id":"ckbpvdoks0000qlm88ndz3z7a","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Motivation of using semi-supervised learning in music modelling<br>2 - Two SSL frameworks based on latent generative models - <strong>Kingma et al</strong> and <strong>VaDE</strong><br>3 - Applications of these frameworks on music-related tasks<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>In a <a href=\"/2020/01/26/vae-symbolic-music/\">previous post</a>, we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both <strong>inference</strong> and <strong>generation</strong> capabilities. Furthermore, by using extra techniques such as <strong>disentanglement</strong>, <strong>latent regularization</strong>, or using a more complex prior such as <strong>Gaussian mixture model</strong>, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.</p>\n<p>In this post, we introduce the application of <strong>semi-supervised learning (SSL)</strong>, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times <strong>we do not have enough labelled data</strong> for all kinds of reasons – annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.</p>\n<h2 id=\"2-Why-Semi-Supervised-Learning\"><a href=\"#2-Why-Semi-Supervised-Learning\" class=\"headerlink\" title=\"2 - Why Semi-Supervised Learning?\"></a>2 - Why Semi-Supervised Learning?</h2><p>The strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal &amp; valence, style, etc.), we can often observe two scenarios: (i) either the <strong>amount of labels is too little</strong>, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes <strong>too noisy and deviated</strong>, due to the subjective nature of these annotations, which hinders the model from learning good representations. </p>\n<p>Therefore, one of the solutions is to introduce SSL – we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of <em>quality</em> labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of <strong>representation learning</strong> because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.</p>\n<h2 id=\"3-Applying-SSL-to-Deep-Learning-Models\"><a href=\"#3-Applying-SSL-to-Deep-Learning-Models\" class=\"headerlink\" title=\"3 - Applying SSL to Deep Learning Models\"></a>3 - Applying SSL to Deep Learning Models</h2><h3 id=\"SSL-using-Deep-Generative-Models\"><a href=\"#SSL-using-Deep-Generative-Models\" class=\"headerlink\" title=\"SSL using Deep Generative Models\"></a>SSL using Deep Generative Models</h3><p>We start from one of the earliest papers that discuss SSL in deep learning models. In <a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Kingma et al.</a> the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.</p>\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\n<p>The generation components can be understood as how each model assumes each data point to be generated. \\(\\textrm{M1}\\) resembles the idea of <strong>latent variable models</strong>, where a data point is generated from a latent prior, and further being projected to the observation space. \\(\\textrm{M2}\\) is simply two strands of \\(\\textrm{M1}\\) – one on the discrete class variable \\(\\textbf{y}\\), and the other on the continuous latent \\(\\textbf{z}\\). \\(\\textrm{M2}\\) can also be viewed as a <strong>disentanglement</strong> model, if we understand it as learning separate spaces for labels in \\(\\textbf{y}\\), and residual information in \\(\\textbf{z}\\) (e.g. writing styles in MNIST). \\(\\textrm{M1} + \\textrm{M2}\\) is generally a hierachical combination of both.</p>\n<p>On the other hand, all exact posterior \\(p(\\textbf{z} | \\textbf{X})\\) are approximated using variational inference by introducing a new distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\). The posterior can also be called the <strong>inference</strong> component, as we are <strong>inferring</strong> the latent distributions from the observations. </p>\n<p>The posterior for \\(\\textrm{M1}\\) is evident to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\), and the model employs a separate classifier (e.g. an SVM) to predict \\(\\textbf{y}\\) from the low-dimension manifold \\(\\textbf{z}\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\(\\textrm{M2}\\), the authors parameterized the posterior to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\), which the class labels are inferred directly from \\(\\textbf{X}\\) using a separate Gaussian inference network.</p>\n<p>So, how does the objective function look like if we want to train the model in a semi-supervised manner?</p>\n<p>For \\(\\textrm{M1}\\), we are basically training a VAE, so the objective function is:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.</p>\n<p>For \\(\\textrm{M2}\\), we need to consider two cases: if label is present (<em>supervised</em>), then the objective function is very similar to the VAE objective function, other than an additional given \\(\\textbf{y}\\):<br>$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (<em>unsupervised</em>), then we <strong>marginalize</strong> over all possibilities of class labels as below:</p>\n<p>$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ </p>\n<p>where the additional <strong>entropy</strong> term \\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\(q_\\phi(y|\\textbf{X})\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\(\\textrm{M1} + \\textrm{M2}\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\(\\phi\\) and \\(\\theta\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.</p>\n<h3 id=\"Variational-Deep-Embedding-VaDE\"><a href=\"#Variational-Deep-Embedding-VaDE\" class=\"headerlink\" title=\"Variational Deep Embedding (VaDE)\"></a>Variational Deep Embedding (VaDE)</h3><figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n<p><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">VaDE</a> employs the idea of <strong>unsupervised and generative approach on clustering</strong>. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\) for the inference component. One can relate this to discrete representation learning using VAE with a <strong>Gaussian mixture prior</strong> – after inferring the latent variable \\(\\textbf{z}\\), the variable is assigned to a particular cluster with index \\(y\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\(z\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (<em>supervised</em>), the KL term is written as:</p>\n<p>$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$</p>\n<p>and when labels are not present (<em>unsupervised</em>), we similarly <strong>marginalize</strong> over all possibilities of class labels, as we have done for the \\(\\textrm{M2}\\) model before:<br>$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$</p>\n<h3 id=\"Comparison\"><a href=\"#Comparison\" class=\"headerlink\" title=\"Comparison\"></a>Comparison</h3><p>Here, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are <strong>latent variable models</strong>, and make use of the <strong>generative</strong> approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to <strong>marginalize</strong> over all classes. In fact, if we look close at the inference component in \\(\\textrm{M1} + \\textrm{M2}\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\(y\\) and a standard Gaussian for \\(\\textbf{z}\\), whereas VaDE integrates both into a single mixture-of-Gaussians.</p>\n<h2 id=\"4-Applications\"><a href=\"#4-Applications\" class=\"headerlink\" title=\"4 - Applications\"></a>4 - Applications</h2><p>The SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both <strong>discriminative</strong> capability for analysis / feature extraction, and <strong>generation</strong> capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn <strong>meaningful musical representations</strong> from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.</p>\n<p>An example discussed for music generation is by <a href=\"http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf\" target=\"_blank\" rel=\"noopener\">Ferreira et al</a> on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from <a href=\"https://arxiv.org/pdf/1704.01444.pdf\" target=\"_blank\" rel=\"noopener\">Radford et al</a> on generating reviews with sentiment. The model used is an \\(\\textrm{mLSTM}\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\(\\textrm{mLSTM}\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\(\\textrm{M1}\\), with the intermediate representation from \\(\\textrm{mLSTM}\\) as \\(\\textbf{z}\\), and a separate logistic regressor is used to predict \\(y\\) from \\(\\textbf{z}\\).</p>\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\n<p>Another example is by <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">Luo et al</a> on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional <em>disentanglement</em> added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both <strong>discrete</strong> and <strong>continuous</strong> representations for both pitch and timbre – <em>discrete</em> representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the <em>continuous</em> representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.</p>\n<p>Another two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the <a href=\"https://google.github.io/tacotron/\" target=\"_blank\" rel=\"noopener\">Tacotron</a> team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve <a href=\"https://arxiv.org/pdf/1810.07217.pdf\" target=\"_blank\" rel=\"noopener\">hierarchical modelling</a> and <a href=\"https://arxiv.org/pdf/1910.01709.pdf\" target=\"_blank\" rel=\"noopener\">semi-supervised learning</a> for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>With the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to “self-supervisedly” learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.</p>\n<p>For the fundamental framework papers, please refer to the list below:</p>\n<ol>\n<li><a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Semi-supervised Learning with Deep Generative Models</a></li>\n<li><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering</a></li>\n<li><a href=\"https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Motivation of using semi-supervised learning in music modelling<br>2 - Two SSL frameworks based on latent generative models - <strong>Kingma et al</strong> and <strong>VaDE</strong><br>3 - Applications of these frameworks on music-related tasks<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>In a <a href=\"/2020/01/26/vae-symbolic-music/\">previous post</a>, we have discussed the usage of the popular VAE framework in symbolic music modelling tasks (surely, the framework can also be adapted to all kinds of music-related tasks). We have also seen that after training, the model jointly learns both <strong>inference</strong> and <strong>generation</strong> capabilities. Furthermore, by using extra techniques such as <strong>disentanglement</strong>, <strong>latent regularization</strong>, or using a more complex prior such as <strong>Gaussian mixture model</strong>, we observe how one or many meaningful, controllable latent space(s) could be learnt to support various downstream creative applications such as style transfer, morphing, analysis, etc.</p>\n<p>In this post, we introduce the application of <strong>semi-supervised learning (SSL)</strong>, which is very compatible with the VAE framework as we will see, to music modelling tasks. The (arguably) biggest pain-point in the music domain is that often times <strong>we do not have enough labelled data</strong> for all kinds of reasons – annotation difficulties, copyright issues, noise and high variance in annotations due to its subjective nature, etc. So, it will be good if the model can learn desirable properties with only limited amount of quality data.</p>\n<h2 id=\"2-Why-Semi-Supervised-Learning\"><a href=\"#2-Why-Semi-Supervised-Learning\" class=\"headerlink\" title=\"2 - Why Semi-Supervised Learning?\"></a>2 - Why Semi-Supervised Learning?</h2><p>The strengths and importance of SSL is especially evident in the music domain in my opinion. In particular, for abstract musical concepts which the labels definitely need human annotations (e.g. mood tags, arousal &amp; valence, style, etc.), we can often observe two scenarios: (i) either the <strong>amount of labels is too little</strong>, which forbids the model to generalize well; or (ii)  when the amount of labels start to scale, it becomes <strong>too noisy and deviated</strong>, due to the subjective nature of these annotations, which hinders the model from learning good representations. </p>\n<p>Therefore, one of the solutions is to introduce SSL – we leverage the abundant amount of unlabelled data to learn common music representations, e.g. note, pitch, structure, etc., and we use only a very small set of <em>quality</em> labels (i.e. labels which are further filtered) to learn the desired abstract property. This further relates to the task of <strong>representation learning</strong> because we need to be able to learn reusable, high quality representations with only a small amount of labelled data in order achieve good results.</p>\n<h2 id=\"3-Applying-SSL-to-Deep-Learning-Models\"><a href=\"#3-Applying-SSL-to-Deep-Learning-Models\" class=\"headerlink\" title=\"3 - Applying SSL to Deep Learning Models\"></a>3 - Applying SSL to Deep Learning Models</h2><h3 id=\"SSL-using-Deep-Generative-Models\"><a href=\"#SSL-using-Deep-Generative-Models\" class=\"headerlink\" title=\"SSL using Deep Generative Models\"></a>SSL using Deep Generative Models</h3><p>We start from one of the earliest papers that discuss SSL in deep learning models. In <a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Kingma et al.</a> the authors proposed a framework of using deep generative models for SSL, with graphical models as illustrated in Figure 1.</p>\n<figure>\n  <img style=\"width:108%;\" src=\"/img/kingma-ssl-3.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Graphical model of 3 formulations proposed in Kingma et al.</figcaption>\n</figure>\n\n<p>The generation components can be understood as how each model assumes each data point to be generated. \\(\\textrm{M1}\\) resembles the idea of <strong>latent variable models</strong>, where a data point is generated from a latent prior, and further being projected to the observation space. \\(\\textrm{M2}\\) is simply two strands of \\(\\textrm{M1}\\) – one on the discrete class variable \\(\\textbf{y}\\), and the other on the continuous latent \\(\\textbf{z}\\). \\(\\textrm{M2}\\) can also be viewed as a <strong>disentanglement</strong> model, if we understand it as learning separate spaces for labels in \\(\\textbf{y}\\), and residual information in \\(\\textbf{z}\\) (e.g. writing styles in MNIST). \\(\\textrm{M1} + \\textrm{M2}\\) is generally a hierachical combination of both.</p>\n<p>On the other hand, all exact posterior \\(p(\\textbf{z} | \\textbf{X})\\) are approximated using variational inference by introducing a new distribution \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\). The posterior can also be called the <strong>inference</strong> component, as we are <strong>inferring</strong> the latent distributions from the observations. </p>\n<p>The posterior for \\(\\textrm{M1}\\) is evident to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X})\\), and the model employs a separate classifier (e.g. an SVM) to predict \\(\\textbf{y}\\) from the low-dimension manifold \\(\\textbf{z}\\), which could be encoded with more meaningful representations and yields better classification accuracy. For \\(\\textrm{M2}\\), the authors parameterized the posterior to be \\(q_{\\phi}(\\textbf{z} | \\textbf{X}, \\textbf{y}) = q_{\\phi}(\\textbf{z} | \\textbf{X}) \\cdot q_{\\phi}(\\textbf{y} | \\textbf{X})\\), which the class labels are inferred directly from \\(\\textbf{X}\\) using a separate Gaussian inference network.</p>\n<p>So, how does the objective function look like if we want to train the model in a semi-supervised manner?</p>\n<p>For \\(\\textrm{M1}\\), we are basically training a VAE, so the objective function is:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}))$$ Additionally, the label classifier is trained separated on only labelled data. Hence, the posterior learnt will serve as a feature extractor used to train the label classifier.</p>\n<p>For \\(\\textrm{M2}\\), we need to consider two cases: if label is present (<em>supervised</em>), then the objective function is very similar to the VAE objective function, other than an additional given \\(\\textbf{y}\\):<br>$$\\mathcal{L(\\textbf{X}, y)} = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)} [ \\log p_\\theta(\\textbf{X}|\\textbf{z}, y) + \\log p_\\theta(y) + \\log p(\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{X}, y)] \\\\ = E_{\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{X}, y)}[\\log p_\\theta(\\textbf{X}|\\textbf{z}, y)] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}))$$ If label is not present (<em>unsupervised</em>), then we <strong>marginalize</strong> over all possibilities of class labels as below:</p>\n<p>$$\\mathcal{U(\\textbf{X})} = \\displaystyle\\sum_{y} q_\\phi(y | \\textbf{X}) \\cdot [ \\mathcal{L(\\textbf{X}, y)} - \\mathcal{D}_{KL}(q_\\phi(y|\\textbf{X}) || p(y)) ] \\\\ = \\displaystyle \\sum_y q_\\phi(y | \\textbf{X}) \\cdot \\mathcal{L(\\textbf{X}, y)} + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$ </p>\n<p>where the additional <strong>entropy</strong> term \\(\\mathcal{H}(q_\\phi(y|\\textbf{X}))\\) pushes the distribution to conform to a multinomial prior distribution. Additionally, to improve the classification capability of \\(q_\\phi(y|\\textbf{X})\\), a classification loss (e.g. cross-entropy loss) can be added during the supervised scenario. The extension to \\(\\textrm{M1} + \\textrm{M2}\\) is then straigtforward by combining the loss terms of both models. All inference and generation parameters, \\(\\phi\\) and \\(\\theta\\), are parameterized using neural networks, with some popular choices in the music domain like 1D or 2D CNNs, RNNs, attention networks etc.</p>\n<h3 id=\"Variational-Deep-Embedding-VaDE\"><a href=\"#Variational-Deep-Embedding-VaDE\" class=\"headerlink\" title=\"Variational Deep Embedding (VaDE)\"></a>Variational Deep Embedding (VaDE)</h3><figure>\n  <img style=\"width:50%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/vade-ssl.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Graphical model of VaDE.</figcaption>\n</figure>\n\n<p><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">VaDE</a> employs the idea of <strong>unsupervised and generative approach on clustering</strong>. Hence as shown in Figure 2, the graphical model is a hierachical structure from \\(\\textbf{X} \\rightarrow \\textbf{z} \\rightarrow y\\) for the inference component. One can relate this to discrete representation learning using VAE with a <strong>Gaussian mixture prior</strong> – after inferring the latent variable \\(\\textbf{z}\\), the variable is assigned to a particular cluster with index \\(y\\). Hence, it is straightforward that the objective function is the ELBO extended to a mixture-of-Gaussian scenario:<br>$$E_{\\textbf{z}\\sim q_{\\phi} (\\textbf{z}, y|\\textbf{X})}[\\log p_\\theta(\\textbf{X}|\\textbf{z})] - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}, y| \\textbf{X}) || p(\\textbf{z}, y))$$ The second KL term regularizes the latent embedding \\(z\\) to lie on the mixture-of-Gaussians manifold. Similarly, we can introduce both supervised and unsupervised scenario in this case: when labels are present (<em>supervised</em>), the KL term is written as:</p>\n<p>$$ - \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}, y) || p(\\textbf{z}|y))$$</p>\n<p>and when labels are not present (<em>unsupervised</em>), we similarly <strong>marginalize</strong> over all possibilities of class labels, as we have done for the \\(\\textrm{M2}\\) model before:<br>$$ - \\displaystyle \\sum_y q_\\phi(y|\\textbf{X}) \\cdot \\mathcal{D}_{KL}(q_\\phi(\\textbf{z}|\\textbf{X}) || p(\\textbf{z}|y)) + \\mathcal{H}(q_\\phi(y|\\textbf{X}))$$</p>\n<h3 id=\"Comparison\"><a href=\"#Comparison\" class=\"headerlink\" title=\"Comparison\"></a>Comparison</h3><p>Here, we can see that both frameworks by Kingma et al. and VaDE share a lot of similarities. Firstly, both frameworks are <strong>latent variable models</strong>, and make use of the <strong>generative</strong> approach. To achieve semi-supervised capabilities, both frameworks adopt the strategy to <strong>marginalize</strong> over all classes. In fact, if we look close at the inference component in \\(\\textrm{M1} + \\textrm{M2}\\), the left strand actually resembles the inference graphical model of VaDE. The main difference in both frameworks lie in the prior distribution. Kingma et al. model 2 separate distributions, which is a multinomial distribution for \\(y\\) and a standard Gaussian for \\(\\textbf{z}\\), whereas VaDE integrates both into a single mixture-of-Gaussians.</p>\n<h2 id=\"4-Applications\"><a href=\"#4-Applications\" class=\"headerlink\" title=\"4 - Applications\"></a>4 - Applications</h2><p>The SSL frameworks above are suitable to be applied in music domain for two reasons: firstly, by training the model we can get both <strong>discriminative</strong> capability for analysis / feature extraction, and <strong>generation</strong> capability for all kinds of creative synthesis. Secondly, we can rely on the generation component to learn <strong>meaningful musical representations</strong> from unlabelled data. Through training the model to generate outputs that are similar to the data distribution, we want the model to learn useful, reusable musical features which can be easily regularized or separated by leveraging only a small amount of labels.</p>\n<p>An example discussed for music generation is by <a href=\"http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf\" target=\"_blank\" rel=\"noopener\">Ferreira et al</a> on generating music with sentiment. Obviously, the amount of unlabelled music is massive, and sentiment-labelled data is extremely scarce. The authors adopted the model from <a href=\"https://arxiv.org/pdf/1704.01444.pdf\" target=\"_blank\" rel=\"noopener\">Radford et al</a> on generating reviews with sentiment. The model used is an \\(\\textrm{mLSTM}\\) which takes in the previous tokens as input, and is trained to predict the next token in an autoregressive manner. The intermediate representation from \\(\\textrm{mLSTM}\\) are used for sentiment classification. Thi model can actually be interpreted as a variant of \\(\\textrm{M1}\\), with the intermediate representation from \\(\\textrm{mLSTM}\\) as \\(\\textbf{z}\\), and a separate logistic regressor is used to predict \\(y\\) from \\(\\textbf{z}\\).</p>\n<figure>\n  <img style=\"width:80%; display: block; margin-left: auto; margin-right: auto;\" src=\"/img/radford-sentiment.png\" alt=\"\"/>\n  <figcaption><br/>Figure 3: Sentiment fine-tuning on mLSTM by Ferreira et al.</figcaption>\n</figure>\n\n<p>Another example is by <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">Luo et al</a> on disentangling pitch and timbre for audio recordings on playing single notes. The model proposed basically resembles with VaDE, with an additional <em>disentanglement</em> added to learn separate spaces for pitch and timbre. The authors studied the results of pitch and timbre classification by using increasing amount of labelled data. An additional advantage demonstrated is that we can learnt both <strong>discrete</strong> and <strong>continuous</strong> representations for both pitch and timbre – <em>discrete</em> representations are intuitive for analysis, as pitch and timbre are normally in discrete terms; however, the <em>continuous</em> representations are useful for applications such as gradual timbre morphing. The representations between two instruments could serve as a blend of both which could help discover new types of instrument timbre styles.</p>\n<p>Another two strong examples demonstrating the strength of SSL-VAE frameworks (which also helped me understand a lot on SSL-VAE applications), though not in the music domain, is by the <a href=\"https://google.github.io/tacotron/\" target=\"_blank\" rel=\"noopener\">Tacotron</a> team. Two of their papers explore similar ideas to VaDE and Kingma et al to involve <a href=\"https://arxiv.org/pdf/1810.07217.pdf\" target=\"_blank\" rel=\"noopener\">hierarchical modelling</a> and <a href=\"https://arxiv.org/pdf/1910.01709.pdf\" target=\"_blank\" rel=\"noopener\">semi-supervised learning</a> for realistic text-to-speech generation. One of the examples is demonstrated on affect conditioning, which is again often a scarely-labelled scenario, yet the authors are able to achieve outstanding results on speech synthesis.</p>\n<h2 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h2><p>With the rise in popularity of using latent variable models for music modelling, it is intuitive that by incorporating the frameworks mentioned above, these models can be extended easily to support SSL capabilities. Perhaps some interesting questions to ask are: what is the lower-bound of the amount of data we need to achieve good results with SSL-VAE architectures? How much could we further improve on the generation component to “self-supervisedly” learn good representations, and reduce the necessity of using more labels? Can the training go even further to purely unsupervised scenarios? These are indeed exciting research problems waiting to be solved.</p>\n<p>For the fundamental framework papers, please refer to the list below:</p>\n<ol>\n<li><a href=\"https://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Semi-supervised Learning with Deep Generative Models</a></li>\n<li><a href=\"https://arxiv.org/pdf/1611.05148.pdf\" target=\"_blank\" rel=\"noopener\">Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering</a></li>\n<li><a href=\"https://papers.nips.cc/paper/7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf\" target=\"_blank\" rel=\"noopener\">Learning Disentangled Representations with Semi-Supervised Deep Generative Models</a></li>\n</ol>\n"},{"title":"Spectrogram Conversion with CNNs","date":"2020-07-24T10:20:39.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief introduction on short-time Fourier transform\n2 - How spectrogram conversion can be implemented using CNNs (based on [nnAudio](https://github.com/KinWaiCheuk/nnAudio))\n<br/>\n\n## 1 - Introduction\n\nRecently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely **spectrograms**. My wonderful colleague Raven Cheuk had released a GPU audio processing named [nnAudio](https://github.com/KinWaiCheuk/nnAudio) last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.\n\n## 2 - Short Time Fourier Transform (STFT)\n\nFirst, we discuss the case for **discrete Fourier transform** (DFT), which converts a given audio signal of length \\\\(L\\\\) into a vector \\\\(X_{DFT}\\\\) of size \\\\(N\\\\), where \\\\(N\\\\) is the number of frequency bins (commonly, we set \\\\(L = N\\\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nHowever, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple **windows**, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the **discrete short-time Fourier transform** (STFT), which converts a given audio signal of length \\\\(L\\\\) into a time-frequency representation of shape \\\\((N, T)\\\\). \\\\(N\\\\) is the number of frequency bins, and \\\\(T\\\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\\\(L_{\\textrm{w}}\\\\) (similarly, \\\\(L_{\\textrm{w}} = N\\\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\\\(H\\\\)) to finish \"sweeping\" the audio signal.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\nGiven an audio signal \\\\(x\\\\), a **complex-form spectrogram** \\\\(X\\\\) which is the output of STFT is expressed by:\n$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$\n\nWe can further use Euler's formula to expand \\\\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\\\) into \\\\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\\\). The term \\\\(w[l]\\\\) is an additional [**window function**](https://en.wikipedia.org/wiki/Window_function) which helps to distribute spectral leakage according to the needs of the application. \n\nFrom Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\\\(N\\\\) **cosine and sine \"filters\"** respectively, and perform **1D convolution** on the audio signal, whereby the **stride** is exactly of the **hop length** \\\\(H\\\\). \n\n## 3 - Inverse STFT\n\nCan inverse STFT be implemented in terms of CNNs as well? In fact, this [torch-stft](https://github.com/pseeth/torch-stft) library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.\n\nIf we put together the equations of discrete DFT and inverse DFT (with window function) as below:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nwe can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, **we can perform inverse STFT using the same cosine and sine \"filters\"**, as the Euler term stays the same. \n\nAs \\\\(X_{DFT}\\\\) is in complex form, we can observe that the multiplication with the Euler term results in:\n$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\\\(x\\\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\\\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\nSince STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\\\((N, 1)\\\\) on the spectrogram with shape \\\\((N, T)\\\\) with stride \\\\((1, 1)\\\\). \n\nThe final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\\\(L_w - H\\\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.\n\n## 4 - Code Implementation\n\nThe above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:\n1. [Short-Time Fourier Transform with 1D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534)\n1. [Inverse STFT with 2D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581)\n\n\n\n\n\n\n","source":"_posts/conv_fourier.md","raw":"---\ntitle: Spectrogram Conversion with CNNs\ndate: 2020-07-24 18:20:39\ntags:\n    - Music Signal Processing\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief introduction on short-time Fourier transform\n2 - How spectrogram conversion can be implemented using CNNs (based on [nnAudio](https://github.com/KinWaiCheuk/nnAudio))\n<br/>\n\n## 1 - Introduction\n\nRecently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely **spectrograms**. My wonderful colleague Raven Cheuk had released a GPU audio processing named [nnAudio](https://github.com/KinWaiCheuk/nnAudio) last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.\n\n## 2 - Short Time Fourier Transform (STFT)\n\nFirst, we discuss the case for **discrete Fourier transform** (DFT), which converts a given audio signal of length \\\\(L\\\\) into a vector \\\\(X_{DFT}\\\\) of size \\\\(N\\\\), where \\\\(N\\\\) is the number of frequency bins (commonly, we set \\\\(L = N\\\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nHowever, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple **windows**, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the **discrete short-time Fourier transform** (STFT), which converts a given audio signal of length \\\\(L\\\\) into a time-frequency representation of shape \\\\((N, T)\\\\). \\\\(N\\\\) is the number of frequency bins, and \\\\(T\\\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\\\(L_{\\textrm{w}}\\\\) (similarly, \\\\(L_{\\textrm{w}} = N\\\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\\\(H\\\\)) to finish \"sweeping\" the audio signal.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\nGiven an audio signal \\\\(x\\\\), a **complex-form spectrogram** \\\\(X\\\\) which is the output of STFT is expressed by:\n$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$\n\nWe can further use Euler's formula to expand \\\\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\\\) into \\\\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\\\). The term \\\\(w[l]\\\\) is an additional [**window function**](https://en.wikipedia.org/wiki/Window_function) which helps to distribute spectral leakage according to the needs of the application. \n\nFrom Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\\\(N\\\\) **cosine and sine \"filters\"** respectively, and perform **1D convolution** on the audio signal, whereby the **stride** is exactly of the **hop length** \\\\(H\\\\). \n\n## 3 - Inverse STFT\n\nCan inverse STFT be implemented in terms of CNNs as well? In fact, this [torch-stft](https://github.com/pseeth/torch-stft) library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.\n\nIf we put together the equations of discrete DFT and inverse DFT (with window function) as below:\n$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$\n\nwe can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, **we can perform inverse STFT using the same cosine and sine \"filters\"**, as the Euler term stays the same. \n\nAs \\\\(X_{DFT}\\\\) is in complex form, we can observe that the multiplication with the Euler term results in:\n$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\\\(x\\\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\\\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\\\).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\nSince STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\\\((N, 1)\\\\) on the spectrogram with shape \\\\((N, T)\\\\) with stride \\\\((1, 1)\\\\). \n\nThe final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\\\(L_w - H\\\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.\n\n## 4 - Code Implementation\n\nThe above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:\n1. [Short-Time Fourier Transform with 1D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534)\n1. [Inverse STFT with 2D-CNNs](https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581)\n\n\n\n\n\n\n","slug":"conv_fourier","published":1,"updated":"2020-11-25T03:35:22.827Z","_id":"ckgcjd4cq0000w19khjzs6q1z","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief introduction on short-time Fourier transform<br>2 - How spectrogram conversion can be implemented using CNNs (based on <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a>)<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely <strong>spectrograms</strong>. My wonderful colleague Raven Cheuk had released a GPU audio processing named <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.</p>\n<h2 id=\"2-Short-Time-Fourier-Transform-STFT\"><a href=\"#2-Short-Time-Fourier-Transform-STFT\" class=\"headerlink\" title=\"2 - Short Time Fourier Transform (STFT)\"></a>2 - Short Time Fourier Transform (STFT)</h2><p>First, we discuss the case for <strong>discrete Fourier transform</strong> (DFT), which converts a given audio signal of length \\(L\\) into a vector \\(X_{DFT}\\) of size \\(N\\), where \\(N\\) is the number of frequency bins (commonly, we set \\(L = N\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>However, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple <strong>windows</strong>, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the <strong>discrete short-time Fourier transform</strong> (STFT), which converts a given audio signal of length \\(L\\) into a time-frequency representation of shape \\((N, T)\\). \\(N\\) is the number of frequency bins, and \\(T\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\(L_{\\textrm{w}}\\) (similarly, \\(L_{\\textrm{w}} = N\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\(H\\)) to finish “sweeping” the audio signal.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\n<p>Given an audio signal \\(x\\), a <strong>complex-form spectrogram</strong> \\(X\\) which is the output of STFT is expressed by:<br>$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$</p>\n<p>We can further use Euler’s formula to expand \\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\) into \\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\). The term \\(w[l]\\) is an additional <a href=\"https://en.wikipedia.org/wiki/Window_function\" target=\"_blank\" rel=\"noopener\"><strong>window function</strong></a> which helps to distribute spectral leakage according to the needs of the application. </p>\n<p>From Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\(N\\) <strong>cosine and sine “filters”</strong> respectively, and perform <strong>1D convolution</strong> on the audio signal, whereby the <strong>stride</strong> is exactly of the <strong>hop length</strong> \\(H\\). </p>\n<h2 id=\"3-Inverse-STFT\"><a href=\"#3-Inverse-STFT\" class=\"headerlink\" title=\"3 - Inverse STFT\"></a>3 - Inverse STFT</h2><p>Can inverse STFT be implemented in terms of CNNs as well? In fact, this <a href=\"https://github.com/pseeth/torch-stft\" target=\"_blank\" rel=\"noopener\">torch-stft</a> library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.</p>\n<p>If we put together the equations of discrete DFT and inverse DFT (with window function) as below:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>we can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, <strong>we can perform inverse STFT using the same cosine and sine “filters”</strong>, as the Euler term stays the same. </p>\n<p>As \\(X_{DFT}\\) is in complex form, we can observe that the multiplication with the Euler term results in:<br>$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\(x\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\n<p>Since STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\((N, 1)\\) on the spectrogram with shape \\((N, T)\\) with stride \\((1, 1)\\). </p>\n<p>The final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\(L_w - H\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.</p>\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>The above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:</p>\n<ol>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534\" target=\"_blank\" rel=\"noopener\">Short-Time Fourier Transform with 1D-CNNs</a></li>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581\" target=\"_blank\" rel=\"noopener\">Inverse STFT with 2D-CNNs</a></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief introduction on short-time Fourier transform<br>2 - How spectrogram conversion can be implemented using CNNs (based on <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a>)<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I have wanted to understand more about the audio domain in music signal processing. The obvious start will be to understand from time-frequency representations first, namely <strong>spectrograms</strong>. My wonderful colleague Raven Cheuk had released a GPU audio processing named <a href=\"https://github.com/KinWaiCheuk/nnAudio\" target=\"_blank\" rel=\"noopener\">nnAudio</a> last year, which implements fast spectrogram conversions on GPU with 1D convolution nets, and I decided to further understand this connection between STFT and 1D CNNs.</p>\n<h2 id=\"2-Short-Time-Fourier-Transform-STFT\"><a href=\"#2-Short-Time-Fourier-Transform-STFT\" class=\"headerlink\" title=\"2 - Short Time Fourier Transform (STFT)\"></a>2 - Short Time Fourier Transform (STFT)</h2><p>First, we discuss the case for <strong>discrete Fourier transform</strong> (DFT), which converts a given audio signal of length \\(L\\) into a vector \\(X_{DFT}\\) of size \\(N\\), where \\(N\\) is the number of frequency bins (commonly, we set \\(L = N\\) for convenience in calculations). DFT basically tells the frequency distribution of the audio signal across multiple frequency bins. The equation can be written as:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>However, the output DFT does not contain any time-related information. Hence, the solution is to chop the audio signal into multiple <strong>windows</strong>, apply DFT on each of them, and concatenate the vector outputs along the time axis. This results in the <strong>discrete short-time Fourier transform</strong> (STFT), which converts a given audio signal of length \\(L\\) into a time-frequency representation of shape \\((N, T)\\). \\(N\\) is the number of frequency bins, and \\(T\\) is the number of time steps, whereby for each time step a DFT operation is performed within a window of length \\(L_{\\textrm{w}}\\) (similarly, \\(L_{\\textrm{w}} = N\\) for convenience in calculations), and the number of steps is determined by how much the window is slided (hop length, \\(H\\)) to finish “sweeping” the audio signal.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/stft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Discrete Short-Time Fourier Transform.</figcaption>\n</figure>\n\n<p>Given an audio signal \\(x\\), a <strong>complex-form spectrogram</strong> \\(X\\) which is the output of STFT is expressed by:<br>$$X[n, t] = \\displaystyle\\sum_{l=1}^{L_w} x[t \\cdot H + l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}$$</p>\n<p>We can further use Euler’s formula to expand \\(e^{-i \\cdot 2 \\pi \\cdot \\frac{n}{N} \\cdot l}\\) into \\(\\cos(2 \\pi \\cdot \\frac{n}{N} \\cdot l) - i\\sin(2 \\pi \\cdot \\frac{n}{N} \\cdot l)\\). The term \\(w[l]\\) is an additional <a href=\"https://en.wikipedia.org/wiki/Window_function\" target=\"_blank\" rel=\"noopener\"><strong>window function</strong></a> which helps to distribute spectral leakage according to the needs of the application. </p>\n<p>From Figure 1, we can already see the resemblance between 1D CNNs and STFT conversions. Understanding from the perspective of convolution networks, we can interpret Figure 1 as having \\(N\\) <strong>cosine and sine “filters”</strong> respectively, and perform <strong>1D convolution</strong> on the audio signal, whereby the <strong>stride</strong> is exactly of the <strong>hop length</strong> \\(H\\). </p>\n<h2 id=\"3-Inverse-STFT\"><a href=\"#3-Inverse-STFT\" class=\"headerlink\" title=\"3 - Inverse STFT\"></a>3 - Inverse STFT</h2><p>Can inverse STFT be implemented in terms of CNNs as well? In fact, this <a href=\"https://github.com/pseeth/torch-stft\" target=\"_blank\" rel=\"noopener\">torch-stft</a> library implemented inverse STFT using 1D transposed convolutional nets. However, here I would like to portray an implementation using 2D convolution nets instead.</p>\n<p>If we put together the equations of discrete DFT and inverse DFT (with window function) as below:<br>$$X_{DFT}[n] = \\displaystyle\\sum_{l=1}^{L} x[l] \\cdot w[l] \\cdot e^{-i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}} \\\\ x[l] = \\frac{1}{N \\cdot w[l]} \\displaystyle\\sum_{n=1}^{N} X_{DFT}[n] \\cdot e^{i \\cdot 2 \\pi \\cdot n \\cdot \\frac{l}{N}}$$</p>\n<p>we can observe that both equations appear to be very related, and the terms are seemingly interchangeable. This also means that if we implement STFT using 1D convolutions, <strong>we can perform inverse STFT using the same cosine and sine “filters”</strong>, as the Euler term stays the same. </p>\n<p>As \\(X_{DFT}\\) is in complex form, we can observe that the multiplication with the Euler term results in:<br>$$(X_{real} + i X_{imag})(\\cos \\phi + i \\sin \\phi) \\\\ = (X_{real}\\cos \\phi - X_{imag}\\sin \\phi) + i(X_{real}\\sin \\phi + X_{imag}\\cos \\phi)$$ and as the input signal \\(x\\) is a real-value signal, we should observe that the values lie within the real part of the output, and \\(X_{real}\\sin \\phi + X_{imag}\\cos \\phi = 0\\).</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/istft.png\" alt=\"\"/>\n  <figcaption><br/>Figure 2: Inverse Short-Time Fourier Transform with the same convolution filters.</figcaption>\n</figure>\n\n<p>Since STFT is just a temporal version of DFT, we can perform inverse DFT using the above-stated method on each time step. Figure 2 illustrates the above-stated method using the same convolution filters. The only difference is that, since now the input is a 2D spectrogram, we have to perform 2D convolution. Hence, we can interpret the operation as performing 2D convolution using the cosine / sine filters of shape \\((N, 1)\\) on the spectrogram with shape \\((N, T)\\) with stride \\((1, 1)\\). </p>\n<p>The final output will be the segments of the original audio, with overlapped redundant parts due to the windows overlapping each other during STFT (see the parts to the left of the red dashed line in Figure 2). We can easily observe that other than the first segment, all segments have a starting overlapped segment of length \\(L_w - H\\), hence by removing these starting overlapped segments and concatenating all segments together we can reconstruct the original audio signal.</p>\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>The above-stated methods are implemented in nnAudio using PyTorch, I provide the portals as follows:</p>\n<ol>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L534\" target=\"_blank\" rel=\"noopener\">Short-Time Fourier Transform with 1D-CNNs</a></li>\n<li><a href=\"https://github.com/KinWaiCheuk/nnAudio/blob/master/Installation/nnAudio/Spectrogram.py#L581\" target=\"_blank\" rel=\"noopener\">Inverse STFT with 2D-CNNs</a></li>\n</ol>\n"},{"title":"ISMIR 2020 - Part 1","date":"2020-10-17T01:10:42.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Various exciting papers (sorted according to topics) and research directions of ISMIR 2020\n2 - My own conference experience of this year's ISMIR\n<br/>\n\n## 1 - Introduction\n\nFinally, ISMIR 2020 is around the corner! The conference was originally designated to take place in Montreal, Canada (which was a dream place for me to visit, because a lot of major conferences like NeurIPS, ICLR, ICML were hosted there before). But sadly due to COVID-19, the conference is changed into a virtual event. It is also my first ISMIR, and I have been so looking forward to it since day 1 of doing MIR research, so indeed it is a little bit disappointed for unable to travel and meet people physically this year.\n\nNevertheless, I can already feel that ISMIR is such a unique conference as compared to others, although in a virtual setting, and I can almost understand why so many ISMIR visitors have repeatedly emphasized that ISMIR is the best conference of all. The topics are super interesting, and the community is super friendly, always willing to share & exchange, and extremely fun to talk to.\n\nBelow I try to summarize the papers & posters that I have personally visited, sorted by relevant topics. The list will by no means be exhaustive, and will be very related to my own focus & familiarity (I am more familiar / interested in controllable music generation, music representation learning, music audio synthesis, and some popular MIR tasks e.g. pitch estimation, voice conversion, source separation etc.).\n\n## 2 - Controllable Symbolic Music Generation\n\n[**Attributes-Aware Deep Music Transformation**](https://program.ismir2020.net/poster_5-06.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\nThis work uses a very similar architecture like [Fader Networks](https://arxiv.org/pdf/1706.00409.pdf) in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\\\(z\\\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in [Music FaderNets](https://program.ismir2020.net/poster_1-13.html), but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.\nOne interesting thing is that the authors do not compare results on linear correlation with [GLSR-VAE](https://arxiv.org/pdf/1707.04588.pdf), because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.\n\n[**BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)**](https://program.ismir2020.net/poster_6-08.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\nCongrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on **personalization**. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user's taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.\n\n[**Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation**](https://program.ismir2020.net/poster_1-05.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\nThis work proposes **connective fusion**, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the [Latent Constraint](https://arxiv.org/pdf/1711.05772.pdf) paper - firstly, we pretrain a VAE to learn latent code \\\\(z\\\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\\\(G\\\\) that generates latent code pair \\\\((z^\\prime_L, z^\\prime_R)\\\\) that is indistuingishable from the input pair\\\\((z_L, z_R)\\\\). During training, we also add in an additional style vector \\\\(s\\\\), hence also learning a style space which controls how the two sequences are connectively fused.\nI was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. [His work last year](http://archives.ismir.net/ismir2019/paper/000100.pdf) has also helped me a lot with Music FaderNets, so huge kudos to him!\n\n[**Generating Music with a Self-Correcting Non-Chronological Autoregressive Model**](https://program.ismir2020.net/poster_6-16.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\nI spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using **edit distance**. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against [orderless NADE](https://www.jmlr.org/papers/volume17/16-272/16-272.pdf) and [CoCoNet](https://arxiv.org/pdf/1903.07227.pdf), but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.\n\n[**PIANOTREE VAE: Structured Representation Learning for Polyphonic Music**](https://program.ismir2020.net/poster_3-06.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a **tree-like structure**, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.\nI definitely think that PianoTree has the potential to be the *de facto* representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: **the ease of usage** (e.g. open source of encoder components and examples of usage), and whether **the data structure is tightly coupled with the proposed VAE model**. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.\n\n[**Learning Interpretable Representation for Controllable Polyphonic Music Generation**](https://program.ismir2020.net/poster_5-05.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\nThis work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the **disentanglement of chords and texture** of a music piece. The architecture adopts a similar idea as their prior work called [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.\nMeeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!\n\n[**Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)**](https://program.ismir2020.net/poster_1-13.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\nMy work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the **insufficiency of labels**, or the **subjectiveness** (and hence large variance) in human-annotated labels. We propose to use low-level features as \"bridges\" to between the music and the high level features. Hence, the model consists of:\n-  **faders**, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement\n-  **presets**, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.\n\nThis method combines the advantages of **rule-based methods** and **data-driven machine learning**. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of **semi-supervised learning**. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.\n\n[**Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm**](https://program.ismir2020.net/poster_1-09.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\nThis work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to \"fill up the blanks\" with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) as well). There are 3 separate components: **SketchVAE** for latent representation learning, **SketchInpainter** for predicting missing measures based on previous and future contexts, and **SketchConnector** which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).\n\n[**The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures**](https://program.ismir2020.net/poster_1-17.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\nThis is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?\n\nFor me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall [this work](https://arxiv.org/pdf/1912.05537.pdf) suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using [fitness scape plot](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html) to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.\n\n## 3 - Disentangled Representation Learning\n\n[**Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds**](https://program.ismir2020.net/poster_5-10.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\nWork by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on [pitch-timbre disentanglement](https://arxiv.org/pdf/1906.08152.pdf) before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? \n\nThis work employs a key idea: **moderate pitch shiftings will not change timbre**. Hence, even if we don't have any labels annotated on pitch and timbre, we can still achieve disentanglement by [contrastive learning paradigms](https://paperswithcode.com/task/contrastive-learning) - data augmentation by transposing the pitch, but enforce relations in \\\\(z_\\textrm{pitch}\\\\) and \\\\(z_\\textrm{timbre}\\\\). The authors propose 4 losses: regression loss, [contrastive loss](https://arxiv.org/pdf/2002.05709.pdf), [cycle consistency loss](https://arxiv.org/pdf/1703.10593v7.pdf) and a new **surrogate label loss**. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.\n\n[**Metric learning VS classification for disentangled music representation learning**](https://program.ismir2020.net/poster_3-15.html)\n\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\nThis interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to [this work](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf)). Firstly, the authors connect classication and metric learning with **proxy-based metric learning**. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are\ngenerally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.\n\n[**dMelodies: A Music Dataset for Disentanglement Learning**](https://program.ismir2020.net/poster_1-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new dataset which resembles [dSprites](https://github.com/deepmind/dsprites-dataset) in the computer vision domain, which is designed for learning and **evaluating disentangled representation learning algorithms for music**. The authors also ran benchmark experiments using common disentanglement methods (\\\\(\\beta\\\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.\nI discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. [Nottingham dataset](https://ifdo.ca/~seymour/nottingham/nottingham.html)) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis' work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)\n\n## 4 - Singing Voice Conversion\n\n[**Zero-Shot Singing Voice Conversion**](https://program.ismir2020.net/poster_1-08.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\nThe most interesting part of this work is the **zero-shot** part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a [pretrained speaker embedding network](https://github.com/CorentinJ/Real-Time-Voice-Cloning), and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the \"pre-trained fine-tune\" idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.\n\n## 5 - Audio Synthesis\n\n[**DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks**](https://program.ismir2020.net/poster_4-16.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\nSuper cool and useful work (can't wait to use the plugin as a producer)! This work uses a **progressive growing GAN** (similar to the idea in [GANSynth]()) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or [R-score in linear regressor](http://proceedings.mlr.press/v80/adel18a/adel18a.pdf), which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in [a previous work using U-Net architecture](https://arxiv.org/pdf/1911.11853.pdf). This could probably shed light to a new spectrum of measurements in terms of factor controllability.\n\nAnother interesting thing is that this work uses **complex STFT spectrogram** as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.\n\n*The remaining topics (source separation, transcription, model pruning and cover song detection) will be covered in [Part 2](/2020/10/17/ismir_2020_pt2/)*.","source":"_posts/ismir_2020.md","raw":"---\ntitle: ISMIR 2020 - Part 1\ndate: 2020-10-17 09:10:42\ntags:\n    - Music Information Retrieval\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Various exciting papers (sorted according to topics) and research directions of ISMIR 2020\n2 - My own conference experience of this year's ISMIR\n<br/>\n\n## 1 - Introduction\n\nFinally, ISMIR 2020 is around the corner! The conference was originally designated to take place in Montreal, Canada (which was a dream place for me to visit, because a lot of major conferences like NeurIPS, ICLR, ICML were hosted there before). But sadly due to COVID-19, the conference is changed into a virtual event. It is also my first ISMIR, and I have been so looking forward to it since day 1 of doing MIR research, so indeed it is a little bit disappointed for unable to travel and meet people physically this year.\n\nNevertheless, I can already feel that ISMIR is such a unique conference as compared to others, although in a virtual setting, and I can almost understand why so many ISMIR visitors have repeatedly emphasized that ISMIR is the best conference of all. The topics are super interesting, and the community is super friendly, always willing to share & exchange, and extremely fun to talk to.\n\nBelow I try to summarize the papers & posters that I have personally visited, sorted by relevant topics. The list will by no means be exhaustive, and will be very related to my own focus & familiarity (I am more familiar / interested in controllable music generation, music representation learning, music audio synthesis, and some popular MIR tasks e.g. pitch estimation, voice conversion, source separation etc.).\n\n## 2 - Controllable Symbolic Music Generation\n\n[**Attributes-Aware Deep Music Transformation**](https://program.ismir2020.net/poster_5-06.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\nThis work uses a very similar architecture like [Fader Networks](https://arxiv.org/pdf/1706.00409.pdf) in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\\\(z\\\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in [Music FaderNets](https://program.ismir2020.net/poster_1-13.html), but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.\nOne interesting thing is that the authors do not compare results on linear correlation with [GLSR-VAE](https://arxiv.org/pdf/1707.04588.pdf), because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.\n\n[**BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)**](https://program.ismir2020.net/poster_6-08.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\nCongrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on **personalization**. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user's taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.\n\n[**Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation**](https://program.ismir2020.net/poster_1-05.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\nThis work proposes **connective fusion**, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the [Latent Constraint](https://arxiv.org/pdf/1711.05772.pdf) paper - firstly, we pretrain a VAE to learn latent code \\\\(z\\\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\\\(G\\\\) that generates latent code pair \\\\((z^\\prime_L, z^\\prime_R)\\\\) that is indistuingishable from the input pair\\\\((z_L, z_R)\\\\). During training, we also add in an additional style vector \\\\(s\\\\), hence also learning a style space which controls how the two sequences are connectively fused.\nI was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. [His work last year](http://archives.ismir.net/ismir2019/paper/000100.pdf) has also helped me a lot with Music FaderNets, so huge kudos to him!\n\n[**Generating Music with a Self-Correcting Non-Chronological Autoregressive Model**](https://program.ismir2020.net/poster_6-16.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\nI spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using **edit distance**. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against [orderless NADE](https://www.jmlr.org/papers/volume17/16-272/16-272.pdf) and [CoCoNet](https://arxiv.org/pdf/1903.07227.pdf), but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.\n\n[**PIANOTREE VAE: Structured Representation Learning for Polyphonic Music**](https://program.ismir2020.net/poster_3-06.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a **tree-like structure**, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.\nI definitely think that PianoTree has the potential to be the *de facto* representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: **the ease of usage** (e.g. open source of encoder components and examples of usage), and whether **the data structure is tightly coupled with the proposed VAE model**. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.\n\n[**Learning Interpretable Representation for Controllable Polyphonic Music Generation**](https://program.ismir2020.net/poster_5-05.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\nThis work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the **disentanglement of chords and texture** of a music piece. The architecture adopts a similar idea as their prior work called [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.\nMeeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!\n\n[**Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)**](https://program.ismir2020.net/poster_1-13.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\nMy work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the **insufficiency of labels**, or the **subjectiveness** (and hence large variance) in human-annotated labels. We propose to use low-level features as \"bridges\" to between the music and the high level features. Hence, the model consists of:\n-  **faders**, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement\n-  **presets**, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.\n\nThis method combines the advantages of **rule-based methods** and **data-driven machine learning**. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of **semi-supervised learning**. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.\n\n[**Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm**](https://program.ismir2020.net/poster_1-09.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\nThis work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to \"fill up the blanks\" with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by [EC\\\\(^2\\\\)-VAE](http://archives.ismir.net/ismir2019/paper/000072.pdf) as well). There are 3 separate components: **SketchVAE** for latent representation learning, **SketchInpainter** for predicting missing measures based on previous and future contexts, and **SketchConnector** which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).\n\n[**The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures**](https://program.ismir2020.net/poster_1-17.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\nThis is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?\n\nFor me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall [this work](https://arxiv.org/pdf/1912.05537.pdf) suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using [fitness scape plot](https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html) to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.\n\n## 3 - Disentangled Representation Learning\n\n[**Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds**](https://program.ismir2020.net/poster_5-10.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\nWork by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on [pitch-timbre disentanglement](https://arxiv.org/pdf/1906.08152.pdf) before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? \n\nThis work employs a key idea: **moderate pitch shiftings will not change timbre**. Hence, even if we don't have any labels annotated on pitch and timbre, we can still achieve disentanglement by [contrastive learning paradigms](https://paperswithcode.com/task/contrastive-learning) - data augmentation by transposing the pitch, but enforce relations in \\\\(z_\\textrm{pitch}\\\\) and \\\\(z_\\textrm{timbre}\\\\). The authors propose 4 losses: regression loss, [contrastive loss](https://arxiv.org/pdf/2002.05709.pdf), [cycle consistency loss](https://arxiv.org/pdf/1703.10593v7.pdf) and a new **surrogate label loss**. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.\n\n[**Metric learning VS classification for disentangled music representation learning**](https://program.ismir2020.net/poster_3-15.html)\n\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\nThis interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to [this work](http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf)). Firstly, the authors connect classication and metric learning with **proxy-based metric learning**. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are\ngenerally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.\n\n[**dMelodies: A Music Dataset for Disentanglement Learning**](https://program.ismir2020.net/poster_1-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\nThis work proposes a new dataset which resembles [dSprites](https://github.com/deepmind/dsprites-dataset) in the computer vision domain, which is designed for learning and **evaluating disentangled representation learning algorithms for music**. The authors also ran benchmark experiments using common disentanglement methods (\\\\(\\beta\\\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.\nI discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. [Nottingham dataset](https://ifdo.ca/~seymour/nottingham/nottingham.html)) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis' work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)\n\n## 4 - Singing Voice Conversion\n\n[**Zero-Shot Singing Voice Conversion**](https://program.ismir2020.net/poster_1-08.html)\n\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\nThe most interesting part of this work is the **zero-shot** part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a [pretrained speaker embedding network](https://github.com/CorentinJ/Real-Time-Voice-Cloning), and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the \"pre-trained fine-tune\" idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.\n\n## 5 - Audio Synthesis\n\n[**DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks**](https://program.ismir2020.net/poster_4-16.html)\n\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\nSuper cool and useful work (can't wait to use the plugin as a producer)! This work uses a **progressive growing GAN** (similar to the idea in [GANSynth]()) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or [R-score in linear regressor](http://proceedings.mlr.press/v80/adel18a/adel18a.pdf), which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in [a previous work using U-Net architecture](https://arxiv.org/pdf/1911.11853.pdf). This could probably shed light to a new spectrum of measurements in terms of factor controllability.\n\nAnother interesting thing is that this work uses **complex STFT spectrogram** as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.\n\n*The remaining topics (source separation, transcription, model pruning and cover song detection) will be covered in [Part 2](/2020/10/17/ismir_2020_pt2/)*.","slug":"ismir_2020","published":1,"updated":"2020-10-17T16:33:18.986Z","_id":"ckgcjeans0008w19k9uev444u","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Various exciting papers (sorted according to topics) and research directions of ISMIR 2020<br>2 - My own conference experience of this year’s ISMIR<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Finally, ISMIR 2020 is around the corner! The conference was originally designated to take place in Montreal, Canada (which was a dream place for me to visit, because a lot of major conferences like NeurIPS, ICLR, ICML were hosted there before). But sadly due to COVID-19, the conference is changed into a virtual event. It is also my first ISMIR, and I have been so looking forward to it since day 1 of doing MIR research, so indeed it is a little bit disappointed for unable to travel and meet people physically this year.</p>\n<p>Nevertheless, I can already feel that ISMIR is such a unique conference as compared to others, although in a virtual setting, and I can almost understand why so many ISMIR visitors have repeatedly emphasized that ISMIR is the best conference of all. The topics are super interesting, and the community is super friendly, always willing to share &amp; exchange, and extremely fun to talk to.</p>\n<p>Below I try to summarize the papers &amp; posters that I have personally visited, sorted by relevant topics. The list will by no means be exhaustive, and will be very related to my own focus &amp; familiarity (I am more familiar / interested in controllable music generation, music representation learning, music audio synthesis, and some popular MIR tasks e.g. pitch estimation, voice conversion, source separation etc.).</p>\n<h2 id=\"2-Controllable-Symbolic-Music-Generation\"><a href=\"#2-Controllable-Symbolic-Music-Generation\" class=\"headerlink\" title=\"2 - Controllable Symbolic Music Generation\"></a>2 - Controllable Symbolic Music Generation</h2><p><a href=\"https://program.ismir2020.net/poster_5-06.html\" target=\"_blank\" rel=\"noopener\"><strong>Attributes-Aware Deep Music Transformation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\n<p>This work uses a very similar architecture like <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks</a> in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\(z\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in <a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\">Music FaderNets</a>, but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.<br>One interesting thing is that the authors do not compare results on linear correlation with <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE</a>, because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-08.html\" target=\"_blank\" rel=\"noopener\"><strong>BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\n<p>Congrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on <strong>personalization</strong>. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user’s taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes <strong>connective fusion</strong>, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraint</a> paper - firstly, we pretrain a VAE to learn latent code \\(z\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\(G\\) that generates latent code pair \\((z^\\prime_L, z^\\prime_R)\\) that is indistuingishable from the input pair\\((z_L, z_R)\\). During training, we also add in an additional style vector \\(s\\), hence also learning a style space which controls how the two sequences are connectively fused.<br>I was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\">His work last year</a> has also helped me a lot with Music FaderNets, so huge kudos to him!</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Generating Music with a Self-Correcting Non-Chronological Autoregressive Model</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\n<p>I spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using <strong>edit distance</strong>. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against <a href=\"https://www.jmlr.org/papers/volume17/16-272/16-272.pdf\" target=\"_blank\" rel=\"noopener\">orderless NADE</a> and <a href=\"https://arxiv.org/pdf/1903.07227.pdf\" target=\"_blank\" rel=\"noopener\">CoCoNet</a>, but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-06.html\" target=\"_blank\" rel=\"noopener\"><strong>PIANOTREE VAE: Structured Representation Learning for Polyphonic Music</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a <strong>tree-like structure</strong>, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.<br>I definitely think that PianoTree has the potential to be the <em>de facto</em> representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: <strong>the ease of usage</strong> (e.g. open source of encoder components and examples of usage), and whether <strong>the data structure is tightly coupled with the proposed VAE model</strong>. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Learning Interpretable Representation for Controllable Polyphonic Music Generation</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the <strong>disentanglement of chords and texture</strong> of a music piece. The architecture adopts a similar idea as their prior work called <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.<br>Meeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\"><strong>Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\n<p>My work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the <strong>insufficiency of labels</strong>, or the <strong>subjectiveness</strong> (and hence large variance) in human-annotated labels. We propose to use low-level features as “bridges” to between the music and the high level features. Hence, the model consists of:</p>\n<ul>\n<li><strong>faders</strong>, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement</li>\n<li><strong>presets</strong>, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.</li>\n</ul>\n<p>This method combines the advantages of <strong>rule-based methods</strong> and <strong>data-driven machine learning</strong>. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of <strong>semi-supervised learning</strong>. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-09.html\" target=\"_blank\" rel=\"noopener\"><strong>Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to “fill up the blanks” with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> as well). There are 3 separate components: <strong>SketchVAE</strong> for latent representation learning, <strong>SketchInpainter</strong> for predicting missing measures based on previous and future contexts, and <strong>SketchConnector</strong> which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-17.html\" target=\"_blank\" rel=\"noopener\"><strong>The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\n<p>This is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?</p>\n<p>For me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">this work</a> suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using <a href=\"https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html\" target=\"_blank\" rel=\"noopener\">fitness scape plot</a> to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.</p>\n<h2 id=\"3-Disentangled-Representation-Learning\"><a href=\"#3-Disentangled-Representation-Learning\" class=\"headerlink\" title=\"3 - Disentangled Representation Learning\"></a>3 - Disentangled Representation Learning</h2><p><a href=\"https://program.ismir2020.net/poster_5-10.html\" target=\"_blank\" rel=\"noopener\"><strong>Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\n<p>Work by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">pitch-timbre disentanglement</a> before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? </p>\n<p>This work employs a key idea: <strong>moderate pitch shiftings will not change timbre</strong>. Hence, even if we don’t have any labels annotated on pitch and timbre, we can still achieve disentanglement by <a href=\"https://paperswithcode.com/task/contrastive-learning\" target=\"_blank\" rel=\"noopener\">contrastive learning paradigms</a> - data augmentation by transposing the pitch, but enforce relations in \\(z_\\textrm{pitch}\\) and \\(z_\\textrm{timbre}\\). The authors propose 4 losses: regression loss, <a href=\"https://arxiv.org/pdf/2002.05709.pdf\" target=\"_blank\" rel=\"noopener\">contrastive loss</a>, <a href=\"https://arxiv.org/pdf/1703.10593v7.pdf\" target=\"_blank\" rel=\"noopener\">cycle consistency loss</a> and a new <strong>surrogate label loss</strong>. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Metric learning VS classification for disentangled music representation learning</strong></a></p>\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\n<p>This interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to <a href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf\" target=\"_blank\" rel=\"noopener\">this work</a>). Firstly, the authors connect classication and metric learning with <strong>proxy-based metric learning</strong>. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are<br>generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-15.html\" target=\"_blank\" rel=\"noopener\"><strong>dMelodies: A Music Dataset for Disentanglement Learning</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new dataset which resembles <a href=\"https://github.com/deepmind/dsprites-dataset\" target=\"_blank\" rel=\"noopener\">dSprites</a> in the computer vision domain, which is designed for learning and <strong>evaluating disentangled representation learning algorithms for music</strong>. The authors also ran benchmark experiments using common disentanglement methods (\\(\\beta\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.<br>I discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. <a href=\"https://ifdo.ca/~seymour/nottingham/nottingham.html\" target=\"_blank\" rel=\"noopener\">Nottingham dataset</a>) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis’ work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)</p>\n<h2 id=\"4-Singing-Voice-Conversion\"><a href=\"#4-Singing-Voice-Conversion\" class=\"headerlink\" title=\"4 - Singing Voice Conversion\"></a>4 - Singing Voice Conversion</h2><p><a href=\"https://program.ismir2020.net/poster_1-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Zero-Shot Singing Voice Conversion</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\n<p>The most interesting part of this work is the <strong>zero-shot</strong> part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a <a href=\"https://github.com/CorentinJ/Real-Time-Voice-Cloning\" target=\"_blank\" rel=\"noopener\">pretrained speaker embedding network</a>, and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the “pre-trained fine-tune” idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.</p>\n<h2 id=\"5-Audio-Synthesis\"><a href=\"#5-Audio-Synthesis\" class=\"headerlink\" title=\"5 - Audio Synthesis\"></a>5 - Audio Synthesis</h2><p><a href=\"https://program.ismir2020.net/poster_4-16.html\" target=\"_blank\" rel=\"noopener\"><strong>DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\n<p>Super cool and useful work (can’t wait to use the plugin as a producer)! This work uses a <strong>progressive growing GAN</strong> (similar to the idea in <a href=\"\">GANSynth</a>) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or <a href=\"http://proceedings.mlr.press/v80/adel18a/adel18a.pdf\">R-score in linear regressor</a>, which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in <a href=\"https://arxiv.org/pdf/1911.11853.pdf\" target=\"_blank\" rel=\"noopener\">a previous work using U-Net architecture</a>. This could probably shed light to a new spectrum of measurements in terms of factor controllability.</p>\n<p>Another interesting thing is that this work uses <strong>complex STFT spectrogram</strong> as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.</p>\n<p><em>The remaining topics (source separation, transcription, model pruning and cover song detection) will be covered in <a href=\"/2020/10/17/ismir_2020_pt2/\">Part 2</a></em>.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Various exciting papers (sorted according to topics) and research directions of ISMIR 2020<br>2 - My own conference experience of this year’s ISMIR<br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Finally, ISMIR 2020 is around the corner! The conference was originally designated to take place in Montreal, Canada (which was a dream place for me to visit, because a lot of major conferences like NeurIPS, ICLR, ICML were hosted there before). But sadly due to COVID-19, the conference is changed into a virtual event. It is also my first ISMIR, and I have been so looking forward to it since day 1 of doing MIR research, so indeed it is a little bit disappointed for unable to travel and meet people physically this year.</p>\n<p>Nevertheless, I can already feel that ISMIR is such a unique conference as compared to others, although in a virtual setting, and I can almost understand why so many ISMIR visitors have repeatedly emphasized that ISMIR is the best conference of all. The topics are super interesting, and the community is super friendly, always willing to share &amp; exchange, and extremely fun to talk to.</p>\n<p>Below I try to summarize the papers &amp; posters that I have personally visited, sorted by relevant topics. The list will by no means be exhaustive, and will be very related to my own focus &amp; familiarity (I am more familiar / interested in controllable music generation, music representation learning, music audio synthesis, and some popular MIR tasks e.g. pitch estimation, voice conversion, source separation etc.).</p>\n<h2 id=\"2-Controllable-Symbolic-Music-Generation\"><a href=\"#2-Controllable-Symbolic-Music-Generation\" class=\"headerlink\" title=\"2 - Controllable Symbolic Music Generation\"></a>2 - Controllable Symbolic Music Generation</h2><p><a href=\"https://program.ismir2020.net/poster_5-06.html\" target=\"_blank\" rel=\"noopener\"><strong>Attributes-Aware Deep Music Transformation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_attr.png\" alt=\"\"/>\n</figure>\n\n<p>This work uses a very similar architecture like <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks</a> in the computer vision domain - a conditional VAE, with an additional adversarial component to ensure latent \\(z\\) does not incorporate condition information. Evaluation on controllability is done on monophonic music. I tried the same architecture on polyphonic music in <a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\">Music FaderNets</a>, but I found that it does not produce optimal results in terms of linearity as compared to other latent regularization methods.<br>One interesting thing is that the authors do not compare results on linear correlation with <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE</a>, because they argued that GLSR-VAE is not designed to enforce linear correlation between latent values and attributes. I agree this to a certain extent, but to me linear correlation between both is still the most intuitive way to achieve controllability on low-level attributes, hence measuring that is still important in the context of controllable generation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-08.html\" target=\"_blank\" rel=\"noopener\"><strong>BebopNet: Deep Neural Models for Personalized Jazz Improvisations (Best Paper Award)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_bebop.png\" alt=\"\"/>\n</figure>\n\n<p>Congrats on this paper getting the best research award of this year! Compared to other similar works, this work focuses on <strong>personalization</strong>. Within the pipeline, other than the generation component, a dataset personal to the user is collected to train personal preference metrics, very much like an active learning strategy. As the music plays, the user adjusts a meter to display the level of satisfaction of the currently heard jazz solo. Then a regression model is trained to predict the user’s taste. Finally, a beam serach is employed by using the criterion of score predicted the user preference regression model. The output of beam search should result in a music piece most adhered to the user preference. A very simple idea, but could be widely adoptable to all kinds of generation models to add in more degree of personalization.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Connective Fusion: Learning Transformational Joining of Sequences with Application to Melody Creation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_conn.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes <strong>connective fusion</strong>, which is a generation scheme by transforming between two given music sequences. The architecture is inspired by the <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraint</a> paper - firstly, we pretrain a VAE to learn latent code \\(z\\) for a music sequence. Then, using a GAN-like actor-critic method, we learn a generator \\(G\\) that generates latent code pair \\((z^\\prime_L, z^\\prime_R)\\) that is indistuingishable from the input pair\\((z_L, z_R)\\). During training, we also add in an additional style vector \\(s\\), hence also learning a style space which controls how the two sequences are connectively fused.<br>I was fortunate enough to discuss with the author Taketo Akama about several issues of using VAE for music generation. In general, we found a significant tradeoff between attribute controllability and reconstruction (identity preservation), and training to generate longer sequence seems to really be a hassle. <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\">His work last year</a> has also helped me a lot with Music FaderNets, so huge kudos to him!</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Generating Music with a Self-Correcting Non-Chronological Autoregressive Model</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_edit.png\" alt=\"\"/>\n</figure>\n\n<p>I spotted this work previously during ML4MD and find it interesting because it suggests a very different approach towards music generation, which is using <strong>edit distance</strong>. The two key differences with common music generation idea is that (i) music composition can be non-chronological in nature, and (ii) the generation process should allow adding and removing notes. The input representaion used is pixel-like piano roll, so the approach inherits the problem of not distinguishing long sustains and continuous short onsets. Also, the evaluation is done with comparison against <a href=\"https://www.jmlr.org/papers/volume17/16-272/16-272.pdf\" target=\"_blank\" rel=\"noopener\">orderless NADE</a> and <a href=\"https://arxiv.org/pdf/1903.07227.pdf\" target=\"_blank\" rel=\"noopener\">CoCoNet</a>, but with several recent works suggesting that richer vocabulary of event tokens can improve generation results, it might me interesting to see how this work compares or even adds value on top of these works.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-06.html\" target=\"_blank\" rel=\"noopener\"><strong>PIANOTREE VAE: Structured Representation Learning for Polyphonic Music</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_pianotree.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new hierarchical representation for polyphonic music. Commonly, polyphonic music is either represented by piano rolls (which is commonly treated like pixels), or MIDI event tokens. The authors suggest a <strong>tree-like structure</strong>, where each beat is a tree node, and the notes played on the same beat are the childrens of the node. They also propose a VAE model structure which has one-to-one correspondence with the data structure, and the evaluation shows that as compared to previous representations, PianoTree VAE is superior in terms of reconstruction and downstream music generation.<br>I definitely think that PianoTree has the potential to be the <em>de facto</em> representation of polyphonic music, because indeed it is more reasonable to understand polyphonic music in terms of hierachical structure, as compared to a flat sequence of tokens. However, I personally think that the common usage of PianoTree will depend on two key factor: <strong>the ease of usage</strong> (e.g. open source of encoder components and examples of usage), and whether <strong>the data structure is tightly coupled with the proposed VAE model</strong>. Event tokens are used widespread because any kind of sequence models / NLP models can be ported on top of that representation. Can PianoTree be ported easily to other kinds of architectures, and will the performance on all aspects remain the same? This is a crucial point for whether the structure will replace event tokens and be adopted widely in my opinion.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-05.html\" target=\"_blank\" rel=\"noopener\"><strong>Learning Interpretable Representation for Controllable Polyphonic Music Generation</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_interpretable.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a demonstration of the power of PianoTree VAE above. This time, the authors explore the <strong>disentanglement of chords and texture</strong> of a music piece. The architecture adopts a similar idea as their prior work called <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> (which inspires Music FaderNets a huge lot as well!), where a chord encoder and texture encoder is used for latent representation learning, and a chord decoder with the PianoTree VAE decoder is used for reconstruction. They evaluated the results on three practical generation tasks: compositional style transfer, texture variation via sampling, and accompaniment arrangement. And, their demo and quality of generation is really superb, so it seems like PianoTree could really work well.<br>Meeting the NYU Shanghai team has also been a great experience, especially the discussions with Ziyu Wang has been really enjoyable. Huge kudos to them!</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-13.html\" target=\"_blank\" rel=\"noopener\"><strong>Music FaderNets: Controllable Music Generation Based on High-level Features via Low-level Feature Modelling (My Own Work)</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_fadernets.png\" alt=\"\"/>\n</figure>\n\n<p>My work on controllable polyphonic music generation! At first I wanted to work on controllable geneeration based on emotion, but I found that representations of high-level musical qualities are not easy to learn with supervised learning techniques, either because of the <strong>insufficiency of labels</strong>, or the <strong>subjectiveness</strong> (and hence large variance) in human-annotated labels. We propose to use low-level features as “bridges” to between the music and the high level features. Hence, the model consists of:</p>\n<ul>\n<li><strong>faders</strong>, where each fader controls a low-level attribute of the music sample independently in a continuous manner. This relies on latent regularization and feature disentanglement</li>\n<li><strong>presets</strong>, which learn the relationship between the levels of the sliding knobs of low-level features, and the selected high-level feature. This relies on Gaussian Mixture VAEs which imposes hierachical dependencies.</li>\n</ul>\n<p>This method combines the advantages of <strong>rule-based methods</strong> and <strong>data-driven machine learning</strong>. Rule-based systems are good at interpretability (i.e. you can explicitly hear that some factors are obviously changing during generation), but it is not robust to all situations; whereas machine learning methods are the total opposite. Another interesting point is the usage of <strong>semi-supervised learning</strong>. Since we know that arousal labels are noisy, we can choose only the quality ones with lesser variance and higher representability for training. In this work we prove that lesser labels can be a good thing - using the semi-supervised setting of GM-VAE to train, with only 1% of labelled arousal data, we can learn well-separated, discriminative mixtures. This can provide a feasible approach to learn representations of other kinds of abstract high-level features.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-09.html\" target=\"_blank\" rel=\"noopener\"><strong>Music SketchNet: Controllable Music Generation via Factorized Representations of Pitch and Rhythm</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_sketchnet.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores the application of music inpainting - given partial musical ideas (i.e. music segments), the model is able to “fill up the blanks” with sequences of similar style. An additional controllable factor is provided in this model on pitch and rhythm (pretty much inspired by <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\">EC\\(^2\\)-VAE</a> as well). There are 3 separate components: <strong>SketchVAE</strong> for latent representation learning, <strong>SketchInpainter</strong> for predicting missing measures based on previous and future contexts, and <strong>SketchConnector</strong> which finalizes the generation by simulating user controls with random unmasking (a common technique in training language generators).</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-17.html\" target=\"_blank\" rel=\"noopener\"><strong>The Jazz Transformer on the Front Line: Exploring the Shortcomings of AI-composed Music through Quantitative Measures</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jazz.png\" alt=\"\"/>\n</figure>\n\n<p>This is a really interesting work that tries to answer a lot of pressing questions related to Transformer-based music generation. Are Transformers really that good? If not, what are the culprits? Does structure-related labels help generation?</p>\n<p>For me the real key contributions for this work are the findings concluded on the proposed objective metrics used to evaluate the generated music. There are so many objective metrics being proposed (I recall <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">this work</a> suggesting several metrics for Transformer AE as well), but for Transformers which are often crowned for more structured generation, how do we evaluate structureness other than subjective tests? I find the idea of using <a href=\"https://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S3_ScapePlot.html\" target=\"_blank\" rel=\"noopener\">fitness scape plot</a> to quantify structureness super interesting. Although the field will never agree on a set of evaluation metrics, but understanding where Transformers are still short of in overall will definitely drive the community to pinpoint on certain areas to improve.</p>\n<h2 id=\"3-Disentangled-Representation-Learning\"><a href=\"#3-Disentangled-Representation-Learning\" class=\"headerlink\" title=\"3 - Disentangled Representation Learning\"></a>3 - Disentangled Representation Learning</h2><p><a href=\"https://program.ismir2020.net/poster_5-10.html\" target=\"_blank\" rel=\"noopener\"><strong>Unsupervised Disentanglement of Pitch and Timbre for Isolated Musical Instrument Sounds</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_jyun.png\" alt=\"\"/>\n</figure>\n\n<p>Work by my senpais, Yin-Jyun Luo and and Raven Cheuk, so definitely hands down! Jyun worked on <a href=\"https://arxiv.org/pdf/1906.08152.pdf\" target=\"_blank\" rel=\"noopener\">pitch-timbre disentanglement</a> before, and in this work he decided to push it further - can we do such disentanglement in an unsupervised manner? </p>\n<p>This work employs a key idea: <strong>moderate pitch shiftings will not change timbre</strong>. Hence, even if we don’t have any labels annotated on pitch and timbre, we can still achieve disentanglement by <a href=\"https://paperswithcode.com/task/contrastive-learning\" target=\"_blank\" rel=\"noopener\">contrastive learning paradigms</a> - data augmentation by transposing the pitch, but enforce relations in \\(z_\\textrm{pitch}\\) and \\(z_\\textrm{timbre}\\). The authors propose 4 losses: regression loss, <a href=\"https://arxiv.org/pdf/2002.05709.pdf\" target=\"_blank\" rel=\"noopener\">contrastive loss</a>, <a href=\"https://arxiv.org/pdf/1703.10593v7.pdf\" target=\"_blank\" rel=\"noopener\">cycle consistency loss</a> and a new <strong>surrogate label loss</strong>. I personally think the power of this framework is not just for disentangling timbre and pitch, but unsupervised representation learning as a whole. Can this unsupervised framework be applied on other harder problems (e.g. music sequences, and disentangling musical factors)? How would data augmentation happen in different problems, and would that affect the formulation of losses? These will be interesting questions that require much creativity to explore.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Metric learning VS classification for disentangled music representation learning</strong></a></p>\n<figure>\n  <img style=\"width:105%;\" src=\"/img/ismir_metric.png\" alt=\"\"/>\n</figure>\n\n<p>This interesting work connects 3 things together: metric learning (learns similarity between examples), classification, and disentangled representation learning (which corresponds to <a href=\"http://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_disentangledmusicsim_icassp2020.pdf\" target=\"_blank\" rel=\"noopener\">this work</a>). Firstly, the authors connect classication and metric learning with <strong>proxy-based metric learning</strong>. Then, with all combinations of models and their disentangled version, evaluation is done on 4 types of tasks: training time, similarity retrieval, auto-tagging, and triplet-prediction. Results show that classification-based models are<br>generally advantageous for training time, similarity retrieval, and auto-tagging, while deep metric learning exhibits better performance for triplet-prediction. Disentanglement slightly improves the result on most settings.</p>\n<p><a href=\"https://program.ismir2020.net/poster_1-15.html\" target=\"_blank\" rel=\"noopener\"><strong>dMelodies: A Music Dataset for Disentanglement Learning</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_dmel.png\" alt=\"\"/>\n</figure>\n\n<p>This work proposes a new dataset which resembles <a href=\"https://github.com/deepmind/dsprites-dataset\" target=\"_blank\" rel=\"noopener\">dSprites</a> in the computer vision domain, which is designed for learning and <strong>evaluating disentangled representation learning algorithms for music</strong>. The authors also ran benchmark experiments using common disentanglement methods (\\(\\beta\\)-VAE, Annealed-VAE and Factor-VAE). Overall, the results suggest that disentanglement is comparable, but reconstruction accuracy is much worse, and the sensitivity on hyperparameters are much higher. This again proves the tradeoff between reconstruction and disentanglement / controllability using VAEs on music data.<br>I discussed with the author Ashis Pati on why not use real-world monophonic music dataset (e.g. <a href=\"https://ifdo.ca/~seymour/nottingham/nottingham.html\" target=\"_blank\" rel=\"noopener\">Nottingham dataset</a>) with attribute annotations, but generating synthetic data instead. He suggests that it is to preserve the orthogonality and balanced composition of each attribute within the dataset. It seems like the balance between orthogonality and resemblance to real music is a lot more delicate that expected when creating a dataset like this. (Meanwhile, Ashis’ work has been very crucial to Music FaderNets, and it is such a joy to finally meet him and chat in person. One of the coolest moment during the conference!)</p>\n<h2 id=\"4-Singing-Voice-Conversion\"><a href=\"#4-Singing-Voice-Conversion\" class=\"headerlink\" title=\"4 - Singing Voice Conversion\"></a>4 - Singing Voice Conversion</h2><p><a href=\"https://program.ismir2020.net/poster_1-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Zero-Shot Singing Voice Conversion</strong></a></p>\n<figure>\n  <img style=\"width:80%;\" src=\"/img/ismir_singing.png\" alt=\"\"/>\n</figure>\n\n<p>The most interesting part of this work is the <strong>zero-shot</strong> part, which largely incorporates ideas from the speech domain. Speaker embedding networks were found to be successful for enabling zero-shot voice conversion of speech, whereby the system can model and adapt to new unseen voices on the fly. The authors adopted the same idea for singing voice conversion by using a <a href=\"https://github.com/CorentinJ/Real-Time-Voice-Cloning\" target=\"_blank\" rel=\"noopener\">pretrained speaker embedding network</a>, and then using the WORLD vocoder with learnable parameters for synthesis. It seems like the “pre-trained fine-tune” idea from other domains has influenced much works in MIR, moreover this work shows that using relevant foreign-domain embeddings (speech) on music tasks (singing voice) can actually work.</p>\n<h2 id=\"5-Audio-Synthesis\"><a href=\"#5-Audio-Synthesis\" class=\"headerlink\" title=\"5 - Audio Synthesis\"></a>5 - Audio Synthesis</h2><p><a href=\"https://program.ismir2020.net/poster_4-16.html\" target=\"_blank\" rel=\"noopener\"><strong>DrumGAN: Synthesis of Drum Sounds with Timbral Feature Conditioning Using Generative Adversarial Networks</strong></a></p>\n<figure>\n  <img style=\"width:60%;\" src=\"/img/ismir_drumgan.png\" alt=\"\"/>\n</figure>\n\n<p>Super cool and useful work (can’t wait to use the plugin as a producer)! This work uses a <strong>progressive growing GAN</strong> (similar to the idea in <a href=\"\">GANSynth</a>) to synthesize different types of drum sounds. Moreover, to achieve user controllability, the model allows several factors to be changed during input time, including  brightness, boominess, hardness etc. to synthesize different kinds of drum sounds. To evaluate controllability, unlike using Spearman / Pearson correlation or <a href=\"http://proceedings.mlr.press/v80/adel18a/adel18a.pdf\">R-score in linear regressor</a>, which are more popular in the music generation domain, this work evaluates against several other baseline scores as proposed in <a href=\"https://arxiv.org/pdf/1911.11853.pdf\" target=\"_blank\" rel=\"noopener\">a previous work using U-Net architecture</a>. This could probably shed light to a new spectrum of measurements in terms of factor controllability.</p>\n<p>Another interesting thing is that this work uses <strong>complex STFT spectrogram</strong> as the audio representation. When I worked on piano audio synthesis, the common representation used is the magnitude Mel-spectrogram, which is why for the output a vocoder (e.g. WaveNet, WaveGAN, WaveGlow) is needed to invert Mel-spectrograms to audio. But in this work, the output directly reconstructs the real and imaginary parts of the spectrogram, and to reconstruct the audio we only need to do an inverse STFT. This can ensure better audio reconstruction quality, and phase information might also help audio representation learning.</p>\n<p><em>The remaining topics (source separation, transcription, model pruning and cover song detection) will be covered in <a href=\"/2020/10/17/ismir_2020_pt2/\">Part 2</a></em>.</p>\n"},{"title":"ISMIR 2020 - Part 2","date":"2020-10-17T01:10:42.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 6 - Music Source Separation\n\n[**Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation**](https://program.ismir2020.net/poster_2-04.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\nU-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, **Time-Distributed Blocks** which does not have inter-frame operations, and **Time-Frequency Blocks** which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The [demo](https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab) provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.\n\n[**Content based singing voice source separation via strong conditioning using aligned phonemes**](https://program.ismir2020.net/poster_6-07.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\nThis work explores **informed source separation** - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the [FiLM](https://arxiv.org/pdf/1709.07871.pdf) layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.\n\n[**Exploring Aligned Lyrics-informed Singing Voice Separation**](https://program.ismir2020.net/poster_5-08.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\nSimilar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art [Open Unmix](https://sigsep.github.io/open-unmix/) model, then the authors propose to use an additional **lyric encoder** to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from [text-to-speech](https://paperswithcode.com/task/text-to-speech-synthesis) models, where the text information is encoded to condition on the speech synthesis component.\n\n[**Multitask Learning for Instrument Activation Aware Music Source Separation**](https://program.ismir2020.net/poster_5-16.html)\n\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\nThis work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use **instrument activation detection** as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.\n\n## 7 - Music Transcription / Pitch Estimation\n\n[**Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks**](https://program.ismir2020.net/poster_2-18.html)\n\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\nThis work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) **phase information does help** for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.\n\n[**Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams**](https://program.ismir2020.net/poster_3-01.html)\n\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\nThis is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where **deep clustering** separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an [existing multi-pitch estimator](https://brianmcfee.net/papers/ismir2017_salience.pdf)) provide complementary information for timbre-based clustering and part separation.\n\n[**Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model**](https://program.ismir2020.net/poster_3-17.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\nThis work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an **autoregressive model** by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their [demo](https://program.ismir2020.net/lbd_444.html) is super excellent, such sleek and smooth visualization on real-time music transcription!\n\n## 8 - Model Pruning\n\n[**Ultra-light deep MIR by trimming lottery ticket**](https://program.ismir2020.net/poster_4-11.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\nThe [lottery ticket hypothesis](https://arxiv.org/pdf/1803.03635.pdf) paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as **structured trimming**, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.\n\n## 9 - Cover Song Detection\n\n[**Combining musical features for cover detection**](https://program.ismir2020.net/poster_2-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\nIn previous cover song detection works, either the harmonic-related representation (e.g. [HPCP](https://www.upf.edu/web/mtg/hpcp), [cremaPCP](https://brianmcfee.net/papers/ismir2017_chord.pdf)) or the melody-related representation (e.g. [dominant melody](https://arxiv.org/pdf/1907.01824.pdf), [multi-pitch](https://arxiv.org/pdf/1910.09862.pdf)) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\\\(d_\\textrm{melody}\\\\) and \\\\(d_\\textrm{cremaPCP}\\\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.\n\n[**Less is more: Faster and better music version identification with embedding distillation**](https://program.ismir2020.net/poster_6-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\nIn [a previous work](https://arxiv.org/pdf/1910.12551.pdf), the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a **latent space reconfiguration** method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.\n\n\n## 10 - Last Words on ISMIR 2020\n\nThat's all for my ISMIR 2020! I think the most magical moment for me would be when I could finally chat with some of the authors (and some are really big names!) of the works that I really like throughout my journey of MIR, and furthermore being able to exchange opinions with them. Just hope to be able to meet all of them physically some day!","source":"_posts/ismir_2020_pt2.md","raw":"---\ntitle: ISMIR 2020 - Part 2\ndate: 2020-10-17 09:10:42\ntags:\n    - Music Information Retrieval\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n## 6 - Music Source Separation\n\n[**Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation**](https://program.ismir2020.net/poster_2-04.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\nU-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, **Time-Distributed Blocks** which does not have inter-frame operations, and **Time-Frequency Blocks** which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The [demo](https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab) provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.\n\n[**Content based singing voice source separation via strong conditioning using aligned phonemes**](https://program.ismir2020.net/poster_6-07.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\nThis work explores **informed source separation** - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the [FiLM](https://arxiv.org/pdf/1709.07871.pdf) layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.\n\n[**Exploring Aligned Lyrics-informed Singing Voice Separation**](https://program.ismir2020.net/poster_5-08.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\nSimilar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art [Open Unmix](https://sigsep.github.io/open-unmix/) model, then the authors propose to use an additional **lyric encoder** to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from [text-to-speech](https://paperswithcode.com/task/text-to-speech-synthesis) models, where the text information is encoded to condition on the speech synthesis component.\n\n[**Multitask Learning for Instrument Activation Aware Music Source Separation**](https://program.ismir2020.net/poster_5-16.html)\n\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\nThis work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use **instrument activation detection** as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.\n\n## 7 - Music Transcription / Pitch Estimation\n\n[**Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks**](https://program.ismir2020.net/poster_2-18.html)\n\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\nThis work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) **phase information does help** for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.\n\n[**Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams**](https://program.ismir2020.net/poster_3-01.html)\n\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\nThis is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where **deep clustering** separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an [existing multi-pitch estimator](https://brianmcfee.net/papers/ismir2017_salience.pdf)) provide complementary information for timbre-based clustering and part separation.\n\n[**Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model**](https://program.ismir2020.net/poster_3-17.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\nThis work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an **autoregressive model** by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their [demo](https://program.ismir2020.net/lbd_444.html) is super excellent, such sleek and smooth visualization on real-time music transcription!\n\n## 8 - Model Pruning\n\n[**Ultra-light deep MIR by trimming lottery ticket**](https://program.ismir2020.net/poster_4-11.html)\n\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\nThe [lottery ticket hypothesis](https://arxiv.org/pdf/1803.03635.pdf) paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as **structured trimming**, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.\n\n## 9 - Cover Song Detection\n\n[**Combining musical features for cover detection**](https://program.ismir2020.net/poster_2-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\nIn previous cover song detection works, either the harmonic-related representation (e.g. [HPCP](https://www.upf.edu/web/mtg/hpcp), [cremaPCP](https://brianmcfee.net/papers/ismir2017_chord.pdf)) or the melody-related representation (e.g. [dominant melody](https://arxiv.org/pdf/1907.01824.pdf), [multi-pitch](https://arxiv.org/pdf/1910.09862.pdf)) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\\\(d_\\textrm{melody}\\\\) and \\\\(d_\\textrm{cremaPCP}\\\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.\n\n[**Less is more: Faster and better music version identification with embedding distillation**](https://program.ismir2020.net/poster_6-15.html)\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\nIn [a previous work](https://arxiv.org/pdf/1910.12551.pdf), the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a **latent space reconfiguration** method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.\n\n\n## 10 - Last Words on ISMIR 2020\n\nThat's all for my ISMIR 2020! I think the most magical moment for me would be when I could finally chat with some of the authors (and some are really big names!) of the works that I really like throughout my journey of MIR, and furthermore being able to exchange opinions with them. Just hope to be able to meet all of them physically some day!","slug":"ismir_2020_pt2","published":1,"updated":"2020-10-17T16:33:20.759Z","_id":"ckgcl7cuz000cw19khrquantz","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"6-Music-Source-Separation\"><a href=\"#6-Music-Source-Separation\" class=\"headerlink\" title=\"6 - Music Source Separation\"></a>6 - Music Source Separation</h2><p><a href=\"https://program.ismir2020.net/poster_2-04.html\" target=\"_blank\" rel=\"noopener\"><strong>Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\n<p>U-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, <strong>Time-Distributed Blocks</strong> which does not have inter-frame operations, and <strong>Time-Frequency Blocks</strong> which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The <a href=\"https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab\" target=\"_blank\" rel=\"noopener\">demo</a> provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-07.html\" target=\"_blank\" rel=\"noopener\"><strong>Content based singing voice source separation via strong conditioning using aligned phonemes</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores <strong>informed source separation</strong> - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the <a href=\"https://arxiv.org/pdf/1709.07871.pdf\" target=\"_blank\" rel=\"noopener\">FiLM</a> layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Exploring Aligned Lyrics-informed Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\n<p>Similar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art <a href=\"https://sigsep.github.io/open-unmix/\" target=\"_blank\" rel=\"noopener\">Open Unmix</a> model, then the authors propose to use an additional <strong>lyric encoder</strong> to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from <a href=\"https://paperswithcode.com/task/text-to-speech-synthesis\" target=\"_blank\" rel=\"noopener\">text-to-speech</a> models, where the text information is encoded to condition on the speech synthesis component.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Multitask Learning for Instrument Activation Aware Music Source Separation</strong></a></p>\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\n<p>This work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use <strong>instrument activation detection</strong> as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.</p>\n<h2 id=\"7-Music-Transcription-Pitch-Estimation\"><a href=\"#7-Music-Transcription-Pitch-Estimation\" class=\"headerlink\" title=\"7 - Music Transcription / Pitch Estimation\"></a>7 - Music Transcription / Pitch Estimation</h2><p><a href=\"https://program.ismir2020.net/poster_2-18.html\" target=\"_blank\" rel=\"noopener\"><strong>Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks</strong></a></p>\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) <strong>phase information does help</strong> for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-01.html\" target=\"_blank\" rel=\"noopener\"><strong>Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams</strong></a></p>\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\n<p>This is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where <strong>deep clustering</strong> separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">existing multi-pitch estimator</a>) provide complementary information for timbre-based clustering and part separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-17.html\" target=\"_blank\" rel=\"noopener\"><strong>Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\n<p>This work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an <strong>autoregressive model</strong> by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their <a href=\"https://program.ismir2020.net/lbd_444.html\" target=\"_blank\" rel=\"noopener\">demo</a> is super excellent, such sleek and smooth visualization on real-time music transcription!</p>\n<h2 id=\"8-Model-Pruning\"><a href=\"#8-Model-Pruning\" class=\"headerlink\" title=\"8 - Model Pruning\"></a>8 - Model Pruning</h2><p><a href=\"https://program.ismir2020.net/poster_4-11.html\" target=\"_blank\" rel=\"noopener\"><strong>Ultra-light deep MIR by trimming lottery ticket</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\n<p>The <a href=\"https://arxiv.org/pdf/1803.03635.pdf\" target=\"_blank\" rel=\"noopener\">lottery ticket hypothesis</a> paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as <strong>structured trimming</strong>, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.</p>\n<h2 id=\"9-Cover-Song-Detection\"><a href=\"#9-Cover-Song-Detection\" class=\"headerlink\" title=\"9 - Cover Song Detection\"></a>9 - Cover Song Detection</h2><p><a href=\"https://program.ismir2020.net/poster_2-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Combining musical features for cover detection</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\n<p>In previous cover song detection works, either the harmonic-related representation (e.g. <a href=\"https://www.upf.edu/web/mtg/hpcp\" target=\"_blank\" rel=\"noopener\">HPCP</a>, <a href=\"https://brianmcfee.net/papers/ismir2017_chord.pdf\" target=\"_blank\" rel=\"noopener\">cremaPCP</a>) or the melody-related representation (e.g. <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">dominant melody</a>, <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">multi-pitch</a>) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\(d_\\textrm{melody}\\) and \\(d_\\textrm{cremaPCP}\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Less is more: Faster and better music version identification with embedding distillation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\n<p>In <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">a previous work</a>, the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a <strong>latent space reconfiguration</strong> method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.</p>\n<h2 id=\"10-Last-Words-on-ISMIR-2020\"><a href=\"#10-Last-Words-on-ISMIR-2020\" class=\"headerlink\" title=\"10 - Last Words on ISMIR 2020\"></a>10 - Last Words on ISMIR 2020</h2><p>That’s all for my ISMIR 2020! I think the most magical moment for me would be when I could finally chat with some of the authors (and some are really big names!) of the works that I really like throughout my journey of MIR, and furthermore being able to exchange opinions with them. Just hope to be able to meet all of them physically some day!</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<h2 id=\"6-Music-Source-Separation\"><a href=\"#6-Music-Source-Separation\" class=\"headerlink\" title=\"6 - Music Source Separation\"></a>6 - Music Source Separation</h2><p><a href=\"https://program.ismir2020.net/poster_2-04.html\" target=\"_blank\" rel=\"noopener\"><strong>Investigating U-Nets with various Intermediate Blocks for Spectrogram-based Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_unets.png\" alt=\"\"/>\n</figure>\n\n<p>U-Nets are very common in singing voice separation, with their prior success in image segmentation. This work further inspects the usage of various intermediate blocks by providing comparison and evaluations. 2 types of intermediate blocks are used, <strong>Time-Distributed Blocks</strong> which does not have inter-frame operations, and <strong>Time-Frequency Blocks</strong> which considers both time and frequency domain. The variants of each block are inspected (fully connected, CNN, RNN etc.). The <a href=\"https://www.youtube.com/watch?v=DuOvWpckoVE&feature=youtu.be&ab_channel=KU-Intelligence-Engineering-Lab\" target=\"_blank\" rel=\"noopener\">demo</a> provided by this work is really superb - the best configuration found in this work yields a very clean singing voice separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-07.html\" target=\"_blank\" rel=\"noopener\"><strong>Content based singing voice source separation via strong conditioning using aligned phonemes</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme1.png\" alt=\"\"/>\n</figure>\n\n<p>This work explores <strong>informed source separation</strong> - utilizing prior knowledge about the mixture and target source. In this work, the conditioning information used is lyrics, which are further aligned in the granularity of phonemes. This work uses the <a href=\"https://arxiv.org/pdf/1709.07871.pdf\" target=\"_blank\" rel=\"noopener\">FiLM</a> layer for conditioning, which the conditioning input is a 2D matrix of phonemes w.r.t. time. For weak conditioning, the same FiLM operation to the whole input patch; for strong conditioning, different FiLM operations are computed at different time frames.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-08.html\" target=\"_blank\" rel=\"noopener\"><strong>Exploring Aligned Lyrics-informed Singing Voice Separation</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_phoneme2.png\" alt=\"\"/>\n</figure>\n\n<p>Similar to the above work, this work also utilizes aligned lyrics / phonemes for improving singing voice separation. The architecture is different - this work takes the backbone from the state-of-the-art <a href=\"https://sigsep.github.io/open-unmix/\" target=\"_blank\" rel=\"noopener\">Open Unmix</a> model, then the authors propose to use an additional <strong>lyric encoder</strong> to learn embeddings for conditioning on the backbone. This idea resembles much with the idea from <a href=\"https://paperswithcode.com/task/text-to-speech-synthesis\" target=\"_blank\" rel=\"noopener\">text-to-speech</a> models, where the text information is encoded to condition on the speech synthesis component.</p>\n<p><a href=\"https://program.ismir2020.net/poster_5-16.html\" target=\"_blank\" rel=\"noopener\"><strong>Multitask Learning for Instrument Activation Aware Music Source Separation</strong></a></p>\n<figure>\n  <img style=\"width:50%;\" src=\"/img/ismir_multitask.png\" alt=\"\"/>\n</figure>\n\n<p>This work leverages multitask learning for source separation. Multitask learning states that by choosing a relevant subsidiary task, and allow it to train in line with the original task, can improve the performance of the original task. This work chooses to use <strong>instrument activation detection</strong> as the subsidary task, because it can intuitively suppress wrongly predicted activation by the source separation model at the supposed silent segments. By training on a larger dataset with multitask learning, the model can perform better on almost all aspects as compared to Open Unmix.</p>\n<h2 id=\"7-Music-Transcription-Pitch-Estimation\"><a href=\"#7-Music-Transcription-Pitch-Estimation\" class=\"headerlink\" title=\"7 - Music Transcription / Pitch Estimation\"></a>7 - Music Transcription / Pitch Estimation</h2><p><a href=\"https://program.ismir2020.net/poster_2-18.html\" target=\"_blank\" rel=\"noopener\"><strong>Multiple F0 Estimation in Vocal Ensembles using Convolutional Neural Networks</strong></a></p>\n<figure>\n  <img style=\"width:65%;\" src=\"/img/ismir_vocal.png\" alt=\"\"/>\n</figure>\n\n<p>This work is a direct adaptation of CNNs on F0 estimation, applying on vocal ensembles. The key takeaways for me in this work is of 3-fold: (i) <strong>phase information does help</strong> for F0 estimation tasks (would it also be the same for other tasks? this will be interesting to explore); (ii) deeper models will work better; (iii) late concatenation of magnitude and phase information works better than early concatenation of both.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-01.html\" target=\"_blank\" rel=\"noopener\"><strong>Multi-Instrument Music Transcription Based on Deep Spherical Clustering of Spectrograms and Pitchgrams</strong></a></p>\n<figure>\n  <img style=\"width:90%;\" src=\"/img/ismir_spherical.png\" alt=\"\"/>\n</figure>\n\n<p>This is a super interesting work! For previous music transcription works, the output will be of a pre-defined set of instruments, with activation predicted for each instrument. This work intends to transcribe arbitrary instruments, hence being able to transcribe undefined instruments that are not included in the training data. The key idea is also inspired by methods from the speech domain, where <strong>deep clustering</strong> separates a speech mixture to an arbitrary number of speakers based on the characteristics of voices. Hence, the spectrograms and pitchgrams (estimated by an <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">existing multi-pitch estimator</a>) provide complementary information for timbre-based clustering and part separation.</p>\n<p><a href=\"https://program.ismir2020.net/poster_3-17.html\" target=\"_blank\" rel=\"noopener\"><strong>Polyphonic Piano Transcription Using Autoregressive Multi-state Note Model</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_transcription.png\" alt=\"\"/>\n</figure>\n\n<p>This work recognizes the problem of frame-level transcription: some frames might start after the onset events, which makes it harder to distinguish and transcribe. To solve this, the authors use an <strong>autoregressive model</strong> by utilizing the time-frequency and predicted transcription of the previous frame, and feeding them during the training of current step. Training of the autoregressive model is done via teacher-forcing. Results show that the model provides significantly higher accuracy on both note onset and offset estimation compared to its non-auto-regressive version. And just one thing to add: their <a href=\"https://program.ismir2020.net/lbd_444.html\" target=\"_blank\" rel=\"noopener\">demo</a> is super excellent, such sleek and smooth visualization on real-time music transcription!</p>\n<h2 id=\"8-Model-Pruning\"><a href=\"#8-Model-Pruning\" class=\"headerlink\" title=\"8 - Model Pruning\"></a>8 - Model Pruning</h2><p><a href=\"https://program.ismir2020.net/poster_4-11.html\" target=\"_blank\" rel=\"noopener\"><strong>Ultra-light deep MIR by trimming lottery ticket</strong></a></p>\n<figure>\n  <img style=\"width:70%;\" src=\"/img/ismir_lottery.png\" alt=\"\"/>\n</figure>\n\n<p>The <a href=\"https://arxiv.org/pdf/1803.03635.pdf\" target=\"_blank\" rel=\"noopener\">lottery ticket hypothesis</a> paper is the best paper in ICLR 2020, which motivates me to looking into this interesting work. Also, model compression is a really useful technique in an industrial setting as it significantly reduces memory footprint when scaling up to large-scale applications. With the new proposed approach by the authors known as <strong>structured trimming</strong>, which remove units based on magnitude, activation and normalization-based criteria, model size can be even more lighter without trading off much in terms of accuracy. The cool thing of this paper is that it evaluates the trimmed model on various popular MIR tasks, and these efficient trimmed subnetworks, removing up to 85% of the weights in deep models, could be found.</p>\n<h2 id=\"9-Cover-Song-Detection\"><a href=\"#9-Cover-Song-Detection\" class=\"headerlink\" title=\"9 - Cover Song Detection\"></a>9 - Cover Song Detection</h2><p><a href=\"https://program.ismir2020.net/poster_2-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Combining musical features for cover detection</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_doras.png\" alt=\"\"/>\n</figure>\n\n<p>In previous cover song detection works, either the harmonic-related representation (e.g. <a href=\"https://www.upf.edu/web/mtg/hpcp\" target=\"_blank\" rel=\"noopener\">HPCP</a>, <a href=\"https://brianmcfee.net/papers/ismir2017_chord.pdf\" target=\"_blank\" rel=\"noopener\">cremaPCP</a>) or the melody-related representation (e.g. <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">dominant melody</a>, <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">multi-pitch</a>) is used. This work simply puts both together, and explores various fusion methods to inspect its improvement. The key intuition is that some cover songs are similar in harmonic content but not in dominant melody, and some are of the opposite. The interesting finding is that with only a simple average aggregation of \\(d_\\textrm{melody}\\) and \\(d_\\textrm{cremaPCP}\\), the model is able to yield the best improvement over individual models, and (strangely) it performs even better than a more sophisticated late fusion model.</p>\n<p><a href=\"https://program.ismir2020.net/poster_6-15.html\" target=\"_blank\" rel=\"noopener\"><strong>Less is more: Faster and better music version identification with embedding distillation</strong></a></p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/ismir_furkan.png\" alt=\"\"/>\n</figure>\n\n<p>In <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">a previous work</a>, the authors proposed a musically-motivated embedding learning model for cover song detection, but the required embedding size is pretty huge at around 16,000. In this work, the authors experimented with various methods to reduce the amount of dimension in the embedding for large-scale retrieval applications. The results show that with a <strong>latent space reconfiguration</strong> method, which is very similar to transfer learning methods by fine-tuning additional dense layers on a pre-trained model, coupling with a normalized softmax loss, the model can achieve the best performance even under an embedding size of 256. Strangely, this performs better than training the whole network + dense layers from scratch.</p>\n<h2 id=\"10-Last-Words-on-ISMIR-2020\"><a href=\"#10-Last-Words-on-ISMIR-2020\" class=\"headerlink\" title=\"10 - Last Words on ISMIR 2020\"></a>10 - Last Words on ISMIR 2020</h2><p>That’s all for my ISMIR 2020! I think the most magical moment for me would be when I could finally chat with some of the authors (and some are really big names!) of the works that I really like throughout my journey of MIR, and furthermore being able to exchange opinions with them. Just hope to be able to meet all of them physically some day!</p>\n"},{"title":"Parameterized Pooling Layers","date":"2020-11-25T02:24:37.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Two parameterized pooling layers which aim to improve beyond average and max pooling\n2 - The techniques introduced are: [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf)\n<br/>\n\n## 1 - Introduction\n\nPooling layers in deep learning serve the purpose of **aggregating information** - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? \n\nThe very first encounter of most deep learning practitioners with pooling layers should be within the stack of \"conv - pool - relu\" block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to **downsample** the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on **temporal aggregation** for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\\\((d,)\\\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\\\((M, T)\\\\), where \\\\(T\\\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.\n\nThe most common pooling methods are either **average pooling** or **max pooling**. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion & democracy, whereas max pooling is more like tyranny & eliticism where only the best speaks.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\nThe also explains why average pooling produces **smoother, blurrer** feature maps, and max pooling produces **sharper, discontinuous** feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: **is there a way to exploit states between average and max pooling?** Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\\\(p\\\\), such that it gives us the best pooled output for our own application? This is the motivation of **parameterized / adaptive pooling** methods.\n\nBelow I will discuss two methods that I recently read up, which is [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf). Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.\n\n<br/>\n\n## 2 - AutoPool\n\nAutoPool, proposed by McFee et al, generalizes the pooling equation as below:\n\n$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(\\alpha \\in [0, \\infty)\\\\) is the trainable scalar parameter.\n\nWe can easily see that this equation takes the form of a **weighted sum** - each element \\\\(x\\\\), contributes to the final aggregated value \\\\(y\\\\), with a weight factor determined by function \\\\(w\\\\).\n\n1. when \\\\(\\alpha = 0\\\\), it is clear that \\\\(w(\\alpha, x) = \\frac{1}{|X|}\\\\) because \\\\(e^{\\alpha \\cdot x} = 1\\\\) and the denominator resembles the number of elements in \\\\(X\\\\). The corresponds to **average pooling**, and each value has equal contribution.\n\n2. when \\\\(\\alpha = 1\\\\), the authors term this as **softmax pooling**, as each value contributes with a factor of its softmax value.\n\n3. when \\\\(\\alpha \\to \\infty\\\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} = \n\\displaystyle\\lim_{\\alpha \\to \\infty}\n\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + ... }$$ Hence, by dividing \\\\(x_{max}\\\\) on both numerator and denominator, we can see that only if \\\\(x = x_{max}\\\\), then the limit equals to \\\\(1\\\\), or else the limit equals to \\\\(0\\\\). We can see that this corresponds to **max pooling**.\n\n<br/>\n\n## 2 - Generalized Mean Pooling (GeMPool)\n\nGeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:\n\n$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(p \\in [1, \\infty)\\\\) is the trainable scalar parameter.\n\n1. when \\\\(p = 1\\\\), this clearly corresponds to **average pooling**;\n\n2. when \\\\(p \\to \\infty\\\\), it corresponds to **max pooling**. A way to prove this is to calculate the following limit:\n$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + ...)^\\frac{1}{p} = x_{max}$$\n\n<br/>\n\n## 3 - Other Aggregating Mechanisms\n\nBoth methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the **exponent** of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that **the contributing factor of large values increase**, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn't been conducted before to the best of my knowledge.\n\nA more sophisticated method of aggregating values is to use **attention**, as a weightage is learnt for each value, known as **attention mask**, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.\n\n<br/>\n\n## 4 - Code Implementation\n\nI provide the portals to the original source code / reimplementation of the parameterized pooling methods:\n1. [AutoPool official implementation in Keras](https://github.com/marl/autopool/blob/master/autopool/autopool.py)\n2. [Generalized Mean Pooling reimplementation in PyTorch](https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py)\n3. [Github Gist on both pooling methods in TF2 Keras](https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01) \n\n\n\n\n\n","source":"_posts/param-pooling.md","raw":"---\ntitle: Parameterized Pooling Layers\ndate: 2020-11-25 10:24:37\ntags:\n    - Music Signal Processing\n    - Deep Learning\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - Two parameterized pooling layers which aim to improve beyond average and max pooling\n2 - The techniques introduced are: [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf)\n<br/>\n\n## 1 - Introduction\n\nPooling layers in deep learning serve the purpose of **aggregating information** - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? \n\nThe very first encounter of most deep learning practitioners with pooling layers should be within the stack of \"conv - pool - relu\" block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to **downsample** the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on **temporal aggregation** for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\\\((d,)\\\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\\\((M, T)\\\\), where \\\\(T\\\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.\n\nThe most common pooling methods are either **average pooling** or **max pooling**. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion & democracy, whereas max pooling is more like tyranny & eliticism where only the best speaks.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\nThe also explains why average pooling produces **smoother, blurrer** feature maps, and max pooling produces **sharper, discontinuous** feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: **is there a way to exploit states between average and max pooling?** Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\\\(p\\\\), such that it gives us the best pooled output for our own application? This is the motivation of **parameterized / adaptive pooling** methods.\n\nBelow I will discuss two methods that I recently read up, which is [AutoPool](https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf) and [Generalized Mean Pooling (GeMPool)](https://arxiv.org/pdf/1711.02512.pdf). Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.\n\n<br/>\n\n## 2 - AutoPool\n\nAutoPool, proposed by McFee et al, generalizes the pooling equation as below:\n\n$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(\\alpha \\in [0, \\infty)\\\\) is the trainable scalar parameter.\n\nWe can easily see that this equation takes the form of a **weighted sum** - each element \\\\(x\\\\), contributes to the final aggregated value \\\\(y\\\\), with a weight factor determined by function \\\\(w\\\\).\n\n1. when \\\\(\\alpha = 0\\\\), it is clear that \\\\(w(\\alpha, x) = \\frac{1}{|X|}\\\\) because \\\\(e^{\\alpha \\cdot x} = 1\\\\) and the denominator resembles the number of elements in \\\\(X\\\\). The corresponds to **average pooling**, and each value has equal contribution.\n\n2. when \\\\(\\alpha = 1\\\\), the authors term this as **softmax pooling**, as each value contributes with a factor of its softmax value.\n\n3. when \\\\(\\alpha \\to \\infty\\\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} = \n\\displaystyle\\lim_{\\alpha \\to \\infty}\n\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + ... }$$ Hence, by dividing \\\\(x_{max}\\\\) on both numerator and denominator, we can see that only if \\\\(x = x_{max}\\\\), then the limit equals to \\\\(1\\\\), or else the limit equals to \\\\(0\\\\). We can see that this corresponds to **max pooling**.\n\n<br/>\n\n## 2 - Generalized Mean Pooling (GeMPool)\n\nGeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:\n\n$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$\n\nwhere \\\\(y\\\\) is the aggregated value, \\\\(X\\\\) is the set of values, and \\\\(p \\in [1, \\infty)\\\\) is the trainable scalar parameter.\n\n1. when \\\\(p = 1\\\\), this clearly corresponds to **average pooling**;\n\n2. when \\\\(p \\to \\infty\\\\), it corresponds to **max pooling**. A way to prove this is to calculate the following limit:\n$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + ...)^\\frac{1}{p} = x_{max}$$\n\n<br/>\n\n## 3 - Other Aggregating Mechanisms\n\nBoth methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the **exponent** of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that **the contributing factor of large values increase**, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn't been conducted before to the best of my knowledge.\n\nA more sophisticated method of aggregating values is to use **attention**, as a weightage is learnt for each value, known as **attention mask**, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.\n\n<br/>\n\n## 4 - Code Implementation\n\nI provide the portals to the original source code / reimplementation of the parameterized pooling methods:\n1. [AutoPool official implementation in Keras](https://github.com/marl/autopool/blob/master/autopool/autopool.py)\n2. [Generalized Mean Pooling reimplementation in PyTorch](https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py)\n3. [Github Gist on both pooling methods in TF2 Keras](https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01) \n\n\n\n\n\n","slug":"param-pooling","published":1,"updated":"2020-11-25T08:45:22.962Z","_id":"ckhwutnte00002f9kgkfwhopl","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Two parameterized pooling layers which aim to improve beyond average and max pooling<br>2 - The techniques introduced are: <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a><br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Pooling layers in deep learning serve the purpose of <strong>aggregating information</strong> - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? </p>\n<p>The very first encounter of most deep learning practitioners with pooling layers should be within the stack of “conv - pool - relu” block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to <strong>downsample</strong> the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on <strong>temporal aggregation</strong> for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\((d,)\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\((M, T)\\), where \\(T\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.</p>\n<p>The most common pooling methods are either <strong>average pooling</strong> or <strong>max pooling</strong>. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion &amp; democracy, whereas max pooling is more like tyranny &amp; eliticism where only the best speaks.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\n<p>The also explains why average pooling produces <strong>smoother, blurrer</strong> feature maps, and max pooling produces <strong>sharper, discontinuous</strong> feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: <strong>is there a way to exploit states between average and max pooling?</strong> Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\(p\\), such that it gives us the best pooled output for our own application? This is the motivation of <strong>parameterized / adaptive pooling</strong> methods.</p>\n<p>Below I will discuss two methods that I recently read up, which is <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a>. Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.</p>\n<br/>\n\n<h2 id=\"2-AutoPool\"><a href=\"#2-AutoPool\" class=\"headerlink\" title=\"2 - AutoPool\"></a>2 - AutoPool</h2><p>AutoPool, proposed by McFee et al, generalizes the pooling equation as below:</p>\n<p>$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(\\alpha \\in [0, \\infty)\\) is the trainable scalar parameter.</p>\n<p>We can easily see that this equation takes the form of a <strong>weighted sum</strong> - each element \\(x\\), contributes to the final aggregated value \\(y\\), with a weight factor determined by function \\(w\\).</p>\n<ol>\n<li><p>when \\(\\alpha = 0\\), it is clear that \\(w(\\alpha, x) = \\frac{1}{|X|}\\) because \\(e^{\\alpha \\cdot x} = 1\\) and the denominator resembles the number of elements in \\(X\\). The corresponds to <strong>average pooling</strong>, and each value has equal contribution.</p>\n</li>\n<li><p>when \\(\\alpha = 1\\), the authors term this as <strong>softmax pooling</strong>, as each value contributes with a factor of its softmax value.</p>\n</li>\n<li><p>when \\(\\alpha \\to \\infty\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} =<br>\\displaystyle\\lim_{\\alpha \\to \\infty}<br>\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + … }$$ Hence, by dividing \\(x_{max}\\) on both numerator and denominator, we can see that only if \\(x = x_{max}\\), then the limit equals to \\(1\\), or else the limit equals to \\(0\\). We can see that this corresponds to <strong>max pooling</strong>.</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"2-Generalized-Mean-Pooling-GeMPool\"><a href=\"#2-Generalized-Mean-Pooling-GeMPool\" class=\"headerlink\" title=\"2 - Generalized Mean Pooling (GeMPool)\"></a>2 - Generalized Mean Pooling (GeMPool)</h2><p>GeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:</p>\n<p>$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(p \\in [1, \\infty)\\) is the trainable scalar parameter.</p>\n<ol>\n<li><p>when \\(p = 1\\), this clearly corresponds to <strong>average pooling</strong>;</p>\n</li>\n<li><p>when \\(p \\to \\infty\\), it corresponds to <strong>max pooling</strong>. A way to prove this is to calculate the following limit:<br>$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + …)^\\frac{1}{p} = x_{max}$$</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"3-Other-Aggregating-Mechanisms\"><a href=\"#3-Other-Aggregating-Mechanisms\" class=\"headerlink\" title=\"3 - Other Aggregating Mechanisms\"></a>3 - Other Aggregating Mechanisms</h2><p>Both methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the <strong>exponent</strong> of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that <strong>the contributing factor of large values increase</strong>, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn’t been conducted before to the best of my knowledge.</p>\n<p>A more sophisticated method of aggregating values is to use <strong>attention</strong>, as a weightage is learnt for each value, known as <strong>attention mask</strong>, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.</p>\n<br/>\n\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>I provide the portals to the original source code / reimplementation of the parameterized pooling methods:</p>\n<ol>\n<li><a href=\"https://github.com/marl/autopool/blob/master/autopool/autopool.py\" target=\"_blank\" rel=\"noopener\">AutoPool official implementation in Keras</a></li>\n<li><a href=\"https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling reimplementation in PyTorch</a></li>\n<li><a href=\"https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01\" target=\"_blank\" rel=\"noopener\">Github Gist on both pooling methods in TF2 Keras</a> </li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - Two parameterized pooling layers which aim to improve beyond average and max pooling<br>2 - The techniques introduced are: <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a><br><br/></p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Pooling layers in deep learning serve the purpose of <strong>aggregating information</strong> - given a bunch of numbers, how do I summarize them into 1 number which represents this bunch of numbers the most? </p>\n<p>The very first encounter of most deep learning practitioners with pooling layers should be within the stack of “conv - pool - relu” block in image classification architectures, e.g. LeNet, ResNet, etc. Pooling layers come after convolution layers, with the purpose to <strong>downsample</strong> the image, also hoping to produce a more compact representation within a lower dimension. Another common usage of pooling layers is on <strong>temporal aggregation</strong> for sequence data, e.g. summarizing values across a time axis. For example, to learn an embedding (e.g. song embedding, sentence embedding) of shape \\((d,)\\) from a 2-D sequence data (e.g. spectrograms, word embeddings) with shape \\((M, T)\\), where \\(T\\) is the temporal axis, it is very common to apply pooling on the temporal axis to reduce the representation into 1-D.</p>\n<p>The most common pooling methods are either <strong>average pooling</strong> or <strong>max pooling</strong>. Average pooling takes the mean of a given set of values, hence the contribution of each value to the final aggregated value is equal. Whereas, max pooling takes only the max value, hence the max value contributes fully to the final aggregated value. A (probably inappropiate) analogy will be: average pooling is more like collective opinion &amp; democracy, whereas max pooling is more like tyranny &amp; eliticism where only the best speaks.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/mean-vs-max-pool.png\" alt=\"\"/>\n  <figcaption><br/>Figure 1: Average Pooling VS Max Pooling. Is there a way to exploit states between both?</figcaption>\n</figure>\n\n<p>The also explains why average pooling produces <strong>smoother, blurrer</strong> feature maps, and max pooling produces <strong>sharper, discontinuous</strong> feature maps. However, it is not guaranteed that either of the representation is the best for our applications. The question is: <strong>is there a way to exploit states between average and max pooling?</strong> Furthermore, can we rely on backpropagation and deep learning to learn a parameter \\(p\\), such that it gives us the best pooled output for our own application? This is the motivation of <strong>parameterized / adaptive pooling</strong> methods.</p>\n<p>Below I will discuss two methods that I recently read up, which is <a href=\"https://www.justinsalamon.com/uploads/4/3/9/4/4394963/mcfee_autopool_taslp_2018.pdf\" target=\"_blank\" rel=\"noopener\">AutoPool</a> and <a href=\"https://arxiv.org/pdf/1711.02512.pdf\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling (GeMPool)</a>. Both methods are commonly used in papers across signal processing, MIR, and image recognition applications.</p>\n<br/>\n\n<h2 id=\"2-AutoPool\"><a href=\"#2-AutoPool\" class=\"headerlink\" title=\"2 - AutoPool\"></a>2 - AutoPool</h2><p>AutoPool, proposed by McFee et al, generalizes the pooling equation as below:</p>\n<p>$$w(\\alpha, x) = \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} \\\\  y = \\displaystyle\\sum_{x \\in X} x \\cdot w(\\alpha, x)$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(\\alpha \\in [0, \\infty)\\) is the trainable scalar parameter.</p>\n<p>We can easily see that this equation takes the form of a <strong>weighted sum</strong> - each element \\(x\\), contributes to the final aggregated value \\(y\\), with a weight factor determined by function \\(w\\).</p>\n<ol>\n<li><p>when \\(\\alpha = 0\\), it is clear that \\(w(\\alpha, x) = \\frac{1}{|X|}\\) because \\(e^{\\alpha \\cdot x} = 1\\) and the denominator resembles the number of elements in \\(X\\). The corresponds to <strong>average pooling</strong>, and each value has equal contribution.</p>\n</li>\n<li><p>when \\(\\alpha = 1\\), the authors term this as <strong>softmax pooling</strong>, as each value contributes with a factor of its softmax value.</p>\n</li>\n<li><p>when \\(\\alpha \\to \\infty\\), the max value will have more contributing factor. This is because $$\\displaystyle\\lim_{\\alpha \\to \\infty} \\frac{e^{\\alpha \\cdot x}}{\\displaystyle\\sum_{z \\in X} e^{\\alpha \\cdot z}} =<br>\\displaystyle\\lim_{\\alpha \\to \\infty}<br>\\frac{ (\\frac{e^{\\alpha \\cdot x}}{e^{\\alpha \\cdot x_{max}}}) } { 1 + (\\frac{e^{\\alpha \\cdot x_1}}{e^{\\alpha \\cdot x_{max}}}) + (\\frac{e^{\\alpha \\cdot x_2}}{e^{\\alpha \\cdot x_{max}}}) + … }$$ Hence, by dividing \\(x_{max}\\) on both numerator and denominator, we can see that only if \\(x = x_{max}\\), then the limit equals to \\(1\\), or else the limit equals to \\(0\\). We can see that this corresponds to <strong>max pooling</strong>.</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"2-Generalized-Mean-Pooling-GeMPool\"><a href=\"#2-Generalized-Mean-Pooling-GeMPool\" class=\"headerlink\" title=\"2 - Generalized Mean Pooling (GeMPool)\"></a>2 - Generalized Mean Pooling (GeMPool)</h2><p>GeMPool, first proposed by Radenovic et al., generalizes the pooling equation as below:</p>\n<p>$$y = (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}}$$</p>\n<p>where \\(y\\) is the aggregated value, \\(X\\) is the set of values, and \\(p \\in [1, \\infty)\\) is the trainable scalar parameter.</p>\n<ol>\n<li><p>when \\(p = 1\\), this clearly corresponds to <strong>average pooling</strong>;</p>\n</li>\n<li><p>when \\(p \\to \\infty\\), it corresponds to <strong>max pooling</strong>. A way to prove this is to calculate the following limit:<br>$$ \\lim_{p \\to \\infty} (\\frac{1}{|X|} \\displaystyle\\sum_{x \\in X} x^p)^{\\frac{1}{p}} = \\lim_{p \\to \\infty} (\\frac{1}{|X|})^\\frac{1}{p} \\cdot x_{max} \\cdot ((\\frac{x_1}{x_{max}})^{p} + (\\frac{x_2}{x_{max}})^{p} + …)^\\frac{1}{p} = x_{max}$$</p>\n</li>\n</ol>\n<br/>\n\n<h2 id=\"3-Other-Aggregating-Mechanisms\"><a href=\"#3-Other-Aggregating-Mechanisms\" class=\"headerlink\" title=\"3 - Other Aggregating Mechanisms\"></a>3 - Other Aggregating Mechanisms</h2><p>Both methods aforementioned are parameterizing pooling methods with a single scalar value. We find the common design of such equation is to parameterize the <strong>exponent</strong> of the equation. We see that when the exponent is at its base value, the equation falls back to average pooling. As the value of exponent is increased, we can see that <strong>the contributing factor of large values increase</strong>, where for small values the contributing factor decreases. Several papers and applications have conducted ablation studies that show parameterized pooling improves model performance, but comparison across different parameterized pooling methods hasn’t been conducted before to the best of my knowledge.</p>\n<p>A more sophisticated method of aggregating values is to use <strong>attention</strong>, as a weightage is learnt for each value, known as <strong>attention mask</strong>, however the amount of parameters on the aggregation also scales up w.r.t the size of values. It will be exciting to see if pooling mechanisms and attention mechanisms could be compared side-by-side in terms of bringing improvement to model performance.</p>\n<br/>\n\n<h2 id=\"4-Code-Implementation\"><a href=\"#4-Code-Implementation\" class=\"headerlink\" title=\"4 - Code Implementation\"></a>4 - Code Implementation</h2><p>I provide the portals to the original source code / reimplementation of the parameterized pooling methods:</p>\n<ol>\n<li><a href=\"https://github.com/marl/autopool/blob/master/autopool/autopool.py\" target=\"_blank\" rel=\"noopener\">AutoPool official implementation in Keras</a></li>\n<li><a href=\"https://github.com/JDAI-CV/fast-reid/fastreid/layers/pooling.py\" target=\"_blank\" rel=\"noopener\">Generalized Mean Pooling reimplementation in PyTorch</a></li>\n<li><a href=\"https://gist.github.com/gudgud96/72d6530a5a4ecaece09532e0ed1b3e01\" target=\"_blank\" rel=\"noopener\">Github Gist on both pooling methods in TF2 Keras</a> </li>\n</ol>\n"},{"title":"Challenges in Productionizing Cover Detection Systems","date":"2021-02-25T11:12:09.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief survey on recent cover song detection systems\n2 - Challenges in deploying cover song detection systems to production\n\n\n## 1 - Introduction\n\nRecently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. \n\nThe experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed \"baby\" - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a **very, very, very long way** from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.\n\nI could now understand clearly why the **model is often not the primary concern within the stack**, especially when the team is resource limited. More resources can be directed to R&D afterwards, but a **seamlessly served model**, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n## 2 - A Very Brief Survey on Recent Advances in Cover Song Detection\n\nCover song detection, to the music industry, is the **potential \"upgraded\" version of audio fingerprinting systems**, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to **identify \"music of any version, in any performance / cover, in any form\"**.\n\nTo the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.\n\n### Dominant Melody-Based\n\nThe idea is to match the **dominant melody** of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by [Doras et al. 2019](https://arxiv.org/pdf/1907.01824.pdf) and [Doras et al. 2020](https://arxiv.org/pdf/1910.09862.pdf), which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include [Sailer et al.](https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf) and [Tsai et al.](https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758), which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\nHowever, the performance of dominant melody based solutions is tightly coupled with **the accuracy of the extracted dominant melody** (see [this recent work](https://brianmcfee.net/papers/ismir2017_salience.pdf) on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) \"wobbly\" pitch glides due to singing techniques. For alignment-based methods, since we often need **pitch intervals** to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.\n\n### Harmonic-Based\n\nThe idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by [MOVE](https://arxiv.org/pdf/1910.12551.pdf) and [Re-MOVE](https://arxiv.org/pdf/2010.03284.pdf) which uses [cremaPCP](https://github.com/bmcfee/crema) as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, [Serra et al.](https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf)'s method using [HPCP](https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles) as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\nThe potential problem in harmonic-based methods is that there can be **more false positives** in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in **quadratic time**, which imposes a limit on detection speed and hence harder to scale.\n\n### Hybrid Methods\n\nThe most recent hybrid attempt is by [Yesiler and Doras](https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y) which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include [MFCC and HPCP fusion](https://arxiv.org/pdf/1707.04680.pdf), with improvements using [ensemble-based comparison](https://arxiv.org/pdf/1905.11700.pdf).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\nFrom a system standpoint, hybrid methods **add levels of complexity** when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.\n\nThere is also an important work which introduces the representation of **2D Fourier Transform** (2DFT) for CSD by [Seetharaman et al.](https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf). 2DFT (see [this video](https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen) for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on \"faithful covers\", but failed when the cover has a larger extent of variation. \n\n### End-to-End Based\n\nEnd-to-end based systems are often favoured due to its **simplicity** for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including [CQTNet](https://arxiv.org/pdf/1911.00334.pdf), [TPPNet](https://www.ijcai.org/Proceedings/2019/0673.pdf), and the recent [ByteCover](https://arxiv.org/pdf/2010.14022.pdf) lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a [person re-identification problem](https://paperswithcode.com/task/person-re-identification), and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n## 3 - Thoughts and Discussion regarding CSD in Production\n\nI would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.\n\n### Snippet Detection\n\nBecause CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only **few seconds of recording**. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.\n\nBut, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) **doesn't show resemblance** / **marginally resembles** with the reference (irrelevant sections chosen); or more often (ii) **resembles more with other references** depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don't generalize well to snippet forms of query - alignment based methods are dependent on query & reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by [Zalkow et al.](https://www.mdpi.com/2076-3417/10/1/19/pdf) which works on \"shingles\" in classical music.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\nResearch work also did not focus on **which section (where)** in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW & Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. \n\n### Benchmark Results May Not Transfer To Other Datasets\n\nThe performance of CSD algorithms are highly dependent on **what kind of corpus you are comparing against**. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many \"competitive candidates\" for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. \n\nAnother note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if **transfer learning** (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.\n\n### Metrics Used May Not Reflect Practical Needs\n\nFor a very long time, CSD has been formulated as an **information retrieval** problem - \"given a song, can you retrieve the most similar cover tracks?\" This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like \"given a track (original / cover), can you tell me which work it belongs to?\", which is more relevant to an **identification problem** (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.\n\n### Computer Vision-Based Models Perform Best?\n\nByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see [this re-ID strong baseline paper](https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf)). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed **\"musically-aware\"** network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, [pooling methods](https://gudgud96.github.io/2020/11/25/param-pooling/) etc.)? This would be a question that I would love to seek answer for.\n\n## 4 - Conclusion\n\nCSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.\n\n## 5 - Further References\n1 - Yesiler et al. - [Version Identification in the 20s - ISMIR2020](https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49), ISMIR 2020 Tutorial.\n2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - [link](https://medias.ircam.fr/x9f5132)\n<br/>","source":"_posts/challenge-csd.md","raw":"---\ntitle: Challenges in Productionizing Cover Detection Systems\ndate: 2021-02-25 19:12:09\ntags:\n    - Music Signal Processing\n    - ML in Production\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very brief survey on recent cover song detection systems\n2 - Challenges in deploying cover song detection systems to production\n\n\n## 1 - Introduction\n\nRecently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. \n\nThe experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed \"baby\" - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a **very, very, very long way** from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.\n\nI could now understand clearly why the **model is often not the primary concern within the stack**, especially when the team is resource limited. More resources can be directed to R&D afterwards, but a **seamlessly served model**, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n## 2 - A Very Brief Survey on Recent Advances in Cover Song Detection\n\nCover song detection, to the music industry, is the **potential \"upgraded\" version of audio fingerprinting systems**, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to **identify \"music of any version, in any performance / cover, in any form\"**.\n\nTo the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.\n\n### Dominant Melody-Based\n\nThe idea is to match the **dominant melody** of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by [Doras et al. 2019](https://arxiv.org/pdf/1907.01824.pdf) and [Doras et al. 2020](https://arxiv.org/pdf/1910.09862.pdf), which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include [Sailer et al.](https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf) and [Tsai et al.](https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758), which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\nHowever, the performance of dominant melody based solutions is tightly coupled with **the accuracy of the extracted dominant melody** (see [this recent work](https://brianmcfee.net/papers/ismir2017_salience.pdf) on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) \"wobbly\" pitch glides due to singing techniques. For alignment-based methods, since we often need **pitch intervals** to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.\n\n### Harmonic-Based\n\nThe idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by [MOVE](https://arxiv.org/pdf/1910.12551.pdf) and [Re-MOVE](https://arxiv.org/pdf/2010.03284.pdf) which uses [cremaPCP](https://github.com/bmcfee/crema) as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, [Serra et al.](https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf)'s method using [HPCP](https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles) as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\nThe potential problem in harmonic-based methods is that there can be **more false positives** in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in **quadratic time**, which imposes a limit on detection speed and hence harder to scale.\n\n### Hybrid Methods\n\nThe most recent hybrid attempt is by [Yesiler and Doras](https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y) which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include [MFCC and HPCP fusion](https://arxiv.org/pdf/1707.04680.pdf), with improvements using [ensemble-based comparison](https://arxiv.org/pdf/1905.11700.pdf).\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\nFrom a system standpoint, hybrid methods **add levels of complexity** when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.\n\nThere is also an important work which introduces the representation of **2D Fourier Transform** (2DFT) for CSD by [Seetharaman et al.](https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf). 2DFT (see [this video](https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen) for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on \"faithful covers\", but failed when the cover has a larger extent of variation. \n\n### End-to-End Based\n\nEnd-to-end based systems are often favoured due to its **simplicity** for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including [CQTNet](https://arxiv.org/pdf/1911.00334.pdf), [TPPNet](https://www.ijcai.org/Proceedings/2019/0673.pdf), and the recent [ByteCover](https://arxiv.org/pdf/2010.14022.pdf) lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a [person re-identification problem](https://paperswithcode.com/task/person-re-identification), and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n## 3 - Thoughts and Discussion regarding CSD in Production\n\nI would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.\n\n### Snippet Detection\n\nBecause CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only **few seconds of recording**. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.\n\nBut, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) **doesn't show resemblance** / **marginally resembles** with the reference (irrelevant sections chosen); or more often (ii) **resembles more with other references** depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don't generalize well to snippet forms of query - alignment based methods are dependent on query & reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by [Zalkow et al.](https://www.mdpi.com/2076-3417/10/1/19/pdf) which works on \"shingles\" in classical music.\n\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\nResearch work also did not focus on **which section (where)** in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW & Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. \n\n### Benchmark Results May Not Transfer To Other Datasets\n\nThe performance of CSD algorithms are highly dependent on **what kind of corpus you are comparing against**. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many \"competitive candidates\" for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. \n\nAnother note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if **transfer learning** (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.\n\n### Metrics Used May Not Reflect Practical Needs\n\nFor a very long time, CSD has been formulated as an **information retrieval** problem - \"given a song, can you retrieve the most similar cover tracks?\" This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like \"given a track (original / cover), can you tell me which work it belongs to?\", which is more relevant to an **identification problem** (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.\n\n### Computer Vision-Based Models Perform Best?\n\nByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see [this re-ID strong baseline paper](https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf)). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed **\"musically-aware\"** network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, [pooling methods](https://gudgud96.github.io/2020/11/25/param-pooling/) etc.)? This would be a question that I would love to seek answer for.\n\n## 4 - Conclusion\n\nCSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.\n\n## 5 - Further References\n1 - Yesiler et al. - [Version Identification in the 20s - ISMIR2020](https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49), ISMIR 2020 Tutorial.\n2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - [link](https://medias.ircam.fr/x9f5132)\n<br/>","slug":"challenge-csd","published":1,"updated":"2021-03-02T17:48:23.485Z","_id":"cklqf68yn0000y59k0kv6fs2z","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief survey on recent cover song detection systems<br>2 - Challenges in deploying cover song detection systems to production</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. </p>\n<p>The experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed “baby” - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a <strong>very, very, very long way</strong> from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.</p>\n<p>I could now understand clearly why the <strong>model is often not the primary concern within the stack</strong>, especially when the team is resource limited. More resources can be directed to R&amp;D afterwards, but a <strong>seamlessly served model</strong>, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n<h2 id=\"2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\"><a href=\"#2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\" class=\"headerlink\" title=\"2 - A Very Brief Survey on Recent Advances in Cover Song Detection\"></a>2 - A Very Brief Survey on Recent Advances in Cover Song Detection</h2><p>Cover song detection, to the music industry, is the <strong>potential “upgraded” version of audio fingerprinting systems</strong>, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to <strong>identify “music of any version, in any performance / cover, in any form”</strong>.</p>\n<p>To the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.</p>\n<h3 id=\"Dominant-Melody-Based\"><a href=\"#Dominant-Melody-Based\" class=\"headerlink\" title=\"Dominant Melody-Based\"></a>Dominant Melody-Based</h3><p>The idea is to match the <strong>dominant melody</strong> of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2019</a> and <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2020</a>, which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include <a href=\"https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf\" target=\"_blank\" rel=\"noopener\">Sailer et al.</a> and <a href=\"https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758\" target=\"_blank\" rel=\"noopener\">Tsai et al.</a>, which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\n<p>However, the performance of dominant melody based solutions is tightly coupled with <strong>the accuracy of the extracted dominant melody</strong> (see <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">this recent work</a> on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) “wobbly” pitch glides due to singing techniques. For alignment-based methods, since we often need <strong>pitch intervals</strong> to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.</p>\n<h3 id=\"Harmonic-Based\"><a href=\"#Harmonic-Based\" class=\"headerlink\" title=\"Harmonic-Based\"></a>Harmonic-Based</h3><p>The idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">MOVE</a> and <a href=\"https://arxiv.org/pdf/2010.03284.pdf\" target=\"_blank\" rel=\"noopener\">Re-MOVE</a> which uses <a href=\"https://github.com/bmcfee/crema\" target=\"_blank\" rel=\"noopener\">cremaPCP</a> as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, <a href=\"https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf\" target=\"_blank\" rel=\"noopener\">Serra et al.</a>‘s method using <a href=\"https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles\" target=\"_blank\" rel=\"noopener\">HPCP</a> as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\n<p>The potential problem in harmonic-based methods is that there can be <strong>more false positives</strong> in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in <strong>quadratic time</strong>, which imposes a limit on detection speed and hence harder to scale.</p>\n<h3 id=\"Hybrid-Methods\"><a href=\"#Hybrid-Methods\" class=\"headerlink\" title=\"Hybrid Methods\"></a>Hybrid Methods</h3><p>The most recent hybrid attempt is by <a href=\"https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y\" target=\"_blank\" rel=\"noopener\">Yesiler and Doras</a> which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include <a href=\"https://arxiv.org/pdf/1707.04680.pdf\" target=\"_blank\" rel=\"noopener\">MFCC and HPCP fusion</a>, with improvements using <a href=\"https://arxiv.org/pdf/1905.11700.pdf\" target=\"_blank\" rel=\"noopener\">ensemble-based comparison</a>.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\n<p>From a system standpoint, hybrid methods <strong>add levels of complexity</strong> when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.</p>\n<p>There is also an important work which introduces the representation of <strong>2D Fourier Transform</strong> (2DFT) for CSD by <a href=\"https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf\" target=\"_blank\" rel=\"noopener\">Seetharaman et al.</a>. 2DFT (see <a href=\"https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen\" target=\"_blank\" rel=\"noopener\">this video</a> for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on “faithful covers”, but failed when the cover has a larger extent of variation. </p>\n<h3 id=\"End-to-End-Based\"><a href=\"#End-to-End-Based\" class=\"headerlink\" title=\"End-to-End Based\"></a>End-to-End Based</h3><p>End-to-end based systems are often favoured due to its <strong>simplicity</strong> for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including <a href=\"https://arxiv.org/pdf/1911.00334.pdf\" target=\"_blank\" rel=\"noopener\">CQTNet</a>, <a href=\"https://www.ijcai.org/Proceedings/2019/0673.pdf\" target=\"_blank\" rel=\"noopener\">TPPNet</a>, and the recent <a href=\"https://arxiv.org/pdf/2010.14022.pdf\" target=\"_blank\" rel=\"noopener\">ByteCover</a> lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a <a href=\"https://paperswithcode.com/task/person-re-identification\" target=\"_blank\" rel=\"noopener\">person re-identification problem</a>, and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n<h2 id=\"3-Thoughts-and-Discussion-regarding-CSD-in-Production\"><a href=\"#3-Thoughts-and-Discussion-regarding-CSD-in-Production\" class=\"headerlink\" title=\"3 - Thoughts and Discussion regarding CSD in Production\"></a>3 - Thoughts and Discussion regarding CSD in Production</h2><p>I would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.</p>\n<h3 id=\"Snippet-Detection\"><a href=\"#Snippet-Detection\" class=\"headerlink\" title=\"Snippet Detection\"></a>Snippet Detection</h3><p>Because CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only <strong>few seconds of recording</strong>. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.</p>\n<p>But, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) <strong>doesn’t show resemblance</strong> / <strong>marginally resembles</strong> with the reference (irrelevant sections chosen); or more often (ii) <strong>resembles more with other references</strong> depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don’t generalize well to snippet forms of query - alignment based methods are dependent on query &amp; reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by <a href=\"https://www.mdpi.com/2076-3417/10/1/19/pdf\" target=\"_blank\" rel=\"noopener\">Zalkow et al.</a> which works on “shingles” in classical music.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\n<p>Research work also did not focus on <strong>which section (where)</strong> in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW &amp; Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. </p>\n<h3 id=\"Benchmark-Results-May-Not-Transfer-To-Other-Datasets\"><a href=\"#Benchmark-Results-May-Not-Transfer-To-Other-Datasets\" class=\"headerlink\" title=\"Benchmark Results May Not Transfer To Other Datasets\"></a>Benchmark Results May Not Transfer To Other Datasets</h3><p>The performance of CSD algorithms are highly dependent on <strong>what kind of corpus you are comparing against</strong>. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many “competitive candidates” for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. </p>\n<p>Another note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if <strong>transfer learning</strong> (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.</p>\n<h3 id=\"Metrics-Used-May-Not-Reflect-Practical-Needs\"><a href=\"#Metrics-Used-May-Not-Reflect-Practical-Needs\" class=\"headerlink\" title=\"Metrics Used May Not Reflect Practical Needs\"></a>Metrics Used May Not Reflect Practical Needs</h3><p>For a very long time, CSD has been formulated as an <strong>information retrieval</strong> problem - “given a song, can you retrieve the most similar cover tracks?” This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like “given a track (original / cover), can you tell me which work it belongs to?”, which is more relevant to an <strong>identification problem</strong> (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.</p>\n<h3 id=\"Computer-Vision-Based-Models-Perform-Best\"><a href=\"#Computer-Vision-Based-Models-Perform-Best\" class=\"headerlink\" title=\"Computer Vision-Based Models Perform Best?\"></a>Computer Vision-Based Models Perform Best?</h3><p>ByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see <a href=\"https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\" target=\"_blank\" rel=\"noopener\">this re-ID strong baseline paper</a>). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed <strong>“musically-aware”</strong> network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, <a href=\"https://gudgud96.github.io/2020/11/25/param-pooling/\">pooling methods</a> etc.)? This would be a question that I would love to seek answer for.</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4 - Conclusion\"></a>4 - Conclusion</h2><p>CSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.</p>\n<h2 id=\"5-Further-References\"><a href=\"#5-Further-References\" class=\"headerlink\" title=\"5 - Further References\"></a>5 - Further References</h2><p>1 - Yesiler et al. - <a href=\"https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49\" target=\"_blank\" rel=\"noopener\">Version Identification in the 20s - ISMIR2020</a>, ISMIR 2020 Tutorial.<br>2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - <a href=\"https://medias.ircam.fr/x9f5132\" target=\"_blank\" rel=\"noopener\">link</a><br><br/></p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very brief survey on recent cover song detection systems<br>2 - Challenges in deploying cover song detection systems to production</p>\n<h2 id=\"1-Introduction\"><a href=\"#1-Introduction\" class=\"headerlink\" title=\"1 - Introduction\"></a>1 - Introduction</h2><p>Recently, I had the opportunity to experiment, build and deploy cover detection systems (CSD) to production. I would love to take this chance to note down some observations and thoughts throughout building the system, and summarize some issues that I find while deploying such systems to production. </p>\n<p>The experience of bringing academia work into production is a mixture of exciting and demoralizing moments. The exciting part is that you are really creating value for the users / stakeholders with your meticulously-trained, carefully-assessed “baby” - your model. Sometimes, it might even be the case that the faster the inference speed of your model / system, the more revenue is generated. The demoralizing part is that there is a <strong>very, very, very long way</strong> from bringing academia models to serving production use cases. A model with 95% accuracy on benchmarks would not suffice, it also has to be fast enough, cost-effective, has minimal downtime, best not to drain too much GPU money, and most importantly robust enough to serve any use cases provided by (often more than one type of) clients. 95% of the problems are often very boring problems, but they are necessary to make the 5% interesting part shine.</p>\n<p>I could now understand clearly why the <strong>model is often not the primary concern within the stack</strong>, especially when the team is resource limited. More resources can be directed to R&amp;D afterwards, but a <strong>seamlessly served model</strong>, though mediocre in performance, with minimal downtime and latency, is of priority to showcase the potential of the proposed technology and drive momentum.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-ml-ops.png\" alt=\"\"/>\n</figure>\n\n<h2 id=\"2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\"><a href=\"#2-A-Very-Brief-Survey-on-Recent-Advances-in-Cover-Song-Detection\" class=\"headerlink\" title=\"2 - A Very Brief Survey on Recent Advances in Cover Song Detection\"></a>2 - A Very Brief Survey on Recent Advances in Cover Song Detection</h2><p>Cover song detection, to the music industry, is the <strong>potential “upgraded” version of audio fingerprinting systems</strong>, because audio fingerprinting systems can only identify originals, but it cannot withstand variance in instrumentation / arrangement. Whereas for CSD, if we can already identify a cover track, then identifying the original track is basically a trivial problem. This is why CSD systems are of high interests in e.g. the music rights / licensing / publishing bodies, to <strong>identify “music of any version, in any performance / cover, in any form”</strong>.</p>\n<p>To the very best of my knowledge, I roughly categorized the common types of CSD algorithms into the following 4 categories: dominant melody based, harmonic based, hybrid methods, and end-to-end based.</p>\n<h3 id=\"Dominant-Melody-Based\"><a href=\"#Dominant-Melody-Based\" class=\"headerlink\" title=\"Dominant Melody-Based\"></a>Dominant Melody-Based</h3><p>The idea is to match the <strong>dominant melody</strong> of the same composition, because cover tracks share similar dominant melody patterns, although it might be transposed to a different pitch. The most recent work is by <a href=\"https://arxiv.org/pdf/1907.01824.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2019</a> and <a href=\"https://arxiv.org/pdf/1910.09862.pdf\" target=\"_blank\" rel=\"noopener\">Doras et al. 2020</a>, which trains a network to learn dominant melody embeddings that reflect melody similarity via variants of triplet-loss functions. Several works in this category include <a href=\"https://www.music-ir.org/mirex/abstracts/2006/CS_sailer.pdf\" target=\"_blank\" rel=\"noopener\">Sailer et al.</a> and <a href=\"https://jise.iis.sinica.edu.tw/JISESearch/pages/View/PaperView.jsf?keyId=45_758\" target=\"_blank\" rel=\"noopener\">Tsai et al.</a>, which commonly extract the dominant melody, calculate the pitch intervals (for pitch invariance) and run alignment algorithms such as dynamic time warping or Smith-Waterman algorithm to retrieve a similarity score.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-dominant.png\" alt=\"\"/>\n  <figcaption><br/>Dominant melodies extracted for original and cover track.</figcaption>\n</figure>\n\n<p>However, the performance of dominant melody based solutions is tightly coupled with <strong>the accuracy of the extracted dominant melody</strong> (see <a href=\"https://brianmcfee.net/papers/ismir2017_salience.pdf\" target=\"_blank\" rel=\"noopener\">this recent work</a> on F0 estimation). Dominant melody extraction might be disrupted by (i) mistaking accompaniment as melody, or vice versa, and (ii) “wobbly” pitch glides due to singing techniques. For alignment-based methods, since we often need <strong>pitch intervals</strong> to calculate cover similarity, it is highly sensitive to the unwanted notes introduced in the melody extraction phase. Dominant melody methods could also have missed out songs with (i) raps (no-pitch content), and also (ii) instrumentals because models are often built catering towards vocal tracks. If the melody extraction module is a trained neural network, it could also be biased on e.g. the genre, vocal presence, vocal gender of tracks it is trained on, hence lack generalization.</p>\n<h3 id=\"Harmonic-Based\"><a href=\"#Harmonic-Based\" class=\"headerlink\" title=\"Harmonic-Based\"></a>Harmonic-Based</h3><p>The idea is to use tonal features, e.g. chromas (or pitch class profiles) or chords, as covers share similar tonal progression. The most recent work is by <a href=\"https://arxiv.org/pdf/1910.12551.pdf\" target=\"_blank\" rel=\"noopener\">MOVE</a> and <a href=\"https://arxiv.org/pdf/2010.03284.pdf\" target=\"_blank\" rel=\"noopener\">Re-MOVE</a> which uses <a href=\"https://github.com/bmcfee/crema\" target=\"_blank\" rel=\"noopener\">cremaPCP</a> as the feature representation, training (musically motivated) neural networks to learn similarity via triplet-loss functions. For a long time, <a href=\"https://iopscience.iop.org/article/10.1088/1367-2630/11/9/093017/pdf\" target=\"_blank\" rel=\"noopener\">Serra et al.</a>‘s method using <a href=\"https://en.wikipedia.org/wiki/Harmonic_pitch_class_profiles\" target=\"_blank\" rel=\"noopener\">HPCP</a> as representation, calculating cross recurrence plots and calculating similarity scores using the QMax algorithm has been the state-of-the-art method in CSD.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hpcp.png\" alt=\"\"/>\n  <figcaption><br/>HPCP cross recurrence plot and cover similarity via QMax algorithm.</figcaption>\n</figure>\n\n<p>The potential problem in harmonic-based methods is that there can be <strong>more false positives</strong> in a larger corpus, because there exists more tracks with similar harmonic progressions / pitch class profiles (especially in the pop genre) when compared to a reference track. For non-neural-network methods, algorithms aligning 2D cross recurrence plots between query and reference are in <strong>quadratic time</strong>, which imposes a limit on detection speed and hence harder to scale.</p>\n<h3 id=\"Hybrid-Methods\"><a href=\"#Hybrid-Methods\" class=\"headerlink\" title=\"Hybrid Methods\"></a>Hybrid Methods</h3><p>The most recent hybrid attempt is by <a href=\"https://repositori.upf.edu/bitstream/handle/10230/45719/doras_ismir_combi.pdf?sequence=1&isAllowed=y\" target=\"_blank\" rel=\"noopener\">Yesiler and Doras</a> which combines both dominant melody and cremaPCP as representations. The paper illustrates that both features are complementary and a simple averaging in scores could boost the performance. Some other hybrid methods include <a href=\"https://arxiv.org/pdf/1707.04680.pdf\" target=\"_blank\" rel=\"noopener\">MFCC and HPCP fusion</a>, with improvements using <a href=\"https://arxiv.org/pdf/1905.11700.pdf\" target=\"_blank\" rel=\"noopener\">ensemble-based comparison</a>.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-hybrid.png\" alt=\"\"/>\n  <figcaption><br/>Normalized distance plot for: dominant melody VS cremaPCP (left), multi-pitch VS CQT (mid), and cremaPCP VS Chroma (right). Each feature reflects a different aspect of similarity, hence suggesting complementarity via feature combination.</figcaption>\n</figure>\n\n<p>From a system standpoint, hybrid methods <strong>add levels of complexity</strong> when building the CSD system. Parallelizing the extraction of multiple input features and the respective processing steps could be more complex depending on the pipeline, and it would require more resources to maintain the more components involved and the higher level of complexity.</p>\n<p>There is also an important work which introduces the representation of <strong>2D Fourier Transform</strong> (2DFT) for CSD by <a href=\"https://interactiveaudiolab.github.io/assets/papers/seetharaman_rafii_icassp17.pdf\" target=\"_blank\" rel=\"noopener\">Seetharaman et al.</a>. 2DFT (see <a href=\"https://www.youtube.com/watch?v=Iz6C1ny-F2Q&ab_channel=BarryVanVeen\" target=\"_blank\" rel=\"noopener\">this video</a> for explanation) breaks down images into sums of sinusoidal grids at different periods and orientations, represented by points in the 2DFT. Running 2DFT on CQT spectrogram gives a key-invariant representation of the audio. The model achieved good results on “faithful covers”, but failed when the cover has a larger extent of variation. </p>\n<h3 id=\"End-to-End-Based\"><a href=\"#End-to-End-Based\" class=\"headerlink\" title=\"End-to-End Based\"></a>End-to-End Based</h3><p>End-to-end based systems are often favoured due to its <strong>simplicity</strong> for building, as you only need a single component instead of multiple components to make the system work. A series of work by Yu et al. including <a href=\"https://arxiv.org/pdf/1911.00334.pdf\" target=\"_blank\" rel=\"noopener\">CQTNet</a>, <a href=\"https://www.ijcai.org/Proceedings/2019/0673.pdf\" target=\"_blank\" rel=\"noopener\">TPPNet</a>, and the recent <a href=\"https://arxiv.org/pdf/2010.14022.pdf\" target=\"_blank\" rel=\"noopener\">ByteCover</a> lies in this domain. The idea is to use just CQT spectrograms as input representations, and train carefully designed neural networks to directly output the similarity score between two songs. ByteCover even referenced CSD as a <a href=\"https://paperswithcode.com/task/person-re-identification\" target=\"_blank\" rel=\"noopener\">person re-identification problem</a>, and its architecture design is largely adapted from re-ID, while achieving state-of-the-art performance by far.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-bytecover.png\" alt=\"\"/>\n  <figcaption><br/>ByteCover architecture.</figcaption>\n</figure>\n\n<h2 id=\"3-Thoughts-and-Discussion-regarding-CSD-in-Production\"><a href=\"#3-Thoughts-and-Discussion-regarding-CSD-in-Production\" class=\"headerlink\" title=\"3 - Thoughts and Discussion regarding CSD in Production\"></a>3 - Thoughts and Discussion regarding CSD in Production</h2><p>I would love to discuss the four issues below that I have encountered while building CSD systems in production, which shows some different concerns between production and research.</p>\n<h3 id=\"Snippet-Detection\"><a href=\"#Snippet-Detection\" class=\"headerlink\" title=\"Snippet Detection\"></a>Snippet Detection</h3><p>Because CSD is a potential upgrade for audio fingerprinting systems, it is pretty much hoped to perform like e.g. Shazam / Soundhound, which can detect a track within only <strong>few seconds of recording</strong>. Acoustic fingerprinting is very good in this scenario because you can already find confident matches of fingerprint hashes with only seconds of recording.</p>\n<p>But, detecting a cover song from just snippets is totally different - there can be cases where the seconds exhibit in the query (i) <strong>doesn’t show resemblance</strong> / <strong>marginally resembles</strong> with the reference (irrelevant sections chosen); or more often (ii) <strong>resembles more with other references</strong> depending on the feature used (e.g. similar melody / tonal progression). Currently, most models don’t generalize well to snippet forms of query - alignment based methods are dependent on query &amp; reference lengths, and deep-learning based methods are trained on corpuses of full tracks. Most CSD research also do not tackle this aspect of the problem - the closest I could find would be by <a href=\"https://www.mdpi.com/2076-3417/10/1/19/pdf\" target=\"_blank\" rel=\"noopener\">Zalkow et al.</a> which works on “shingles” in classical music.</p>\n<figure>\n  <img style=\"width:100%;\" src=\"/img/csd-shingles.png\" alt=\"\"/>\n</figure>\n\n<p>Research work also did not focus on <strong>which section (where)</strong> in the reference has the highest resemblance with the query (or vice versa). This is extremely useful for identifying e.g. long remixes / performances with more than one work involved. Alignment-based methods like DTW &amp; Smith Waterman are natural for answering this question, but it might be non-trivial for deep metric-learning based methods. </p>\n<h3 id=\"Benchmark-Results-May-Not-Transfer-To-Other-Datasets\"><a href=\"#Benchmark-Results-May-Not-Transfer-To-Other-Datasets\" class=\"headerlink\" title=\"Benchmark Results May Not Transfer To Other Datasets\"></a>Benchmark Results May Not Transfer To Other Datasets</h3><p>The performance of CSD algorithms are highly dependent on <strong>what kind of corpus you are comparing against</strong>. I find it possible to have a model performing very well on large, well-known benchmark datasets, but it could still perform badly on another small, curated test set, simply because there are too many “competitive candidates” for your queries in this particular dataset, depending on the features you used. An example I failed on is to use pitch class profiles as feature representation, and test on a small set of Chinese ballad songs, which often have very similar chord progressions and tonality. </p>\n<p>Another note is that the current biggest open-sourced CSD dataset generally represents Western music context, and might not be generalizable to other regional music genres and types. It might be an exciting problem to explore if <strong>transfer learning</strong> (pre-train - fine-tune) helps CSD models adapt from one genre to another. To sum up, there are too many aspects of variations that cover songs could possess, and no single public benchmark dataset could possibly summarize all of them in its entirety.</p>\n<h3 id=\"Metrics-Used-May-Not-Reflect-Practical-Needs\"><a href=\"#Metrics-Used-May-Not-Reflect-Practical-Needs\" class=\"headerlink\" title=\"Metrics Used May Not Reflect Practical Needs\"></a>Metrics Used May Not Reflect Practical Needs</h3><p>For a very long time, CSD has been formulated as an <strong>information retrieval</strong> problem - “given a song, can you retrieve the most similar cover tracks?” This is why retrieval based metrics like mean average precision (mAP), mean rank, P@10 etc. are used in academia up until now. However, there rarely is a use case for CSD in such recommendation-like scenarios. More often, the use case looks like “given a track (original / cover), can you tell me which work it belongs to?”, which is more relevant to an <strong>identification problem</strong> (and much like person re-ID). Hence, metrics like top K accuracy, precision, recall, etc. should be a more suitable and straightforward metric to assess the system. However, most research papers do not report these metrics and hence making it difficult to compare on them.</p>\n<h3 id=\"Computer-Vision-Based-Models-Perform-Best\"><a href=\"#Computer-Vision-Based-Models-Perform-Best\" class=\"headerlink\" title=\"Computer Vision-Based Models Perform Best?\"></a>Computer Vision-Based Models Perform Best?</h3><p>ByteCover is currently performing best on most of the large-scale benchmark datasets, including SHS-100K and Da-TaCos. The backbone of ByteCover is basically a ResNet-IBN model, which is a common architecture used in face re-identification problems (see <a href=\"https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\" target=\"_blank\" rel=\"noopener\">this re-ID strong baseline paper</a>). This makes me wonder if CSD problems, or even MIR problems, can be solved in general using computer-vision based methods by merely having music represented in CQT spectrograms, even replicating the trajectory of model improvements proposed in the re-ID domain. If common CV-based models work so well, this also makes me wonder if previous proposed <strong>“musically-aware”</strong> network architectures are actually learning about music features that we desire. Is domain-specific architecture design less important, as compared to general model training techniques (e.g. annealed learning rate, BNNeck, loss function choices, <a href=\"https://gudgud96.github.io/2020/11/25/param-pooling/\">pooling methods</a> etc.)? This would be a question that I would love to seek answer for.</p>\n<h2 id=\"4-Conclusion\"><a href=\"#4-Conclusion\" class=\"headerlink\" title=\"4 - Conclusion\"></a>4 - Conclusion</h2><p>CSD systems are gaining more and more attention in the music tech field, from startups to huge DSPs, especially due to the increase in amount of published music thanks to digital streaming, which creates a huge demand for efficient rights management, and hence accurate music identification systems. Given the long history of CSD, there might already be answers for solving some of the problems mentioned above, and there will definitely be a strong demand for bridging academia research and industry needs in this field (much like the face recognition domain years ago). It would be no doubt that CSD technology will play a vital role in the music industry, especially on the publishing, licensing, royalties payout and legal aspects in the very near future.</p>\n<h2 id=\"5-Further-References\"><a href=\"#5-Further-References\" class=\"headerlink\" title=\"5 - Further References\"></a>5 - Further References</h2><p>1 - Yesiler et al. - <a href=\"https://docs.google.com/presentation/d/17GDjTE9GV0cWxpYlsiXLvgPkVAg70Ho4RwPUyyL-j0U/edit#slide=id.g9602847f92_0_49\" target=\"_blank\" rel=\"noopener\">Version Identification in the 20s - ISMIR2020</a>, ISMIR 2020 Tutorial.<br>2 - PhD thesis Defence on Cover Song Detection by Guillaume Doras - <a href=\"https://medias.ircam.fr/x9f5132\" target=\"_blank\" rel=\"noopener\">link</a><br><br/></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ck5s3jzfj00004qv5g9865rpz","tag_id":"ck5s3yyg300064qv5evr99y2v","_id":"ck5s3yyg400074qv5e8ls7ygf"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0iy0001tbm86k7r8jw0","_id":"ck89oi0j10003tbm8gm9e58j0"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0j00002tbm8b5htefr6","_id":"ck89oi0j10004tbm8ajru04wh"},{"post_id":"ck8jt5xfe0000jbm81um92vdu","tag_id":"ck8jt5xfj0001jbm8dllq10kz","_id":"ck8jt5xfl0002jbm8e18p3kpx"},{"post_id":"ck8jt5xfe0000jbm81um92vdu","tag_id":"ck89oi0j00002tbm8b5htefr6","_id":"ck8jt5xfl0003jbm87hvxh4b4"},{"post_id":"ckbpvdoks0000qlm88ndz3z7a","tag_id":"ck89oi0iy0001tbm86k7r8jw0","_id":"ckbpvdol00001qlm8ek7a0zws"},{"post_id":"ckgcjd4cq0000w19khjzs6q1z","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"ckgcjd4d10003w19kc0mv76d4"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ckgcjd4dd0004w19k4yhyhcc8","_id":"ckgcjd4df0006w19kgrnrbwl8"},{"post_id":"ckbpvdoks0000qlm88ndz3z7a","tag_id":"ckgcjd4dd0004w19k4yhyhcc8","_id":"ckgcjd4df0007w19khky7a6je"},{"post_id":"ckgcjeans0008w19k9uev444u","tag_id":"ckgcjf4oj000aw19k1ebe5cjw","_id":"ckgcjf4oj000bw19k784r7fzv"},{"post_id":"ckgcl7cuz000cw19khrquantz","tag_id":"ckgcjf4oj000aw19k1ebe5cjw","_id":"ckgclabjt000dw19k5h5f9nhi"},{"post_id":"ckhwutnte00002f9kgkfwhopl","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"ckhwuu4ct00012f9kdfgc58kn"},{"post_id":"ckhwutnte00002f9kgkfwhopl","tag_id":"ckhwuuf6000022f9kaxjk187i","_id":"ckhwuuf6100032f9k2blm007o"},{"post_id":"cklqf68yn0000y59k0kv6fs2z","tag_id":"ckgcjd4cy0002w19k82ta7lzp","_id":"cklsa7ayi0004y59k512sdj7j"},{"post_id":"cklqf68yn0000y59k0kv6fs2z","tag_id":"cklqf68yw0001y59k3pu4dzx4","_id":"cklsa7ayq0005y59kfkdndop0"}],"Tag":[{"name":"centOS","_id":"ck5rzh79g00018dv5hvt3fro0"},{"name":"redis","_id":"ck5rzh79h00028dv5bcno9ru7"},{"name":"test1","_id":"ck5rzhckr00058dv5e1cj86cx"},{"name":"test2","_id":"ck5rzhcks00068dv55wag4oae"},{"name":"General Thoughts","_id":"ck5s3yyg300064qv5evr99y2v"},{"name":"AI Music","_id":"ck5s3z7pb00084qv54k2z6635"},{"name":"VAE","_id":"ck89oi0iy0001tbm86k7r8jw0"},{"name":"Symbolic Music","_id":"ck89oi0j00002tbm8b5htefr6"},{"name":"Transformer","_id":"ck8jt5xfj0001jbm8dllq10kz"},{"name":"Music Signal Processing","_id":"ckgcjd4cy0002w19k82ta7lzp"},{"name":"Music Representation Learning","_id":"ckgcjd4dd0004w19k4yhyhcc8"},{"name":"Music Information Retrieval","_id":"ckgcjf4oj000aw19k1ebe5cjw"},{"name":"Deep Learning","_id":"ckhwuuf6000022f9kaxjk187i"},{"name":"ML in Production","_id":"cklqf68yw0001y59k3pu4dzx4"}]}}