{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"source/img/cook.jpg","path":"img/cook.jpg","modified":0,"renderable":0},{"_id":"source/img/dao.png","path":"img/dao.png","modified":0,"renderable":0},{"_id":"source/img/fifa-deep-nn.png","path":"img/fifa-deep-nn.png","modified":0,"renderable":0},{"_id":"source/img/ge14.jpg","path":"img/ge14.jpg","modified":0,"renderable":0},{"_id":"source/img/german.jpg","path":"img/german.jpg","modified":0,"renderable":0},{"_id":"source/img/happiness.jpg","path":"img/happiness.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_1.jpg","path":"img/img_small_1.jpg","modified":0,"renderable":0},{"_id":"source/img/img_small_2.jpg","path":"img/img_small_2.jpg","modified":0,"renderable":0},{"_id":"source/img/linear-regressor-knockout.PNG","path":"img/linear-regressor-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/loc.png","path":"img/loc.png","modified":0,"renderable":0},{"_id":"source/img/logo.png","path":"img/logo.png","modified":0,"renderable":0},{"_id":"source/img/model_performance.png","path":"img/model_performance.png","modified":0,"renderable":0},{"_id":"source/img/music-supply.jpg","path":"img/music-supply.jpg","modified":0,"renderable":0},{"_id":"source/img/nn-knockout.PNG","path":"img/nn-knockout.PNG","modified":0,"renderable":0},{"_id":"source/img/religion.png","path":"img/religion.png","modified":0,"renderable":0},{"_id":"source/img/slide_1.jpg","path":"img/slide_1.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_2.jpg","path":"img/slide_2.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_3.jpg","path":"img/slide_3.jpg","modified":0,"renderable":0},{"_id":"source/img/slide_4.jpg","path":"img/slide_4.jpg","modified":0,"renderable":0},{"_id":"source/img/trust.jpg","path":"img/trust.jpg","modified":0,"renderable":0},{"_id":"source/img/work-for-you.jpg","path":"img/work-for-you.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-life.jpg","path":"img/ai-life.jpg","modified":0,"renderable":0},{"_id":"source/img/belgium.jpg","path":"img/belgium.jpg","modified":0,"renderable":0},{"_id":"source/img/blockchain.png","path":"img/blockchain.png","modified":0,"renderable":0},{"_id":"source/img/money.jpg","path":"img/money.jpg","modified":0,"renderable":0},{"_id":"source/img/passing-mark.jpg","path":"img/passing-mark.jpg","modified":0,"renderable":0},{"_id":"source/img/prof.png","path":"img/prof.png","modified":0,"renderable":0},{"_id":"source/img/results.png","path":"img/results.png","modified":0,"renderable":0},{"_id":"source/img/science-empire.jpg","path":"img/science-empire.jpg","modified":0,"renderable":0},{"_id":"source/img/ubi.jpeg","path":"img/ubi.jpeg","modified":0,"renderable":0},{"_id":"source/img/unstoppable.jpg","path":"img/unstoppable.jpg","modified":0,"renderable":0},{"_id":"source/img/win-ai.jpg","path":"img/win-ai.jpg","modified":0,"renderable":0},{"_id":"source/img/brazil.png","path":"img/brazil.png","modified":0,"renderable":0},{"_id":"source/img/consumerism.gif","path":"img/consumerism.gif","modified":0,"renderable":0},{"_id":"source/img/licc.jpg","path":"img/licc.jpg","modified":0,"renderable":0},{"_id":"source/img/plants.jpg","path":"img/plants.jpg","modified":0,"renderable":0},{"_id":"source/img/science-dark.jpg","path":"img/science-dark.jpg","modified":0,"renderable":0},{"_id":"source/img/udacity-nb.jpg","path":"img/udacity-nb.jpg","modified":0,"renderable":0},{"_id":"source/img/storytelling.png","path":"img/storytelling.png","modified":0,"renderable":0},{"_id":"source/img/imperialism.jpg","path":"img/imperialism.jpg","modified":0,"renderable":0},{"_id":"source/img/money2.jpg","path":"img/money2.jpg","modified":0,"renderable":0},{"_id":"source/img/humanism.jpeg","path":"img/humanism.jpeg","modified":0,"renderable":0},{"_id":"source/img/too-big-to-fail.jpg","path":"img/too-big-to-fail.jpg","modified":0,"renderable":0},{"_id":"source/img/gossip.jpg","path":"img/gossip.jpg","modified":0,"renderable":0},{"_id":"source/img/profile.jpeg","path":"img/profile.jpeg","modified":0,"renderable":0},{"_id":"source/img/najib-tun-m.jpg","path":"img/najib-tun-m.jpg","modified":0,"renderable":0},{"_id":"source/img/sapiens.jpg","path":"img/sapiens.jpg","modified":0,"renderable":0},{"_id":"source/img/ai-electric.png","path":"img/ai-electric.png","modified":0,"renderable":0},{"_id":"source/img/taryn.png","path":"img/taryn.png","modified":0,"renderable":0},{"_id":"source/about/profile.jpeg","path":"about/profile.jpeg","modified":0,"renderable":0},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css.map","path":"css/aircloud.css.map","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.less","path":"css/aircloud.less","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/js/index.js","path":"js/index.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css","path":"css/aircloud.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-aircloud/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.css","path":"css/aircloud.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.css.map","path":"css/aircloud.css.map","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/aircloud.less","path":"css/aircloud.less","modified":0,"renderable":1},{"_id":"themes/aircloud/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/index.js","path":"js/index.js","modified":0,"renderable":1},{"_id":"themes/aircloud/source/js/gitment.js","path":"js/gitment.js","modified":0,"renderable":1},{"_id":"source/resume_new.pdf","path":"resume_new.pdf","modified":0,"renderable":0},{"_id":"source/img/extres.png","path":"img/extres.png","modified":0,"renderable":0},{"_id":"source/img/ashis.png","path":"img/ashis.png","modified":0,"renderable":0},{"_id":"source/img/virtuoso.png","path":"img/virtuoso.png","modified":0,"renderable":0},{"_id":"source/img/midivae.png","path":"img/midivae.png","modified":0,"renderable":0},{"_id":"source/img/musicvae.png","path":"img/musicvae.png","modified":0,"renderable":0},{"_id":"source/img/deep-analogy.png","path":"img/deep-analogy.png","modified":0,"renderable":0},{"_id":"source/img/ashis2.png","path":"img/ashis2.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1579847575470},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1579847575470},{"_id":"themes/landscape/README.md","hash":"37fae88639ef60d63bd0de22314d7cc4c5d94b07","modified":1579847575470},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1579847575470},{"_id":"themes/landscape/_config.yml","hash":"79ac6b9ed6a4de5a21ea53fc3f5a3de92e2475ff","modified":1579847575470},{"_id":"themes/landscape/package.json","hash":"544f21a0b2c7034998b36ae94dba6e3e0f39f228","modified":1579847575476},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1579847568417},{"_id":"themes/landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1579847575470},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1579847575470},{"_id":"themes/landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1579847575470},{"_id":"themes/landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1579847575471},{"_id":"themes/landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1579847575471},{"_id":"themes/landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1579847575471},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1579847575471},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1579847575471},{"_id":"themes/landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1579847575471},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1579847575471},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1579847575471},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1579847575472},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1579847575475},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1579847575475},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1579847575475},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1579847575475},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1579847575475},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1579847575476},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1579847575476},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"d0d753d39038284d52b10e5075979cc97db9cd20","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"950ddd91db8718153b329b96dc14439ab8463ba5","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"c4c835615d96a950d51fa2c3b5d64d0596534fed","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"93518893cf91287e797ebac543c560e2a63b8d0e","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/gauges-analytics.ejs","hash":"aad6312ac197d6c5aaf2104ac863d7eba46b772a","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/google-analytics.ejs","hash":"f921e7f9223d7c95165e0f835f353b2938e40c45","modified":1579847575472},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"5abf77aec957d9445fc71a8310252f0013c84578","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"7e749050be126eadbc42decfbea75124ae430413","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1579847575474},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1579847575474},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1579847575476},{"_id":"themes/landscape/source/css/_variables.styl","hash":"628e307579ea46b5928424313993f17b8d729e92","modified":1579847575479},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1579847575483},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1579847575484},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1579847575487},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1579847575487},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1579847575473},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1579847575474},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1579847575474},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"85ab11e082f4dd86dde72bed653d57ec5381f30c","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1579847575477},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1579847575478},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1579847575478},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1579847575480},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1579847575479},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1579847575482},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1579847575485},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1579847575486},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1579847575486},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1579847575482},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1579847575481},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1579847575483},{"_id":"themes/hexo-theme-aircloud/.gitignore","hash":"5a4a925cfd624633dafaacaced416c8d7272dcef","modified":1579853649534},{"_id":"themes/hexo-theme-aircloud/LICENSE","hash":"218b4bf797149a2751a015812a9adefe368185c1","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/_config.yml","hash":"0ad3a6ab2c9bb07fb1e030052622fdcde5c6f28a","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/readme-en.md","hash":"2903b1e9db12cd72ed6f8c10be14cd7f6afd82cf","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/readme.md","hash":"4be1fc64bd1dc335a986a39594564e89bd7eba43","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/languages/en.yml","hash":"93d77f44c0386df2defce0ac465b19e9a85f4d2f","modified":1579887859365},{"_id":"themes/hexo-theme-aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579853649535},{"_id":"themes/hexo-theme-aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/index.ejs","hash":"09e2407d615be7fe7ac41d11df3b7026e7393080","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/layout.ejs","hash":"7efd113aee90e698e187d0ea1f0b42a1c00d210e","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/post.ejs","hash":"2eb5fc0c2bb801528c3db3b09e6cb4d073e3ad99","modified":1579853649537},{"_id":"themes/hexo-theme-aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/footer.ejs","hash":"2ab1dc9da5183fc5e74a4bddbf0c29f992057ec9","modified":1579887884627},{"_id":"themes/hexo-theme-aircloud/layout/_partial/head.ejs","hash":"3f18d5d4951a205bab25b08d6bf85b054c84a21b","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/layout/_partial/nav.ejs","hash":"f7bb88510ece895a48490c63d33323dc4eff4136","modified":1579887766729},{"_id":"themes/hexo-theme-aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579853649536},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.css","hash":"64914aa6ecd5b948676870e0809e0f220b162e3b","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/common.less","hash":"8aef4d8cfdefbcd2e28d4985a4f79a5005ca0b6c","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579853649540},{"_id":"themes/hexo-theme-aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.css","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/hightlight.less","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579853649541},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.css","hash":"40d7cadf42b130ea1b40de1ae73b2b00e27f476f","modified":1579883019352},{"_id":"themes/hexo-theme-aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579853649542},{"_id":"themes/hexo-theme-aircloud/source/_less/layout.less","hash":"194ac7db2eeee7307fcb7470302f8172100181fb","modified":1579860381468},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.css","hash":"cfe668f5e11de4d20ec6538d480b74a86380de02","modified":1579887938712},{"_id":"themes/hexo-theme-aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888254394},{"_id":"themes/hexo-theme-aircloud/source/_less/post.css","hash":"4adf531589cb55413264c188b29ae47ab703beb8","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/post.less","hash":"bbbd81c03e7581950d82bf971eda49e8bed7bee1","modified":1579883019353},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579853649543},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579853649544},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css","hash":"e6082557a5f0e546169ab1aa0ba29bda4ef5c182","modified":1579883019355},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579853649545},{"_id":"themes/hexo-theme-aircloud/source/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1579853649546},{"_id":"themes/hexo-theme-aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579853649547},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579853649538},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579853649539},{"_id":"themes/hexo-theme-aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579853649547},{"_id":"source/_posts/test-post.md","hash":"623b1b89f04a61ba2905a155459a59ed1dddd8de","modified":1579859460776},{"_id":"source/test-page/index.md","hash":"a80f212d7be56d71b665227a1dcc10ea67d246ef","modified":1579856642376},{"_id":"source/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1579857100564},{"_id":"source/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1579857100564},{"_id":"source/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1579857100565},{"_id":"source/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1579857100565},{"_id":"source/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1579857100566},{"_id":"source/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1579857100566},{"_id":"source/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1579857100567},{"_id":"source/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1579857100567},{"_id":"source/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1579857100569},{"_id":"source/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1579857100569},{"_id":"source/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1579857100569},{"_id":"source/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1579857100569},{"_id":"source/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1579857100571},{"_id":"source/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1579857100572},{"_id":"source/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1579857100574},{"_id":"source/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1579857100577},{"_id":"source/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1579857100577},{"_id":"source/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1579857100577},{"_id":"source/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1579857100578},{"_id":"source/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1579857100582},{"_id":"source/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1579857100584},{"_id":"source/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1579857100562},{"_id":"source/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1579857100562},{"_id":"source/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1579857100563},{"_id":"source/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1579857100570},{"_id":"source/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1579857100572},{"_id":"source/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1579857100573},{"_id":"source/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1579857100575},{"_id":"source/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1579857100576},{"_id":"source/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1579857100582},{"_id":"source/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1579857100583},{"_id":"source/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1579857100584},{"_id":"source/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1579857100563},{"_id":"source/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1579857100563},{"_id":"source/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1579857100568},{"_id":"source/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1579857100572},{"_id":"source/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1579857100576},{"_id":"source/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1579857100583},{"_id":"source/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1579857100578},{"_id":"source/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1579857100568},{"_id":"source/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1579857100570},{"_id":"source/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1579857100567},{"_id":"source/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1579857100582},{"_id":"source/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1579857100566},{"_id":"source/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579857100574},{"_id":"source/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1579857100571},{"_id":"source/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1579857100575},{"_id":"source/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1579857100561},{"_id":"source/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1579857100581},{"_id":"source/about/index.md","hash":"a28211caa2760770a3231379ae858ab7d2cb1747","modified":1580035790739},{"_id":"source/tags/index.md","hash":"22dd3308e0a3db852e008fa8c8d526142e790dcb","modified":1579859110818},{"_id":"source/bloglist/index.md","hash":"205c3f925a02063ef3744e5839d13202d8854582","modified":1579866069472},{"_id":"source/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1579863788426},{"_id":"source/_posts/ai-music-direction.md","hash":"9912b3abcad5caf51eadf6e1378e9b217e1fafac","modified":1579867018254},{"_id":"source/_posts/nature-v2.md","hash":"66f2a37a1d885848cb3c97ef9d86529466e96b60","modified":1579866217542},{"_id":"source/_posts/sapiens-1.md","hash":"613604e163770883841e61a4a47fb3fed184d72c","modified":1579866217543},{"_id":"source/_posts/sapiens-2.md","hash":"ed008d144dcaa5a419b30df28d2bca8fd78741b5","modified":1579866217543},{"_id":"themes/hexo-theme-aircloud/source/css/fonts.css","hash":"c5e7b1d0ada40787eb87fdeef7e64d00588046c3","modified":1579867807637},{"_id":"themes/aircloud/_config.yml","hash":"fce8b918a9ee52e05bd95fdc83bd53e8fe8478ac","modified":1579888968113},{"_id":"themes/aircloud/test.md","hash":"501c404781cfbfe856c8d55dd5f1cd612dd83e41","modified":1579888897249},{"_id":"themes/aircloud/layout/catagory.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968117},{"_id":"themes/aircloud/layout/page.ejs","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968119},{"_id":"themes/aircloud/languages/en.yml","hash":"93d77f44c0386df2defce0ac465b19e9a85f4d2f","modified":1579888968114},{"_id":"themes/aircloud/languages/zh.yml","hash":"9ffaff1f5d240c94e44f9ef3b02bbae146af0dd4","modified":1579888968114},{"_id":"themes/aircloud/layout/404.ejs","hash":"8a30233a7b99831bd771121b5f450aaba412e8d5","modified":1579888968115},{"_id":"themes/aircloud/layout/about.ejs","hash":"cec034166ce08d2f8c961178e07b2f0ceac95cf2","modified":1579888968117},{"_id":"themes/aircloud/layout/archive.ejs","hash":"0f8a062f4f2f0648b23bd8c4a21945a6ca60dc1f","modified":1579888968116},{"_id":"themes/aircloud/layout/index.ejs","hash":"09e2407d615be7fe7ac41d11df3b7026e7393080","modified":1579888968115},{"_id":"themes/aircloud/layout/layout.ejs","hash":"7efd113aee90e698e187d0ea1f0b42a1c00d210e","modified":1579888968116},{"_id":"themes/aircloud/layout/post.ejs","hash":"2eb5fc0c2bb801528c3db3b09e6cb4d073e3ad99","modified":1579888968115},{"_id":"themes/aircloud/layout/tags.ejs","hash":"1a174d9213d25d9bf6ef28aabdaea6661cdd88c8","modified":1579888968116},{"_id":"themes/aircloud/source/_less/about.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/source/_less/about.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968123},{"_id":"themes/aircloud/source/_less/diff.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968124},{"_id":"themes/aircloud/source/_less/diff.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968130},{"_id":"themes/aircloud/source/_less/page.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968127},{"_id":"themes/aircloud/source/_less/page.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968122},{"_id":"themes/aircloud/source/_less/theme.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968121},{"_id":"themes/aircloud/source/_less/theme.less","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1579888968128},{"_id":"themes/aircloud/layout/_partial/donate.ejs","hash":"81c976a3b7fa5c47ef61181d537220eaf1d55eac","modified":1579888968119},{"_id":"themes/aircloud/layout/_partial/footer.ejs","hash":"2ab1dc9da5183fc5e74a4bddbf0c29f992057ec9","modified":1579888968119},{"_id":"themes/aircloud/layout/_partial/head.ejs","hash":"3f18d5d4951a205bab25b08d6bf85b054c84a21b","modified":1579888968118},{"_id":"themes/aircloud/layout/_partial/nav.ejs","hash":"f7bb88510ece895a48490c63d33323dc4eff4136","modified":1579888968118},{"_id":"themes/aircloud/layout/_partial/toc.ejs","hash":"41d11d159011466f0b6272aca9a74df8642b693f","modified":1579888968118},{"_id":"themes/aircloud/source/_less/archive.css","hash":"905efcc06a62d1e8b60df0e12434afa353378d3a","modified":1579888968125},{"_id":"themes/aircloud/source/_less/archive.less","hash":"5538d38614960e69b97a7f80f38b5933851212b8","modified":1579888968128},{"_id":"themes/aircloud/source/_less/common.css","hash":"64914aa6ecd5b948676870e0809e0f220b162e3b","modified":1579888968127},{"_id":"themes/aircloud/source/_less/common.less","hash":"8aef4d8cfdefbcd2e28d4985a4f79a5005ca0b6c","modified":1579888968133},{"_id":"themes/aircloud/source/_less/donate.css","hash":"ae6a676a42321512f0536c5230bb53084aaf2c2f","modified":1579888968129},{"_id":"themes/aircloud/source/_less/donate.less","hash":"d63139f4aa148bf894afa5c1007a4398696a0e4c","modified":1579888968129},{"_id":"themes/aircloud/source/_less/gitment.css","hash":"7d560b64e367129f98424052c660ae82b03a1d02","modified":1579888968131},{"_id":"themes/aircloud/source/_less/gitment.less","hash":"916deb8ecdee798d7a9b43b544e31dfd5bbd6de4","modified":1579888968121},{"_id":"themes/aircloud/source/_less/hightlight.css","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579888968122},{"_id":"themes/aircloud/source/_less/hightlight.less","hash":"4e5a9ec3e88fbc2ce0faabceff8d3f5099ea1012","modified":1579888968126},{"_id":"themes/aircloud/source/_less/index.css","hash":"52fe4d1b93dfb4c9c9d63e24862354b6a0ef47f8","modified":1579888968125},{"_id":"themes/aircloud/source/_less/index.less","hash":"502d689e3568056cc27dd4da7da2499b0be4253e","modified":1579888968126},{"_id":"themes/aircloud/source/_less/layout.less","hash":"194ac7db2eeee7307fcb7470302f8172100181fb","modified":1579888968120},{"_id":"themes/aircloud/source/_less/layout.css","hash":"40d7cadf42b130ea1b40de1ae73b2b00e27f476f","modified":1579888968130},{"_id":"themes/aircloud/source/_less/nav.css","hash":"32d0640c30a3c921e1f19f74cff2c5095f6ae02c","modified":1579889523601},{"_id":"themes/aircloud/source/_less/nav.less","hash":"3256b0e6566be7aa528a7c8ce2edbe4cfc09773b","modified":1579888968130},{"_id":"themes/aircloud/source/_less/post.css","hash":"4adf531589cb55413264c188b29ae47ab703beb8","modified":1579888968127},{"_id":"themes/aircloud/source/_less/post.less","hash":"bbbd81c03e7581950d82bf971eda49e8bed7bee1","modified":1579888968123},{"_id":"themes/aircloud/source/_less/tag.css","hash":"3250887aaae0bc62bd82082d000ce3de8cc55ab6","modified":1579888968121},{"_id":"themes/aircloud/source/_less/tag.less","hash":"47e1ce2f55e2b62beefd0f69dfe7deb594e7b309","modified":1579888968120},{"_id":"themes/aircloud/source/_less/toc.css","hash":"83b1a219e7fe66d9d6cc34600e5a16311381a883","modified":1579888968134},{"_id":"themes/aircloud/source/_less/toc.less","hash":"c873ce552b22b0aa2c51a386a91516cadf9160ba","modified":1579888968125},{"_id":"themes/aircloud/source/_less/variables.css","hash":"9768d38beea904c4febc704192a49c8f7ae6e06c","modified":1579888968122},{"_id":"themes/aircloud/source/_less/variables.less","hash":"49503f7a6c51edd6f1dbdea5345df6bb903b18a5","modified":1579888968124},{"_id":"themes/aircloud/source/css/aircloud.css","hash":"c38c125a9e466cad54b4f28d0fe02cbfa863978c","modified":1579889523606},{"_id":"themes/aircloud/source/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1579888968136},{"_id":"themes/aircloud/source/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1579888968135},{"_id":"themes/aircloud/source/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1579888968137},{"_id":"themes/aircloud/source/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1579888968137},{"_id":"themes/aircloud/source/_less/_partial/footer.css","hash":"e00d722211b4695449d72850340ac0dd701d6ede","modified":1579888968133},{"_id":"themes/aircloud/source/_less/_partial/footer.css.map","hash":"9e8d4df5d08425de5a8b247d0dd8b805c6edc661","modified":1579888968132},{"_id":"themes/aircloud/source/_less/_partial/footer.less","hash":"d1469f97daf750f3e4be18c4d640772780c32a75","modified":1579888968132},{"_id":"themes/aircloud/source/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1579888968138},{"_id":"public/about/index.html","hash":"d38edc94a415b4811e7d0c5e58fa3237df0a97b1","modified":1580035814789},{"_id":"public/tags/index.html","hash":"745433eb918784435d0d3e9217eb83b4aee9c723","modified":1580035814789},{"_id":"public/bloglist/index.html","hash":"65c163dc9bea83bd607bc46cf4a96e8e0f724be6","modified":1580035814789},{"_id":"public/2018/09/24/ai-music-direction/index.html","hash":"0761f3a45f99c181ed9f289fd7c195e3019d0e6a","modified":1580035814789},{"_id":"public/archives/index.html","hash":"ad4d572067a0feed96da62a7e8f2c9c4540b25ec","modified":1580035814789},{"_id":"public/archives/2018/index.html","hash":"ad4d572067a0feed96da62a7e8f2c9c4540b25ec","modified":1580035814789},{"_id":"public/archives/2018/09/index.html","hash":"ad4d572067a0feed96da62a7e8f2c9c4540b25ec","modified":1580035814789},{"_id":"public/tags/General-Thoughts/index.html","hash":"4c6a0ab569f2cfc094d4990b6e27645331f868f7","modified":1580035814789},{"_id":"public/tags/AI-Music/index.html","hash":"4c6a0ab569f2cfc094d4990b6e27645331f868f7","modified":1580035814789},{"_id":"public/index.html","hash":"4aa093e2e89066cc211f1a4f645b5eb6a33349bf","modified":1580035814789},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1580035814789},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1580035814789},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1580035814789},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1580035814789},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1580035814789},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1580035814789},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1580035814789},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1580035814789},{"_id":"public/img/fifa-deep-nn.png","hash":"a2547dedb6d07610a08663b8a346361a3965e6c7","modified":1580035814789},{"_id":"public/img/cook.jpg","hash":"8ae69438278d38836939ea3c30f3c2da9ff003fb","modified":1580035814789},{"_id":"public/img/dao.png","hash":"448cf98ec16f2dc584e7b7abf123853c97c8736f","modified":1580035814789},{"_id":"public/img/ge14.jpg","hash":"ff73f8d254d5c46a742f3c9d13abf0fe22334c97","modified":1580035814789},{"_id":"public/img/img_small_1.jpg","hash":"d76345b71d473a97cb44b6bc3be50619aaa268bf","modified":1580035814789},{"_id":"public/img/img_small_2.jpg","hash":"074f0ff7f1a90d2cddea4ed772592847526ae8cf","modified":1580035814789},{"_id":"public/img/german.jpg","hash":"f223a9576c54f922e6623e21c853c053836da7a7","modified":1580035814789},{"_id":"public/img/happiness.jpg","hash":"9d91f4fd99ea806ca8b3dbfa15e29987c5d6e2d7","modified":1580035814789},{"_id":"public/img/loc.png","hash":"909057e96bed8de9ebdb2c8b59c35126ff0920c3","modified":1580035814789},{"_id":"public/img/linear-regressor-knockout.PNG","hash":"6dfceba4c63d5472b5776efcb6d4a2e205f73b67","modified":1580035814789},{"_id":"public/img/logo.png","hash":"f0e68d08c28671bc770d2da84f9a8f684d493b73","modified":1580035814789},{"_id":"public/img/model_performance.png","hash":"cc6f6b9171182e1d9c8c446680492867fa03aa8a","modified":1580035814789},{"_id":"public/img/religion.png","hash":"e2bf4c75a1ea5d5bb9cddd2adefbb5f9d9aeece2","modified":1580035814789},{"_id":"public/img/music-supply.jpg","hash":"3bbaa3fd23a2140c3d83bac9ee5bd5cc97fea3d9","modified":1580035814789},{"_id":"public/img/nn-knockout.PNG","hash":"f14813c08f3ac408dc71b94e1f6b61a4e05d0547","modified":1580035814789},{"_id":"public/img/slide_1.jpg","hash":"2b44f0d05840b7cd0552d20a6e76bec70e358f27","modified":1580035814789},{"_id":"public/img/slide_2.jpg","hash":"050e3472c4350170f5c46839e729c520aaf7a52c","modified":1580035814789},{"_id":"public/img/slide_3.jpg","hash":"62eae4e6728f54e3cc6676ba2e9cef232e7458fa","modified":1580035814789},{"_id":"public/img/slide_4.jpg","hash":"b280bddf9d7110b6507675278d8b6b70ea03e156","modified":1580035814789},{"_id":"public/img/trust.jpg","hash":"d788000dffc9aa4196eee78202cae6b3e582f3b2","modified":1580035814789},{"_id":"public/img/work-for-you.jpg","hash":"16ca1d2882d9aef469d6892f04500631f5935eba","modified":1580035814789},{"_id":"public/css/aircloud.css.map","hash":"50db34961d11f6f461e23912609d25141068a6fc","modified":1580035814789},{"_id":"public/css/aircloud.less","hash":"45cab2da310dbfcba37ac3db657db77b4adac60d","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1580035814789},{"_id":"public/img/belgium.jpg","hash":"b7b42f6c202f1ef71ecfb60b651387e8307ee439","modified":1580035814789},{"_id":"public/img/ai-life.jpg","hash":"305613dbb39e9d0cb9ed2f31236f4dbda59b62ad","modified":1580035814789},{"_id":"public/img/blockchain.png","hash":"28985f9f8193de87488953617d90b9999244aa63","modified":1580035814789},{"_id":"public/img/passing-mark.jpg","hash":"7e6b697b2e03b26c6cc3625ccb6cb34b848607ee","modified":1580035814789},{"_id":"public/img/prof.png","hash":"a6146cd6b0c7fa451184ec1fb8e47759a15d0a06","modified":1580035814789},{"_id":"public/img/money.jpg","hash":"e585a97ee9dbd54e346ca8be0bff323893742073","modified":1580035814789},{"_id":"public/img/ubi.jpeg","hash":"2ad2490279a951c781cef7b29f371c383f53dab9","modified":1580035814789},{"_id":"public/img/results.png","hash":"81b89bd3a91d4087e06d9448dc0f1dd8ae3fe42f","modified":1580035814789},{"_id":"public/img/science-empire.jpg","hash":"7630846035a9d335e13f1854f2fdcb8717fcd13e","modified":1580035814789},{"_id":"public/img/unstoppable.jpg","hash":"12fd0fe9b1e164636e258c79207ccb7b117edd27","modified":1580035814789},{"_id":"public/img/win-ai.jpg","hash":"3a3ca6517faf8830f836a0415db539dea66ae2c4","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1580035814789},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1580035814789},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1580035814789},{"_id":"public/js/index.js","hash":"1fed4485eedf5309e504aec35596955e5d692c7d","modified":1580035814789},{"_id":"public/css/style.css","hash":"d4cfa90089c78a8b791252afae9fafa3b5658900","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1580035814789},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1580035814789},{"_id":"public/css/gitment.css","hash":"926b553be983d6dd90bcb60c5d6d4ee215d268a6","modified":1580035814789},{"_id":"public/css/aircloud.css","hash":"c38c125a9e466cad54b4f28d0fe02cbfa863978c","modified":1580035814789},{"_id":"public/js/gitment.js","hash":"89687f8fffe1125e08323fd6635ca4e53771c05e","modified":1580035814789},{"_id":"public/img/consumerism.gif","hash":"0fd78dd9bbc90222fcbab925fa30823d35058c0d","modified":1580035814789},{"_id":"public/img/licc.jpg","hash":"1e92ab52c2ea631adbb91932861f0a0c4e70b304","modified":1580035814789},{"_id":"public/img/plants.jpg","hash":"93e8efde27151edcbc11c6858b68d31a012371bc","modified":1580035814789},{"_id":"public/img/science-dark.jpg","hash":"e24199484e1cdab6a3271a82c67c25a9f259f85c","modified":1580035814789},{"_id":"public/img/udacity-nb.jpg","hash":"7fca363ac6ef9ee2eabf4646e85ebaf15849b2e6","modified":1580035814789},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1580035814789},{"_id":"public/img/storytelling.png","hash":"616dd665cc63e03a325f45f9a244a7f411fb27cc","modified":1580035814789},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1580035814789},{"_id":"public/img/humanism.jpeg","hash":"72bf7e3b1e21943b8fce93282806ac8a77724633","modified":1580035814789},{"_id":"public/img/imperialism.jpg","hash":"4c84841e70c706f1b656b2c0914f453dbd74c2e1","modified":1580035814789},{"_id":"public/img/money2.jpg","hash":"b43622be3af4ee928a1d2fdc10033f9c29780729","modified":1580035814789},{"_id":"public/img/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1580035814789},{"_id":"public/about/profile.jpeg","hash":"edb60bdebd1ccaa5576be719739282940ad5e92c","modified":1580035814789},{"_id":"public/img/brazil.png","hash":"f532df342f37fa6a671eb6ab62a695346bda33c5","modified":1580035814789},{"_id":"public/img/too-big-to-fail.jpg","hash":"36532067d1f1a3a295c141dfd54941c53acb4216","modified":1580035814789},{"_id":"public/img/gossip.jpg","hash":"c1d205fec01690386415aa57927d37ba6261bf1c","modified":1580035814789},{"_id":"public/img/najib-tun-m.jpg","hash":"0235462f0d09106c374c977f10d9ffc47b24aa77","modified":1580035814789},{"_id":"public/img/sapiens.jpg","hash":"ebd50a0af8626272d814c3dd7f6d0c0a0a15bd28","modified":1580035814789},{"_id":"public/img/ai-electric.png","hash":"35c42009e71f3b3ca9ef405001b23802071002ed","modified":1580035814789},{"_id":"public/img/taryn.png","hash":"0e26c54f980214b89c78b659d22bdbf7dd7647f1","modified":1580035814789},{"_id":"source/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1580035544388},{"_id":"public/resume_new.pdf","hash":"d7f5d4dc3c4962ce124a419366231da3066e183d","modified":1580035814789},{"_id":"source/.DS_Store","hash":"3a8cf5f8cb7876f58a7d4a1c3dcb1acd1eac8aec","modified":1585280729382},{"_id":"source/_posts/vae-symbolic-music.md","hash":"231c5796b7d33c1d08cc70c872c4bf309ffcc332","modified":1585283338580},{"_id":"source/img/extres.png","hash":"69d1d086fa0c0a8cc449a57ac523a6a46345da43","modified":1585282883093},{"_id":"source/img/ashis.png","hash":"e6ddb88bf9d29a480ded59086f70f6880c6fd3cf","modified":1585282883085},{"_id":"source/img/virtuoso.png","hash":"a00ef13f5cfa7775af0c131c62c9c447044ef4b0","modified":1585282883134},{"_id":"source/img/midivae.png","hash":"c060f51c9eb72566f9ec5adcb7581167c6429948","modified":1585282883099},{"_id":"source/img/musicvae.png","hash":"e2d3f7d11481c160bc640d506ab0d8c86ca585d9","modified":1585282883104},{"_id":"source/img/deep-analogy.png","hash":"cbe306dc2c8e08b1254ec41d80b2c277e5363e57","modified":1585282883092}],"Category":[],"Data":[],"Page":[{"layout":"about","title":"About","date":"2016-04-20T20:48:33.000Z","comments":1,"_content":"## About Me\n\nHi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.\n\nI am currently working as a research assistant under the [Audio, Music, Affective Computing and AI team](http://dorienherremans.com/team) (AMAAI) at Singapore University of Technology and Design. \n\nMy research interests include deep generative models, music generation and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.\n\nI earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, [Visenze](https://www.visenze.com/), and [DSAIR @ NTU](https://dsair.ntu.edu.sg/Pages/Home.aspx).\n\nMusic wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!\n\nMy resume can be found [here](../resume_new.pdf).","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About\"\ndate: 2016-04-21 04:48:33\ncomments: true\n---\n## About Me\n\nHi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.\n\nI am currently working as a research assistant under the [Audio, Music, Affective Computing and AI team](http://dorienherremans.com/team) (AMAAI) at Singapore University of Technology and Design. \n\nMy research interests include deep generative models, music generation and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.\n\nI earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, [Visenze](https://www.visenze.com/), and [DSAIR @ NTU](https://dsair.ntu.edu.sg/Pages/Home.aspx).\n\nMusic wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!\n\nMy resume can be found [here](../resume_new.pdf).","updated":"2020-01-26T10:49:50.739Z","path":"about/index.html","_id":"ck5ryykx7000011v5f6uj62kx","content":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>Hi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.</p>\n<p>I am currently working as a research assistant under the <a href=\"http://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Audio, Music, Affective Computing and AI team</a> (AMAAI) at Singapore University of Technology and Design. </p>\n<p>My research interests include deep generative models, music generation and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.</p>\n<p>I earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, <a href=\"https://www.visenze.com/\" target=\"_blank\" rel=\"noopener\">Visenze</a>, and <a href=\"https://dsair.ntu.edu.sg/Pages/Home.aspx\" target=\"_blank\" rel=\"noopener\">DSAIR @ NTU</a>.</p>\n<p>Music wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!</p>\n<p>My resume can be found <a href=\"../resume_new.pdf\">here</a>.</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"About-Me\"><a href=\"#About-Me\" class=\"headerlink\" title=\"About Me\"></a>About Me</h2><p>Hi there! My name is Hao Hao Tan (鄭豪好) from Kuala Lumpur, Malaysia.</p>\n<p>I am currently working as a research assistant under the <a href=\"http://dorienherremans.com/team\" target=\"_blank\" rel=\"noopener\">Audio, Music, Affective Computing and AI team</a> (AMAAI) at Singapore University of Technology and Design. </p>\n<p>My research interests include deep generative models, music generation and music information retrieval. I hope to study how machine learning techniques could be brought from research to production, and be applied in practical means within the music industry.</p>\n<p>I earned my bachelor’s degree in Computer Science major at Nanyang Technological Univesity, with previous internship experiences at PayPal, <a href=\"https://www.visenze.com/\" target=\"_blank\" rel=\"noopener\">Visenze</a>, and <a href=\"https://dsair.ntu.edu.sg/Pages/Home.aspx\" target=\"_blank\" rel=\"noopener\">DSAIR @ NTU</a>.</p>\n<p>Music wise, I do piano accompaniment, songwriting, music production, and session work as a keyboardist. I am super keen in collaborations on all kinds of music projects, do hit me up if you wanna have fun together!</p>\n<p>My resume can be found <a href=\"../resume_new.pdf\">here</a>.</p>\n"},{"layout":"tags","title":"Tags","_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\n---","date":"2020-01-24T09:45:10.818Z","updated":"2020-01-24T09:45:10.818Z","path":"tags/index.html","comments":1,"_id":"ck5rzbz8j00008dv5cddl8ema","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"Blog List","date":"2020-01-22T20:48:33.000Z","comments":1,"_content":"## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\n\n### Others\nJiang Yu's Blog: https://nguwijy.wordpress.com\nThomas Kipf: http://tkipf.github.io/\nChaitanya Joshi: https://chaitjo.github.io/\n","source":"bloglist/index.md","raw":"---\nlayout: \"about\"\ntitle: \"Blog List\"\ndate: 2020-01-23 04:48:33\ncomments: true\n---\n## Blog List\n\nA list of blogs that I personally follow a lot.\n\n### ML in Music\nKeunwoo Choi: https://keunwoochoi.wordpress.com/\nYixiao Zhang: https://ldzhangyx.github.io/\nMagenta: https://magenta.tensorflow.org/blog\nHao-Wen Dong: https://salu133445.github.io/\n\n### Others\nJiang Yu's Blog: https://nguwijy.wordpress.com\nThomas Kipf: http://tkipf.github.io/\nChaitanya Joshi: https://chaitjo.github.io/\n","updated":"2020-01-24T11:41:09.472Z","path":"bloglist/index.html","_id":"ck5s0x47j0000w5v57s73193s","content":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><p>Jiang Yu’s Blog: <a href=\"https://nguwijy.wordpress.com\" target=\"_blank\" rel=\"noopener\">https://nguwijy.wordpress.com</a><br>Thomas Kipf: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Blog-List\"><a href=\"#Blog-List\" class=\"headerlink\" title=\"Blog List\"></a>Blog List</h2><p>A list of blogs that I personally follow a lot.</p>\n<h3 id=\"ML-in-Music\"><a href=\"#ML-in-Music\" class=\"headerlink\" title=\"ML in Music\"></a>ML in Music</h3><p>Keunwoo Choi: <a href=\"https://keunwoochoi.wordpress.com/\" target=\"_blank\" rel=\"noopener\">https://keunwoochoi.wordpress.com/</a><br>Yixiao Zhang: <a href=\"https://ldzhangyx.github.io/\" target=\"_blank\" rel=\"noopener\">https://ldzhangyx.github.io/</a><br>Magenta: <a href=\"https://magenta.tensorflow.org/blog\" target=\"_blank\" rel=\"noopener\">https://magenta.tensorflow.org/blog</a><br>Hao-Wen Dong: <a href=\"https://salu133445.github.io/\" target=\"_blank\" rel=\"noopener\">https://salu133445.github.io/</a></p>\n<h3 id=\"Others\"><a href=\"#Others\" class=\"headerlink\" title=\"Others\"></a>Others</h3><p>Jiang Yu’s Blog: <a href=\"https://nguwijy.wordpress.com\" target=\"_blank\" rel=\"noopener\">https://nguwijy.wordpress.com</a><br>Thomas Kipf: <a href=\"http://tkipf.github.io/\" target=\"_blank\" rel=\"noopener\">http://tkipf.github.io/</a><br>Chaitanya Joshi: <a href=\"https://chaitjo.github.io/\" target=\"_blank\" rel=\"noopener\">https://chaitjo.github.io/</a></p>\n"}],"Post":[{"title":"Where Could AI Music Possibly Head Towards?","date":"2018-09-24T08:52:50.000Z","_content":"\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","source":"_posts/ai-music-direction.md","raw":"---\ntitle: Where Could AI Music Possibly Head Towards?\ndate: 2018-09-24 16:52:50\ntags:\n    - General Thoughts\n    - AI Music\n---\n\nThere has been quite a hype in AI music generation recently. We have [Flow Machines](http://www.flow-machines.com/) creating their famous song [Daddy's Car](https://www.youtube.com/watch?v=LSHZ_b05W7o). We have startups like [Amper](http://www.ampermusic.com), [Aiva](http://soundcloud.com/user-95265362) and [Jukedeck](http://www.jukedeck.com). We have tech giants like [Google Magenta](http://magenta.tensorflow.org) and [IBM Watson](http://www.ibm.com/watson/music). We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.\n\nAnd some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.\n\nMy point stands firmly: AI music is **NEVER** meant to replace human musicians. \n\nIn fact, here are a few manifestation of its usage --\n\n<br/>\n\n## **1 - Satisfy massive music supply**\n\nIn places which needs ***massive music supply***, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.\n\nBut don't we ever think that the jazz musician will lose his job and be replaced -- in fact, I believe that ***human music will be elevated and be seen as a more precious type of art*** as AI music comes in. \n\nIt is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.\n\nThis, brings to my second point.\n\n<br/>\n\n## **2 - Setting the baseline**\n\nAt the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.\n\nAnd **that is the baseline of music**. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, \"Daddy's Car\" is a baseline, and melodies generated by Jukedeck shows us the passing mark. \n\nAnd if we take a step further, the effort to refine AIM is equal to **raising the baseline of music-making**. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. \n\nListening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.\n\n<br/>\n\n## **3 - As a tool of inspiration**\n\nAI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.\n\nEven Jazz was borned in a situation where some strangers in a room each play random melodies to try to \"reply\" to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.\n\n<br/>\n\n## **The ever-winning ground in front of AI**\n\nWe may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.\n\nBut I believe humans still have one thing that could always outperform AI -- which is the artistic sense within us, the ability within us to appreciate and interpret art. \n\nThere is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it \"just is\" different, and it can't be explained or understood -- even by human ourselves.\n\nBut ironically, ***everything understandable and explainable for human also gives AI the chance to understand and advance in it*** -- even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. \n\nWhich means it may precisely be this **\"un-understandab-ility in art\"** of us, that distinct us from AI.\n\nAI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.\n\nWhich is why I believe in today's world, art and humanities is something that should be given even more focus by every single individual, to make us **\"stay human\"** and **\"stay unbeatable\"**.\n\nThe world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.","slug":"ai-music-direction","published":1,"updated":"2020-01-24T12:15:51.328Z","_id":"ck5s3jzfj00004qv5g9865rpz","comments":1,"layout":"post","photos":[],"link":"","content":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>There has been quite a hype in AI music generation recently. We have <a href=\"http://www.flow-machines.com/\" target=\"_blank\" rel=\"noopener\">Flow Machines</a> creating their famous song <a href=\"https://www.youtube.com/watch?v=LSHZ_b05W7o\" target=\"_blank\" rel=\"noopener\">Daddy’s Car</a>. We have startups like <a href=\"http://www.ampermusic.com\" target=\"_blank\" rel=\"noopener\">Amper</a>, <a href=\"http://soundcloud.com/user-95265362\" target=\"_blank\" rel=\"noopener\">Aiva</a> and <a href=\"http://www.jukedeck.com\" target=\"_blank\" rel=\"noopener\">Jukedeck</a>. We have tech giants like <a href=\"http://magenta.tensorflow.org\" target=\"_blank\" rel=\"noopener\">Google Magenta</a> and <a href=\"http://www.ibm.com/watson/music\" target=\"_blank\" rel=\"noopener\">IBM Watson</a>. We have Taryn Southern (in the picture), composing music using AI. The community is slowly growing and gaining attention from the public.</p>\n<p>And some argue that AI music (in acronym, AIM) is a threat to human, as it invades even the artistic sense of human.</p>\n<p>My point stands firmly: AI music is <strong>NEVER</strong> meant to replace human musicians. </p>\n<p>In fact, here are a few manifestation of its usage –</p>\n<br/>\n\n<h2 id=\"1-Satisfy-massive-music-supply\"><a href=\"#1-Satisfy-massive-music-supply\" class=\"headerlink\" title=\"1 - Satisfy massive music supply\"></a><strong>1 - Satisfy massive music supply</strong></h2><p>In places which needs <strong><em>massive music supply</em></strong>, eg. jazz bars, restaurants, BGM for games and short videos, AI could satisfy this massive demand. After all, a jazz musician still needs a rest after 2 hours of improvisation, but AI doesnt need that.</p>\n<p>But don’t we ever think that the jazz musician will lose his job and be replaced – in fact, I believe that <strong><em>human music will be elevated and be seen as a more precious type of art</em></strong> as AI music comes in. </p>\n<p>It is like economy class and premier class - in the world where we can always listen to AIM everywhere,it should be a more elevating experience when we have a chance to listen to a human musician playing in front of us.</p>\n<p>This, brings to my second point.</p>\n<br/>\n\n<h2 id=\"2-Setting-the-baseline\"><a href=\"#2-Setting-the-baseline\" class=\"headerlink\" title=\"2 - Setting the baseline\"></a><strong>2 - Setting the baseline</strong></h2><p>At the stage when AI music is able to satisfy massive music supply, it does convey that AI music has achieved a certain standard. At this point, AIM must have reached a level which the music generated is no longer some geeky passages with malformed chord progressions and awkward tempo, but the music generated is able to serve its purpose as music.</p>\n<p>And <strong>that is the baseline of music</strong>. If a soul-less machine can produce that, human composers must ensure that they provide something of even higher quality. So yes, “Daddy’s Car” is a baseline, and melodies generated by Jukedeck shows us the passing mark. </p>\n<p>And if we take a step further, the effort to refine AIM is equal to <strong>raising the baseline of music-making</strong>. One step closer AIM approaches us, we should take two steps further to prove that we are better. I personally think that it forms some kind of drive to push the music industry forward. </p>\n<p>Listening to human composed music should be, and must be, a more elevating experience. With AIM setting the baseline, human musicians should try harder to live up to that.</p>\n<br/>\n\n<h2 id=\"3-As-a-tool-of-inspiration\"><a href=\"#3-As-a-tool-of-inspiration\" class=\"headerlink\" title=\"3 - As a tool of inspiration\"></a><strong>3 - As a tool of inspiration</strong></h2><p>AI music could inspire thoughts for human musician, showing collaboration for AI and human in music creation. Composers may just need a motive, a short passage, or even some random notes to start with to compose a new song.</p>\n<p>Even Jazz was borned in a situation where some strangers in a room each play random melodies to try to “reply” to each other (quoted from the movie La La Land). Who knows that the notes generated by AI could inspire one to compose some totally unexpected styles, genres, or even new music vocabulary, as new music are often being produced under randomness and pure chance.</p>\n<br/>\n\n<h2 id=\"The-ever-winning-ground-in-front-of-AI\"><a href=\"#The-ever-winning-ground-in-front-of-AI\" class=\"headerlink\" title=\"The ever-winning ground in front of AI\"></a><strong>The ever-winning ground in front of AI</strong></h2><p>We may have lost to AI in chess, Go, memory, computation, and many others. And we fear that one day, we may lose even more.</p>\n<p>But I believe humans still have one thing that could always outperform AI – which is the artistic sense within us, the ability within us to appreciate and interpret art. </p>\n<p>There is still a difference between a piece played by even the finest AI tuned piano and Martha Argerich - it “just is” different, and it can’t be explained or understood – even by human ourselves.</p>\n<p>But ironically, <strong><em>everything understandable and explainable for human also gives AI the chance to understand and advance in it</em></strong> – even things as complex as debating, involving not just language itself but also logic structures, can be understood by AI. </p>\n<p>Which means it may precisely be this <strong>“un-understandab-ility in art”</strong> of us, that distinct us from AI.</p>\n<p>AI may mimic the logical process of a debater and construct flawless arguments - but it will never be able to mimic the interpretation of public speaking, the art of persuading one to believe, and the creativity in constructing belief-shattering arguments and viewpoints.</p>\n<p>Which is why I believe in today’s world, art and humanities is something that should be given even more focus by every single individual, to make us <strong>“stay human”</strong> and <strong>“stay unbeatable”</strong>.</p>\n<p>The world is not just made up of weights and biases, there must be something more. We as humans in this century, who had already been half-slaves to technology, are obliged to try even harder to find out that particular element which makes us who we are.</p>\n"},{"title":"VAE In Symbolic Music Modelling","date":"2020-01-26T09:54:50.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","source":"_posts/vae-symbolic-music.md","raw":"---\ntitle: VAE In Symbolic Music Modelling\ndate: 2020-01-26 17:54:50\ntags:\n    - VAE\n    - Symbolic Music\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nTLDR: This blog will discuss:\n1 - A very simple VAE introduction\n2 - Several papers that use VAE architecture for various symbolic music modelling tasks\n3 - General thoughts on several aspects of VAE in symbolic music modelling\n\n<br/>\n\n## 1 - VAE\n\nWe know about the VAE's ELBO function as below (refer [here](https://ermongroup.github.io/cs228-notes/inference/variational/) for ELBO derivation):\n$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$\n\nThe first term represents **reconstruction accuracy**, as the expectation of reconstructing \\\\(X\\\\) given \\\\(Z\\\\) needs to be maximized. Latent code \\\\(z\\\\) is sampled from a learnt posterior \\\\(q(Z|X)\\\\).\n\nThe second term represents **KL divergence** -- how deviated is the learnt posterior \\\\(q(Z|X)\\\\) from the prior \\\\(p(Z)\\\\). According to [BetaVAE paper](https://openreview.net/references/pdf?id=Sy2fzU9gl), the \\\\(\\beta\\\\) term weights the influence of KL divergence in the ELBO function.\n\nThe prior distribution \\\\(p(Z)\\\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\\\(\\mathcal{N}(0, \\mathcal{I})\\\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) -- $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.\n\nThe posterior distribution \\\\(q(Z|X)\\\\), in simple terms, is the \"improvement\" that you make on your assumed distribution of \\\\(Z\\\\), after inspecting data samples \\\\(X\\\\). Since the true posterior \\\\(p(Z|X)\\\\) is intractable, hence we use variational inference to get an approximation \\\\(q(Z|X)\\\\), and made it learnt by a neural network.\n\nThe ultimate intuition of the VAE framework is to encode the huge **data space** into a compact **latent space**, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we **utilize the latent space** learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?\n\n<br/>\n\n## 2 - Application\n\n**Symbolic music domain** refers to the usage of **high-level symbols** such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.\n\nTo study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling --\n\n### 1 - [**MusicVAE**](https://arxiv.org/pdf/1803.05428.pdf)\n\n![](/img/musicvae.png)\n\n**Published at:** ICML 2018\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from [Bowman et al.](https://arxiv.org/abs/1511.06349) The key contributions include: \n- it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.\n-  the \"conductor\" layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.\n\nAn extension of this work on multi-track music is available [here](https://arxiv.org/pdf/1806.00195.pdf).\n\n### 2 - [**MIDI-VAE**](https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf)\n\n![](/img/midivae.png)\n\n**Published at:** ISMIR 2018\n**Dataset type:** Multi-track, polyphonic music across jazz, classical, pop\n**Representation used:** Piano roll for each track. Note: for each timestep, instead of modelling 1 *n*-hot vector, *n* 1-hot vectors are modelled (final layer as softmax)\n**Novelty:** One of the very first music style transfer papers in the symbolic domain.\n- The idea is to disentangle a portion out of the latent vector to be responsible for **style classification**, while the remaining should encode the characteristics of the data sample. During generation, \\\\(z_{S_{1}}\\\\) will be swapped to \\\\(z_{S_{2}}\\\\), and decoded with the remaining part of the latent vector.\n- They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.\n\nHow could we get both \\\\(z_{S_{1}}\\\\) and \\\\(z_{S_{2}}\\\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\\\(S_{1}\\\\) and \\\\(S_{2}\\\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.\n\nHowever in this framework, \\\\(z_{S}\\\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information -- sample-related information could also exist in \\\\(z_{S}\\\\). Ensuring **identity transformation** after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using *adversarial training* to ensure sample invariance, such as in [Fader Networks paper](https://arxiv.org/pdf/1706.00409.pdf) or in this [timbre disentanglement paper](https://www.ijcai.org/Proceedings/2019/0652.pdf) should be easily extended from here.\n\n### 3 - [**VirtuosoNet**](http://archives.ismir.net/ismir2019/paper/000112.pdf)\n\n![](/img/virtuoso.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Classical piano music\n**Representation used:** Score and performance features (refer to [this paper](http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf))\n**Novelty:** This paper focuses on expressive piano performance modelling. The key contributions are:\n- As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the *score representation* learnt by a separate score encoder.\n- The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of **hierachical attention**, such that information is being attended on different levels: note, beat and measure during encoding\n- During generation, it either randomly samples the style vector \\\\(z\\\\) from a normal distribution prior, or uses a pre-encoded \\\\(z\\\\) from other performances to decode performance features.\n- An extension of this work, [GNN for Piano Performance Modelling](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf), incorporates the idea of using graphs to model performance events.\n\n### 4 - [**Latent Space Regularization for Explicit Control of Musical Attributes**](https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf)\n\n![](/img/ashis.png)\n\n**Published at:** ML4MD @ ICML 2019\n**Dataset type:** Single track, monophonic music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).\n\nThe author suggests to add a regularization loss term during training, in the form of \n$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$\n\nwhere \\\\(\\mathcal{D}\\\\) represents **distance matrix**, which is a 2-dimensional square matrix of shape \\\\((|S|, |S|)\\\\), containing the distances (taken pairwise) between the elements of a set \\\\(S\\\\). \n\n\\\\(\\mathcal{D}\\\\) is the distance matrix of the \\\\(r^{th}\\\\) dimension value of encoded \\\\(z\\\\) for each sample, while \\\\(\\mathcal{D}_{a}\\\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\\\(r^{th}\\\\) dimension of \\\\(z\\\\), such that \\\\(z^i_r < z^j_r \\Longleftrightarrow a^i < a^j\\\\).\n\nThe interesting ideas that I find in this work is that the regularization loss captures **relative distance** instead of absolute distance, i.e. using \\\\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\\\), or even more directly, using \\\\(MSE(z_r, a)\\\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\\\(z_r\\\\) is learnt to get closer to \\\\(a\\\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\\\(z_r\\\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\\\(z_r\\\\) and \\\\(a\\\\).\n\nFigure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\\\(z_r\\\\) values, and the right side colored using actual \\\\(a\\\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\\\(z_r\\\\) and \\\\(a\\\\) on individual samples are not necessarily accurate.\n\n![](/img/ashis2.png)\n\n### 5 - [**Deep Music Analogy via Latent Representation Disentanglement**](http://archives.ismir.net/ismir2019/paper/000072.pdf)\n\n![](/img/deep-analogy.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** \"Deep music analogy\" shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.\n\n- The proposed EC<sup>2</sup>-VAE architecture splits latent \\\\(z\\\\) into 2 parts -- \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), where \\\\(z_{r}\\\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\\\(z_{p}\\\\) and \\\\(z_{r}\\\\), together with the chord condition, is used to decode into the original sample.\n- Another point of view is to see it as a type of latent regularization -- part of the latent code is \"regularized\" to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.\n- Objective evaluation is of 2-fold:\n    - After pitch transposition, \\\\(\\Delta z_{r}\\\\) should not be changed much and instead \\\\(\\Delta z_{p}\\\\) should be changing. This is by measuring the L1-norm of change in \\\\(z\\\\).\n    - Modifying evaluation methods from [FactorVAE](https://arxiv.org/pdf/1802.05983.pdf), this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).\n\n### 6 - [**Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge**](http://archives.ismir.net/ismir2019/paper/000100.pdf)\n\n![](/img/extres.png)\n\n**Published at:** ISMIR 2019\n**Dataset type:** Single track, monophonic piano music\n**Representation used:** Piano roll (final layer as softmax)\n**Novelty:** This work proposes a model known as ExtRes, which stands for **extraction** model and **residual** model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.\n\nFrom the graphical model, we can see that:\n- During inference, latent code \\\\(z_e\\\\) is learnt to model user-defined attributes \\\\(y\\\\) via a probabilistic encoder with posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and parameters \\\\(\\phi_{e}\\\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\\\(z_r\\\\) is learnt to model input sample \\\\(x\\\\) via another probabilistic encoder with posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and parameters \\\\(\\phi_{r}\\\\), taking in \\\\(y\\\\) as an additional condition during encoding.\n- During generation, latent code \\\\(z_e\\\\) and \\\\(z_r\\\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\\\(\\theta_y\\\\) is trained to decode \\\\(z_e\\\\) into \\\\(y\\\\), and a separate decoder with parameters \\\\(\\theta_x\\\\) is trained to decode \\\\(z_r\\\\) into \\\\(x\\\\), with an additional condition of \\\\(y\\\\).\n\nThe final loss function is hence consists of 4 terms:\n- the reconstruction loss of the input sample \\\\(x\\\\);\n- the reconstruction loss of the attribute sequence \\\\(y\\\\);\n- the KL divergence between posterior \\\\(q_{\\phi_{e}}(z_e|y)\\\\) and prior \\\\(p(z_e)\\\\) for extraction model;\n- the KL divergence between posterior \\\\(q_{\\phi_{r}}(z_r|x, y)\\\\) and prior \\\\(p(z_r)\\\\) for residual model.\n\nHere, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\\\(y\\\\) should \\\\(y\\\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).\n\n<br/>\n\nOther relevant papers that we would like to list here include:\n7 - [A Classifying Variational Autoencoder with Application to Polyphonic Music Generation](https://arxiv.org/pdf/1711.07050.pdf)\n8 - [MahlerNet: Unbounded Orchestral Music with Neural Networks](http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769)\n9 - [Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models](https://arxiv.org/pdf/1711.05772.pdf)\n10 - [GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures](https://arxiv.org/pdf/1707.04588.pdf)\n\n<br/>\n\n## 3 - Thoughts and Discussion\n\nI hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.\n\n### 1 - Common usage of the latent code\n\nWe could observe that a whole lot of applications of VAE are focusing on **music attribute / feature modelling**. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.\n\nHere, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:\n\n(i) **disentanglement**: how are the attributes being *disentangled* from each other, so as to ensure that each latent space governs one and only desired factor;\n(ii) **regularization**: how is the latent space being *regularized* to exhibit a certain desired factor -- either by adding in a classifier, or using some self-defined regularization loss.\n(iii) **identity preservation**: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the *reconstruction quality*, and the *disentanglement quality* of the model. For ensuring disentanglement quality, a common strategy is to use **adversarial training**, such that to ensure the latent space be invariant on the non-governing factors.\n\n### 2 - On \\\\(\\beta\\\\) value\n\nIt is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\\\(\\beta\\\\) value. Among the first 5 papers discussed above, each of them uses \\\\(\\beta\\\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\\\(\\beta\\\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.\n\nIt seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\\\(\\beta\\\\) value. Additionally, the [MIDI-VAE](https://arxiv.org/abs/1809.07600) paper has further showed that the reconstruction accuracy are very much poorer given higher \\\\(\\beta\\\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by [\\\\(\\beta\\\\)-VAE](https://openreview.net/forum?id=Sy2fzU9gl), how could we find a balanced sweet spot for good disentanglement provided with such low range of \\\\(\\beta\\\\) values remain an interesting challenge.\n\n### 3 - On music representation used\n\nCommon music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to [this survey paper](https://arxiv.org/abs/1709.01620)). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even [Transformers](https://arxiv.org/pdf/1912.05537.pdf). Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.\n\nRecently, [Jeong et al.](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.\n\n### 4 - On the measure of \"controllability\"\n\nHow could we evaluate if a model has a \"higher controllability\", on a given factor, during generation? The most related one might be by [Pati et al.](https://github.com/ashispati/AttributeModelling), whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.\n\n### 5 - Can VAE be an end-to-end architecture for music generation?\n\nFrom most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike **language modelling** approaches such as [Music Transformer](https://arxiv.org/pdf/1809.04281.pdf), [MuseNet](https://openai.com/blog/musenet/), and [Pop Music Transformer](https://arxiv.org/pdf/2002.00212.pdf) that can generate minute-long decent music pieces with observable long term structure.\n\nLatent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.","slug":"vae-symbolic-music","published":1,"updated":"2020-03-27T04:28:58.580Z","_id":"ck89oi0it0000tbm8hg9jhjt3","comments":1,"layout":"post","photos":[],"link":"","content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n","site":{"data":{}},"excerpt":"","more":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n<p>TLDR: This blog will discuss:<br>1 - A very simple VAE introduction<br>2 - Several papers that use VAE architecture for various symbolic music modelling tasks<br>3 - General thoughts on several aspects of VAE in symbolic music modelling</p>\n<br/>\n\n<h2 id=\"1-VAE\"><a href=\"#1-VAE\" class=\"headerlink\" title=\"1 - VAE\"></a>1 - VAE</h2><p>We know about the VAE’s ELBO function as below (refer <a href=\"https://ermongroup.github.io/cs228-notes/inference/variational/\" target=\"_blank\" rel=\"noopener\">here</a> for ELBO derivation):<br>$$E_{z\\sim q(Z|X)}[\\log p(X|Z)] - \\beta \\cdot \\mathcal{D}_{KL}(q(Z|X) || p(Z))$$</p>\n<p>The first term represents <strong>reconstruction accuracy</strong>, as the expectation of reconstructing \\(X\\) given \\(Z\\) needs to be maximized. Latent code \\(z\\) is sampled from a learnt posterior \\(q(Z|X)\\).</p>\n<p>The second term represents <strong>KL divergence</strong> – how deviated is the learnt posterior \\(q(Z|X)\\) from the prior \\(p(Z)\\). According to <a href=\"https://openreview.net/references/pdf?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">BetaVAE paper</a>, the \\(\\beta\\) term weights the influence of KL divergence in the ELBO function.</p>\n<p>The prior distribution \\(p(Z)\\), in simple terms, is the assumption of how your data points are distributed. A common choice of prior distribution is the standard Gaussian \\(\\mathcal{N}(0, \\mathcal{I})\\). However, many start to think that a more natural choice of distribution should be a Gaussian Mixture Model (GMM) – $$\\sum_{i=1}^{K} \\phi_{i} \\cdot \\mathcal{N}(\\mu_{i}, \\Sigma_{i})$$ as the distribution of the data points could be mixtures of Gaussian components, rather than just one single standard Gaussian.</p>\n<p>The posterior distribution \\(q(Z|X)\\), in simple terms, is the “improvement” that you make on your assumed distribution of \\(Z\\), after inspecting data samples \\(X\\). Since the true posterior \\(p(Z|X)\\) is intractable, hence we use variational inference to get an approximation \\(q(Z|X)\\), and made it learnt by a neural network.</p>\n<p>The ultimate intuition of the VAE framework is to encode the huge <strong>data space</strong> into a compact <strong>latent space</strong>, where meaningful attributes can be extracted and controlled relatively easier in lower dimension. Hence, the objective would be: how can we <strong>utilize the latent space</strong> learnt for a multitude of music application tasks, including generation, interpolation, disentanglement, style transfer, etc.?</p>\n<br/>\n\n<h2 id=\"2-Application\"><a href=\"#2-Application\" class=\"headerlink\" title=\"2 - Application\"></a>2 - Application</h2><p><strong>Symbolic music domain</strong> refers to the usage of <strong>high-level symbols</strong> such as event tokens, text, or piano roll matrices as representation during music modelling. Audio-based music modelling is not covered in this scope. The reason of using symbolic music representation for modelling is that it incorporates higher level features such as structure, harmony, rhythm etc. directly within the representation itself, without the need of further preprocessing.</p>\n<p>To study the objective above, below we list and discuss several papers that apply VAE framework on symbolic music modelling –</p>\n<h3 id=\"1-MusicVAE\"><a href=\"#1-MusicVAE\" class=\"headerlink\" title=\"1 - MusicVAE\"></a>1 - <a href=\"https://arxiv.org/pdf/1803.05428.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MusicVAE</strong></a></h3><p><img src=\"/img/musicvae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ICML 2018<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This should be one of the very first widely known papers that used VAE on music modelling, bringing in the idea from <a href=\"https://arxiv.org/abs/1511.06349\" target=\"_blank\" rel=\"noopener\">Bowman et al.</a> The key contributions include: </p>\n<ul>\n<li>it clearly demonstrates the power of condensing useful musical information in the latent space. Variations in generated samples are more evident in latent space traversal, instead of data space.</li>\n<li>the “conductor” layer responsible for measure-level embeddings helps in preserving long term structure and reconstruction accuracy in longer sequences.</li>\n</ul>\n<p>An extension of this work on multi-track music is available <a href=\"https://arxiv.org/pdf/1806.00195.pdf\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<h3 id=\"2-MIDI-VAE\"><a href=\"#2-MIDI-VAE\" class=\"headerlink\" title=\"2 - MIDI-VAE\"></a>2 - <a href=\"https://tik-old.ee.ethz.ch/file//b17f34f911d0ecdb66bfc41af9cdf200/MIDIVAE_ISMIR_CR.pdf\" target=\"_blank\" rel=\"noopener\"><strong>MIDI-VAE</strong></a></h3><p><img src=\"/img/midivae.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2018<br><strong>Dataset type:</strong> Multi-track, polyphonic music across jazz, classical, pop<br><strong>Representation used:</strong> Piano roll for each track. Note: for each timestep, instead of modelling 1 <em>n</em>-hot vector, <em>n</em> 1-hot vectors are modelled (final layer as softmax)<br><strong>Novelty:</strong> One of the very first music style transfer papers in the symbolic domain.</p>\n<ul>\n<li>The idea is to disentangle a portion out of the latent vector to be responsible for <strong>style classification</strong>, while the remaining should encode the characteristics of the data sample. During generation, \\(z_{S_{1}}\\) will be swapped to \\(z_{S_{2}}\\), and decoded with the remaining part of the latent vector.</li>\n<li>They also proposed a novel method to represent multi-track polyphonic music by training 3 GRUs, each responsible for pitch, instrument and velocity, used in both encoder and decoder part.</li>\n</ul>\n<p>How could we get both \\(z_{S_{1}}\\) and \\(z_{S_{2}}\\) for style-swap is not detailed in the paper. We assume that we need pairing data samples of style \\(S_{1}\\) and \\(S_{2}\\) each, encode them into latent vectors, cross-swap the style latent part and the residual latent part, and then decode.</p>\n<p>However in this framework, \\(z_{S}\\) is constrained to encode style-related information, but not necessarily to exclude sample-related information – sample-related information could also exist in \\(z_{S}\\). Ensuring <strong>identity transformation</strong> after cross-swapping style and sample latent codes may be a challenge in this framework, however ideas of using <em>adversarial training</em> to ensure sample invariance, such as in <a href=\"https://arxiv.org/pdf/1706.00409.pdf\" target=\"_blank\" rel=\"noopener\">Fader Networks paper</a> or in this <a href=\"https://www.ijcai.org/Proceedings/2019/0652.pdf\" target=\"_blank\" rel=\"noopener\">timbre disentanglement paper</a> should be easily extended from here.</p>\n<h3 id=\"3-VirtuosoNet\"><a href=\"#3-VirtuosoNet\" class=\"headerlink\" title=\"3 - VirtuosoNet\"></a>3 - <a href=\"http://archives.ismir.net/ismir2019/paper/000112.pdf\" target=\"_blank\" rel=\"noopener\"><strong>VirtuosoNet</strong></a></h3><p><img src=\"/img/virtuoso.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Classical piano music<br><strong>Representation used:</strong> Score and performance features (refer to <a href=\"http://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf\" target=\"_blank\" rel=\"noopener\">this paper</a>)<br><strong>Novelty:</strong> This paper focuses on expressive piano performance modelling. The key contributions are:</p>\n<ul>\n<li>As they argue that music scores can be interpreted and performed in various styles, this work uses a conditional VAE (CVAE) architecture for the performance encoder and decoder. The additional condition fed in is the <em>score representation</em> learnt by a separate score encoder.</li>\n<li>The score encoder consists of 3 levels, each encoding note, beat and measure information respectively. This work also uses the idea of <strong>hierachical attention</strong>, such that information is being attended on different levels: note, beat and measure during encoding</li>\n<li>During generation, it either randomly samples the style vector \\(z\\) from a normal distribution prior, or uses a pre-encoded \\(z\\) from other performances to decode performance features.</li>\n<li>An extension of this work, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">GNN for Piano Performance Modelling</a>, incorporates the idea of using graphs to model performance events.</li>\n</ul>\n<h3 id=\"4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\"><a href=\"#4-Latent-Space-Regularization-for-Explicit-Control-of-Musical-Attributes\" class=\"headerlink\" title=\"4 - Latent Space Regularization for Explicit Control of Musical Attributes\"></a>4 - <a href=\"https://musicinformatics.gatech.edu/wp-content_nondefault/uploads/2019/06/Pati-and-Lerch-Latent-Space-Regularization-for-Explicit-Control-o.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Latent Space Regularization for Explicit Control of Musical Attributes</strong></a></h3><p><img src=\"/img/ashis.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ML4MD @ ICML 2019<br><strong>Dataset type:</strong> Single track, monophonic music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This two-page extended abstract tackles the problem of controllable music generation over desired musical attributes. The simple yet powerful idea is that we can regularize some dimensions within the encoded latent vector to reflect the changes in our desired musical attributes (such as rhythm density, pitch range, etc.).</p>\n<p>The author suggests to add a regularization loss term during training, in the form of<br>$$ MSE(tanh(\\mathcal{D}_{z_r}), sign(\\mathcal{D}_a))$$</p>\n<p>where \\(\\mathcal{D}\\) represents <strong>distance matrix</strong>, which is a 2-dimensional square matrix of shape \\((|S|, |S|)\\), containing the distances (taken pairwise) between the elements of a set \\(S\\). </p>\n<p>\\(\\mathcal{D}\\) is the distance matrix of the \\(r^{th}\\) dimension value of encoded \\(z\\) for each sample, while \\(\\mathcal{D}_{a}\\) is the distance matrix of musical attributes for each sample. The idea is to incorporate the relative distance of musical attributes within a training batch by regularizing the \\(r^{th}\\) dimension of \\(z\\), such that \\(z^i_r &lt; z^j_r \\Longleftrightarrow a^i &lt; a^j\\).</p>\n<p>The interesting ideas that I find in this work is that the regularization loss captures <strong>relative distance</strong> instead of absolute distance, i.e. using \\(MSE(\\mathcal{D}_{z_r}, \\mathcal{D}_a)\\), or even more directly, using \\(MSE(z_r, a)\\). According to the author, this is to prevent the latent space to be distributed according to the distribution of the attribute space, as \\(z_r\\) is learnt to get closer to \\(a\\). This might be in direct conflict with the KL-divergence loss since this is trying to enforce a more Gaussian-like structure to the latent space. Hence, there might exists a tradeoff here between (1) the precision of \\(z_r\\) modelling the actual attribute values (as using relative distance will not be that precise as using absolute values), and (2) the correlation metric between \\(z_r\\) and \\(a\\).</p>\n<p>Figure below (through my own experiment) shows the same t-SNE diagram, the left side colored using regularized \\(z_r\\) values, and the right side colored using actual \\(a\\) values. We can see that the overall trend of value change is indeed captured, but the precision between values of \\(z_r\\) and \\(a\\) on individual samples are not necessarily accurate.</p>\n<p><img src=\"/img/ashis2.png\" alt=\"\"></p>\n<h3 id=\"5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\"><a href=\"#5-Deep-Music-Analogy-via-Latent-Representation-Disentanglement\" class=\"headerlink\" title=\"5 - Deep Music Analogy via Latent Representation Disentanglement\"></a>5 - <a href=\"http://archives.ismir.net/ismir2019/paper/000072.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Deep Music Analogy via Latent Representation Disentanglement</strong></a></h3><p><img src=\"/img/deep-analogy.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> “Deep music analogy” shares a very similar concept with music style transfer. This work focuses on disentangling rhythm and pitch from monophonic music, hence achieving controllable synthesis based on a given template of rhythm, a given set of pitches, or a given chord condition.</p>\n<ul>\n<li>The proposed EC<sup>2</sup>-VAE architecture splits latent \\(z\\) into 2 parts – \\(z_{p}\\) and \\(z_{r}\\), where \\(z_{r}\\) is co-erced to reconstruct rhythmic patterns of the sample. Both \\(z_{p}\\) and \\(z_{r}\\), together with the chord condition, is used to decode into the original sample.</li>\n<li>Another point of view is to see it as a type of latent regularization – part of the latent code is “regularized” to be controllable on a particular type of attribute, which in this work the regularization is done by adding a classification loss output by a rhythm classifier.</li>\n<li>Objective evaluation is of 2-fold:<ul>\n<li>After pitch transposition, \\(\\Delta z_{r}\\) should not be changed much and instead \\(\\Delta z_{p}\\) should be changing. This is by measuring the L1-norm of change in \\(z\\).</li>\n<li>Modifying evaluation methods from <a href=\"https://arxiv.org/pdf/1802.05983.pdf\" target=\"_blank\" rel=\"noopener\">FactorVAE</a>, this work proposes to evaluate disentanglement by measuring average variances of the values in each latent dimension after pitch / rhythm augmentation in input samples. Should the disentanglement be successful, when rhythm augmentation is done, the largest variance dimensions should correspond to the dimensions that are explicitly conditioned to model rhythm attributes (and vice versa for pitch attribute).</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\"><a href=\"#6-Controlling-Symbolic-Music-Generation-Based-On-Concept-Learning-From-Domain-Knowledge\" class=\"headerlink\" title=\"6 - Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge\"></a>6 - <a href=\"http://archives.ismir.net/ismir2019/paper/000100.pdf\" target=\"_blank\" rel=\"noopener\"><strong>Controlling Symbolic Music Generation Based On Concept Learning From Domain Knowledge</strong></a></h3><p><img src=\"/img/extres.png\" alt=\"\"></p>\n<p><strong>Published at:</strong> ISMIR 2019<br><strong>Dataset type:</strong> Single track, monophonic piano music<br><strong>Representation used:</strong> Piano roll (final layer as softmax)<br><strong>Novelty:</strong> This work proposes a model known as ExtRes, which stands for <strong>extraction</strong> model and <strong>residual</strong> model. The residual model part is a generative model, while the extraction model allows learning reusable representation for a user-specified concept, given a function based on domain knowledge on the concept.</p>\n<p>From the graphical model, we can see that:</p>\n<ul>\n<li>During inference, latent code \\(z_e\\) is learnt to model user-defined attributes \\(y\\) via a probabilistic encoder with posterior \\(q_{\\phi_{e}}(z_e|y)\\) and parameters \\(\\phi_{e}\\) (the parameters are, in this case, the neural network weights). Separately, latent code \\(z_r\\) is learnt to model input sample \\(x\\) via another probabilistic encoder with posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and parameters \\(\\phi_{r}\\), taking in \\(y\\) as an additional condition during encoding.</li>\n<li>During generation, latent code \\(z_e\\) and \\(z_r\\) and both sampled from a standard Gaussian prior. A decoder with parameters \\(\\theta_y\\) is trained to decode \\(z_e\\) into \\(y\\), and a separate decoder with parameters \\(\\theta_x\\) is trained to decode \\(z_r\\) into \\(x\\), with an additional condition of \\(y\\).</li>\n</ul>\n<p>The final loss function is hence consists of 4 terms:</p>\n<ul>\n<li>the reconstruction loss of the input sample \\(x\\);</li>\n<li>the reconstruction loss of the attribute sequence \\(y\\);</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{e}}(z_e|y)\\) and prior \\(p(z_e)\\) for extraction model;</li>\n<li>the KL divergence between posterior \\(q_{\\phi_{r}}(z_r|x, y)\\) and prior \\(p(z_r)\\) for residual model.</li>\n</ul>\n<p>Here, we can see that the residual model is trained in a CVAE manner, such as to achieve conditional generation, with condition \\(y\\) should \\(y\\) be either obtained from (1) the learnt extraction model, or (2) the dataset itsef (in this case, it resembles with the teacher-forcing training technique).</p>\n<br/>\n\n<p>Other relevant papers that we would like to list here include:<br>7 - <a href=\"https://arxiv.org/pdf/1711.07050.pdf\" target=\"_blank\" rel=\"noopener\">A Classifying Variational Autoencoder with Application to Polyphonic Music Generation</a><br>8 - <a href=\"http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1376485&dswid=-5769\" target=\"_blank\" rel=\"noopener\">MahlerNet: Unbounded Orchestral Music with Neural Networks</a><br>9 - <a href=\"https://arxiv.org/pdf/1711.05772.pdf\" target=\"_blank\" rel=\"noopener\">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br>10 - <a href=\"https://arxiv.org/pdf/1707.04588.pdf\" target=\"_blank\" rel=\"noopener\">GLSR-VAE: Geodesic Latent Space Regularization for Variational AutoEncoder Architectures</a></p>\n<br/>\n\n<h2 id=\"3-Thoughts-and-Discussion\"><a href=\"#3-Thoughts-and-Discussion\" class=\"headerlink\" title=\"3 - Thoughts and Discussion\"></a>3 - Thoughts and Discussion</h2><p>I hereby list some of my thoughts regarding these works as above for future discussion and hopefully for even more exciting future work.</p>\n<h3 id=\"1-Common-usage-of-the-latent-code\"><a href=\"#1-Common-usage-of-the-latent-code\" class=\"headerlink\" title=\"1 - Common usage of the latent code\"></a>1 - Common usage of the latent code</h3><p>We could observe that a whole lot of applications of VAE are focusing on <strong>music attribute / feature modelling</strong>. This is more commonly seen as it spans over several types of tasks including controllable music generation, higher level style transfer, and lower level attribute / feature transfer. Normally, a latent space is being encoded for each factor, so as to achieve separation in modelling different factors in the music piece. During generation, a latent code that exhibit the desired factor is either (i) encoded via the learnt posterior from an existing sample, or (2) sampled through a prior from each space, and then being combined and decoded.</p>\n<p>Here, we can summarize some key aspects that one would encounter while using VAE for music attribute modelling:</p>\n<p>(i) <strong>disentanglement</strong>: how are the attributes being <em>disentangled</em> from each other, so as to ensure that each latent space governs one and only desired factor;<br>(ii) <strong>regularization</strong>: how is the latent space being <em>regularized</em> to exhibit a certain desired factor – either by adding in a classifier, or using some self-defined regularization loss.<br>(iii) <strong>identity preservation</strong>: how can we ensure that the identity of the sample can be retained after transformation, while only being changed on the desired factor? Here, we argue that it is determined by 2 factors: the <em>reconstruction quality</em>, and the <em>disentanglement quality</em> of the model. For ensuring disentanglement quality, a common strategy is to use <strong>adversarial training</strong>, such that to ensure the latent space be invariant on the non-governing factors.</p>\n<h3 id=\"2-On-beta-value\"><a href=\"#2-On-beta-value\" class=\"headerlink\" title=\"2 - On \\(\\beta\\) value\"></a>2 - On \\(\\beta\\) value</h3><p>It is an interesting observation to note that commonly within the literature of VAE music modelling, a lot of the work uses a relatively low \\(\\beta\\) value. Among the first 5 papers discussed above, each of them uses \\(\\beta\\) value of 0.2, 0.1, 0.02, 0.001, and 0.1 respectively, commonly accompanied by an annealing strategy. Only for the 6th paper, \\(\\beta\\) value is within a range of [0.7, 1.0] depending on the attribute modelled.</p>\n<p>It seems that although we are mostly modelling only monophonic or single-track polyphonic music, it has been hard enough to retain the reconstruction accuracy on a higher \\(\\beta\\) value. Additionally, the <a href=\"https://arxiv.org/abs/1809.07600\" target=\"_blank\" rel=\"noopener\">MIDI-VAE</a> paper has further showed that the reconstruction accuracy are very much poorer given higher \\(\\beta\\) values. It would be interesting to unveil the reasons behind why sequential music data are inherently hard to achieve higher reconstruction accuracy. More important, given the fact of the tradeoff between disentanglement and reconstruction as proposed by <a href=\"https://openreview.net/forum?id=Sy2fzU9gl\" target=\"_blank\" rel=\"noopener\">\\(\\beta\\)-VAE</a>, how could we find a balanced sweet spot for good disentanglement provided with such low range of \\(\\beta\\) values remain an interesting challenge.</p>\n<h3 id=\"3-On-music-representation-used\"><a href=\"#3-On-music-representation-used\" class=\"headerlink\" title=\"3 - On music representation used\"></a>3 - On music representation used</h3><p>Common music representation used during modelling include MIDI-like events, piano roll or text (for more details refer to <a href=\"https://arxiv.org/abs/1709.01620\" target=\"_blank\" rel=\"noopener\">this survey paper</a>). For VAE in music modelling, the most common used representation is either MIDI-like events (mostly for polyphonic music), or piano roll. Hence, the encoder and decoder used in VAE are often autoregressive, either using LSTMs, GRUs, or even <a href=\"https://arxiv.org/pdf/1912.05537.pdf\" target=\"_blank\" rel=\"noopener\">Transformers</a>. Often times, the encoder or the decoder part can be further split into hierachies, with each level modelling low to high-level features from note, measure, phrase to the whole segment.</p>\n<p>Recently, <a href=\"http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf\" target=\"_blank\" rel=\"noopener\">Jeong et al.</a> proposed to use graphs instead of normal sequential tokens to represent music performances. Although the superiority of using graph as compared to common sequential representations is not evident yet, this might be a promising and interesting path to pursue for future work.</p>\n<h3 id=\"4-On-the-measure-of-“controllability”\"><a href=\"#4-On-the-measure-of-“controllability”\" class=\"headerlink\" title=\"4 - On the measure of “controllability”\"></a>4 - On the measure of “controllability”</h3><p>How could we evaluate if a model has a “higher controllability”, on a given factor, during generation? The most related one might be by <a href=\"https://github.com/ashispati/AttributeModelling\" target=\"_blank\" rel=\"noopener\">Pati et al.</a>, whom has given an interpretability metric which mainly returns a score depicting the correlation between the latent code and the attribute modelled.</p>\n<h3 id=\"5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\"><a href=\"#5-Can-VAE-be-an-end-to-end-architecture-for-music-generation\" class=\"headerlink\" title=\"5 - Can VAE be an end-to-end architecture for music generation?\"></a>5 - Can VAE be an end-to-end architecture for music generation?</h3><p>From most of the works above, we see VAE being used to generate mainly short segments of music (4 bars, 16 beats, etc.), which are unlike <strong>language modelling</strong> approaches such as <a href=\"https://arxiv.org/pdf/1809.04281.pdf\" target=\"_blank\" rel=\"noopener\">Music Transformer</a>, <a href=\"https://openai.com/blog/musenet/\" target=\"_blank\" rel=\"noopener\">MuseNet</a>, and <a href=\"https://arxiv.org/pdf/2002.00212.pdf\" target=\"_blank\" rel=\"noopener\">Pop Music Transformer</a> that can generate minute-long decent music pieces with observable long term structure.</p>\n<p>Latent space models and language models might each have their own strengths in the context of music generation. Latent space models are useful for feature / attribute modelling, with an extension of usage on style transfer; whereas language models are strong at generation long sequences which exhibit structure. Combining the strengths of both approaches might be an interesting direction for improving the quality and flexibility of state-of-the-art music generation models.</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ck5s3jzfj00004qv5g9865rpz","tag_id":"ck5s3yyg300064qv5evr99y2v","_id":"ck5s3yyg400074qv5e8ls7ygf"},{"post_id":"ck5s3jzfj00004qv5g9865rpz","tag_id":"ck5s3z7pb00084qv54k2z6635","_id":"ck5s3z7pb00094qv533soa73w"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0iy0001tbm86k7r8jw0","_id":"ck89oi0j10003tbm8gm9e58j0"},{"post_id":"ck89oi0it0000tbm8hg9jhjt3","tag_id":"ck89oi0j00002tbm8b5htefr6","_id":"ck89oi0j10004tbm8ajru04wh"}],"Tag":[{"name":"centOS","_id":"ck5rzh79g00018dv5hvt3fro0"},{"name":"redis","_id":"ck5rzh79h00028dv5bcno9ru7"},{"name":"test1","_id":"ck5rzhckr00058dv5e1cj86cx"},{"name":"test2","_id":"ck5rzhcks00068dv55wag4oae"},{"name":"General Thoughts","_id":"ck5s3yyg300064qv5evr99y2v"},{"name":"AI Music","_id":"ck5s3z7pb00084qv54k2z6635"},{"name":"VAE","_id":"ck89oi0iy0001tbm86k7r8jw0"},{"name":"Symbolic Music","_id":"ck89oi0j00002tbm8b5htefr6"}]}}