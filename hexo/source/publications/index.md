---
layout: "publications"
title: "Publications"
date: 2020-01-23 04:48:33
comments: true
---
## Publications / Talks

<h3><mark style="background-color: rgba(39,243,106,0.15);">Parameter Inference of Music Synthesizers using Deep Learning</mark></h3>
<i>Audio Developer Conference (ADC) 2022</i>

> An introductory talk that covers how synthesizer parameter inference facilitates the sound design process, and gives an overview of the recent works that use deep learning to perform parameter inference, as well as the challenges that were faced in solving this task.

<div class="page-tags">
    <a class="item" href="https://www.youtube.com/watch?v=nZ560W6bA3o">VIDEO</a>
    <a class="item" href="https://docs.google.com/presentation/d/1PA4fom6QvCW_YG8L0MMVumrAluljcymndNlaK2HW5t0/edit">SLIDES</a>
</div>

<br/>

<h3><mark style="background-color: rgba(39,243,106,0.15);">Semi-supervised music emotion recognition using noisy student training and harmonic pitch class profiles</mark></h3>
<i>MediaEval 2021, Emotions and Themes in Music Challenge</i>

> An experiment on leveraging semi-supervised learning on music emotion recognition with noisy student training. Ranked 6th on the [challenge](https://multimediaeval.github.io/2021-Emotion-and-Theme-Recognition-in-Music-Task/results).

<div class="page-tags">
    <a class="item" href="https://arxiv.org/pdf/2112.00702.pdf">PDF</a>
    <a class="item" href="https://github.com/gudgud96/noisy-student-emotion-training">CODE</a>
</div>

<br/>

<h3><mark style="background-color: rgba(39,243,106,0.15);">Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling</mark></h3>
<ins>Hao Hao Tan</ins>, Dorien Herremans.<br/>
<i>International Society for Music Information Retrieval (ISMIR) Conference 2020.</i>

> Using **"faders"** (latent regularization & disentanglement) to control low-level musical features and **"presets"** (GM-VAE) to capture the relationship between the "faders" and the abstract, high-level feature for controllable music generation.

<div class="page-tags">
    <a class="item" href="https://arxiv.org/pdf/2007.15474.pdf">PDF</a>
    <a class="item" href="https://github.com/gudgud96/music-fader-nets">CODE</a>
    <a class="item" href="https://music-fadernets.github.io/">DEMO</a>
</div>

<br/>

<h3><mark style="background-color: rgba(39,243,106,0.15);">Generative Modelling for Controllable Audio Synthesis of Expressive Piano Performance</mark></h3>
<ins>Hao Hao Tan</ins>, Yin-Jyun Luo, Dorien Herremans.<br/>
<i>Machine Learning for Media Discovery (ML4MD) Workshop, ICML 2020.</i>

> Introducing **GM-VAEs** on temporal piano performance modelling, hence allowing **fine-grained controllability** on performance style features (articulation & dynamics) for audio synthesis.

<div class="page-tags">
    <a class="item" href="https://arxiv.org/pdf/2006.09833.pdf">PDF</a>
    <a class="item" href="https://github.com/gudgud96/piano-synthesis">CODE</a>
    <a class="item" href="https://piano-performance-synthesis.github.io/">DEMO</a>
</div>

<br/>

<h3><mark style="background-color: rgba(39,243,106,0.15);">ChordAL: A Chord-Based Approach for Music Generation using Bi-LSTMs</mark></h3>
<ins>Hao Hao Tan</ins><br/>
<i>Creative Submission Extended Abstract, International Conference of Computational Creativity (ICCC) 2019.</i>

> A two-stage music generation pipeline with a **chord generator** and a **chord-to-note generator** as a *seq2seq* task. Chord embeddings unveil the pattern of the **Circle-of-Fifths**.

<div class="page-tags">
    <a class="item" href="http://computationalcreativity.net/iccc2019/papers/iccc19-demo-9.pdf">PDF</a>
    <a class="item" href="https://github.com/gudgud96/ChordAL">CODE</a>
    <a class="item" href="https://soundcloud.com/hord-hord-basedomposer">DEMO</a>
</div>